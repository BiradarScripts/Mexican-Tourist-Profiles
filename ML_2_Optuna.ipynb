{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OcFhvpILfPsY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ze10La0LG2g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "35IusksEfjH7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPz6YStPG481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['category'].fillna(train_df['category'].median(), inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N1zHYbUfkVI",
        "outputId": "666f14ce-fc4e-47c6-8e98-422d36dc7be2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e96754ba7e74>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['category'].fillna(train_df['category'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T8YfSC-fG6FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns = ['trip_ID','travelling_with','trip_purpose','first_time_visitor','source_of_info','weather_at_arrival','tour_arrangement','special_requirements'])\n",
        "test_df = test_df.drop(columns = ['trip_ID','travelling_with','trip_purpose','first_time_visitor','source_of_info','weather_at_arrival','tour_arrangement','special_requirements'])"
      ],
      "metadata": {
        "id": "4NFXKt3hfl6C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_nights'] = train_df['mainland_nights'] + train_df['island_nights']\n",
        "test_df['total_nights'] = test_df['mainland_nights'] + test_df['island_nights']"
      ],
      "metadata": {
        "id": "8a33LKcRfnUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "\n",
        "# Function to apply target encoding with K-Fold\n",
        "def target_encode_column(train_df, test_df, target, col, n_splits=5):\n",
        "    # Create Series to store the target-encoded values for train and test\n",
        "    train_encoded = pd.Series(index=train_df.index, dtype='float64')\n",
        "    test_encoded = pd.Series(index=test_df.index, dtype='float64')\n",
        "\n",
        "    # Set up K-Fold for target encoding on the train set\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(train_df):\n",
        "        # Train and validation folds\n",
        "        train_fold, valid_fold = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n",
        "\n",
        "        # Calculate target mean per category in train fold\n",
        "        means = train_fold.groupby(col)[target].mean()\n",
        "\n",
        "        # Map these means to the validation fold\n",
        "        train_encoded.iloc[valid_idx] = valid_fold[col].map(means)\n",
        "\n",
        "    # Fill missing values in train_encoded with the overall target mean\n",
        "    train_encoded.fillna(train_df[target].mean(), inplace=True)\n",
        "\n",
        "    # Map the target encoding means to the test set\n",
        "    category_means = train_df.groupby(col)[target].mean()\n",
        "    test_encoded = test_df[col].map(category_means)\n",
        "\n",
        "    # Fill missing values in test_encoded with the overall target mean\n",
        "    test_encoded.fillna(train_df[target].mean(), inplace=True)\n",
        "\n",
        "    return train_encoded, test_encoded\n",
        "\n",
        "# Apply target encoding on 'visitor_nation'\n",
        "train_df['visitor_nation_encoded'], test_df['visitor_nation_encoded'] = target_encode_column(train_df, test_df, target='category', col='visitor_nation')\n",
        "\n",
        "# Drop the original 'visitor_nation' column if not needed further\n",
        "train_df = train_df.drop(columns=['visitor_nation'])\n",
        "test_df = test_df.drop(columns=['visitor_nation'])\n",
        "\n",
        "# Display first few rows to verify the encoding\n",
        "train_df[['visitor_nation_encoded', 'category']].head(), test_df[['visitor_nation_encoded']].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm-47e3sfomt",
        "outputId": "054cbf5d-3fbd-483e-a20b-933a7281c618"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   visitor_nation_encoded  category\n",
              " 0                0.270869       1.0\n",
              " 1                1.253133       2.0\n",
              " 2                0.696833       2.0\n",
              " 3                0.282976       0.0\n",
              " 4                0.284483       0.0,\n",
              "    visitor_nation_encoded\n",
              " 0                1.500000\n",
              " 1                0.436170\n",
              " 2                0.500000\n",
              " 3                0.823529\n",
              " 4                0.428986)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_people'] = train_df['female_count'] + train_df['male_count']\n",
        "test_df['total_people'] = test_df['female_count'] + test_df['male_count']"
      ],
      "metadata": {
        "id": "WniEfkbffqZv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Average group size\n",
        "train_df['average_group_size'] = train_df['total_people'] / train_df['total_nights']\n",
        "test_df['average_group_size'] = test_df['total_people'] / test_df['total_nights']\n",
        "\n",
        "# 2. Trip duration categories\n",
        "train_df['trip_duration_category'] = pd.cut(train_df['total_nights'], bins=[0, 3, 7, float('inf')], labels=['short', 'medium', 'long'])\n",
        "test_df['trip_duration_category'] = pd.cut(test_df['total_nights'], bins=[0, 3, 7, float('inf')], labels=['short', 'medium', 'long'])\n",
        "\n",
        "# 3. Visitor demographics (family and group indicators)\n",
        "train_df['is_family'] = ((train_df['female_count'] > 0) & (train_df['male_count'] > 0)).astype(int)\n",
        "test_df['is_family'] = ((test_df['female_count'] > 0) & (test_df['male_count'] > 0)).astype(int)\n",
        "\n",
        "train_df['is_group'] = (train_df['total_people'] > 2).astype(int)\n",
        "test_df['is_group'] = (test_df['total_people'] > 2).astype(int)\n",
        "\n",
        "# 4. Days in each type of location (mainland/island ratios)\n",
        "train_df['mainland_ratio'] = train_df['mainland_nights'] / train_df['total_nights']\n",
        "train_df['island_ratio'] = train_df['island_nights'] / train_df['total_nights']\n",
        "\n",
        "test_df['mainland_ratio'] = test_df['mainland_nights'] / test_df['total_nights']\n",
        "test_df['island_ratio'] = test_df['island_nights'] / test_df['total_nights']"
      ],
      "metadata": {
        "id": "vV0aoFVTfshI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns = ['female_count','male_count','mainland_nights','island_nights'])\n",
        "test_df = test_df.drop(columns = ['female_count','male_count','mainland_nights','island_nights'])"
      ],
      "metadata": {
        "id": "LCtpZlhAft54"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['key_activity'] = train_df['key_activity'].replace('Widlife Tourism', 'Wildlife Tourism')\n",
        "test_df['key_activity'] = test_df['key_activity'].replace('Widlife Tourism', 'Wildlife Tourism')"
      ],
      "metadata": {
        "id": "JbKMpNGMfvjB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_key_activity = train_df['key_activity'].mode()[0]\n",
        "\n",
        "train_df['key_activity'].fillna(mode_key_activity, inplace=True)\n",
        "test_df['key_activity'].fillna(mode_key_activity, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCu8C07Qfwv2",
        "outputId": "5fe05fa4-a3e4-4ef5-f651-62edff77ab6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a221c41d21a0>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['key_activity'].fillna(mode_key_activity, inplace=True)\n",
            "<ipython-input-11-a221c41d21a0>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['key_activity'].fillna(mode_key_activity, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Xvn0EGE4fz-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define mappings for range values to approximate numeric values\n",
        "days_before_booked_map = {\n",
        "    '1-7': 4,\n",
        "    '8-14': 11,\n",
        "    '15-30': 22.5,\n",
        "    '31-60': 45.5,\n",
        "    '61-90': 75.5,\n",
        "    '90+': 90\n",
        "}\n",
        "\n",
        "tour_length_map = {\n",
        "    '1-6': 3.5,\n",
        "    '7-14': 10.5,\n",
        "    '15-29': 22,\n",
        "    '30+': 30\n",
        "}\n",
        "\n",
        "# Apply mappings to convert ranges to numeric values\n",
        "train_df['days_before_booked_num'] = train_df['days_before_booked'].map(days_before_booked_map)\n",
        "test_df['days_before_booked_num'] = test_df['days_before_booked'].map(days_before_booked_map)\n",
        "\n",
        "train_df['tour_length_num'] = train_df['tour_length'].map(tour_length_map)\n",
        "test_df['tour_length_num'] = test_df['tour_length'].map(tour_length_map)\n",
        "\n",
        "# Fill missing values with median of each column\n",
        "train_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
        "test_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
        "\n",
        "train_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
        "test_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
        "\n",
        "# Drop the original columns if not needed\n",
        "train_df = train_df.drop(columns=['days_before_booked', 'tour_length'])\n",
        "test_df = test_df.drop(columns=['days_before_booked', 'tour_length'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRX6UkT5f2YQ",
        "outputId": "d887d552-757d-40de-8273-1d1d2d4ea6db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0f46c9e2cb81>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for column in ['transport_package_international', 'package_accomodation', 'food_package', 'insurance_package']:\n",
        "    most_frequent = train_df[column].mode()[0]\n",
        "    train_df[column].fillna(most_frequent, inplace=True)\n",
        "    test_df[column].fillna(most_frequent, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmtvt3Swf4P0",
        "outputId": "0de26696-c971-4b32-ecb9-6976c7c7c3b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a54ac0f19e68>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df[column].fillna(most_frequent, inplace=True)\n",
            "<ipython-input-13-a54ac0f19e68>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df[column].fillna(most_frequent, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
        "test_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
        "\n",
        "train_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n",
        "test_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSMgUwSLf7J8",
        "outputId": "5b941bb6-2f36-4dac-ce5d-6ae05884e10e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-758df23ed5c5>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "Tuuy0_jcf-RM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train_df['age_bracket'] = train_df['age_bracket'].replace({'<18': 'Below 25', '18-24': 'Below 25'})\n",
        "test_df['age_bracket'] = test_df['age_bracket'].replace({'<18': 'Below 25', '18-24': 'Below 25'})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['age_bracket_encoded'] = label_encoder.fit_transform(train_df['age_bracket'])\n",
        "test_df['age_bracket_encoded'] = label_encoder.transform(test_df['age_bracket'])"
      ],
      "metadata": {
        "id": "fw8nxTmkgBNe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns=['age_bracket'])\n",
        "test_df = test_df.drop(columns=['age_bracket'])"
      ],
      "metadata": {
        "id": "oYt-ndzsgDEY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_encoded = pd.get_dummies(train_df, drop_first=True,dtype='int64')"
      ],
      "metadata": {
        "id": "SiGNqNoTgEYR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_encoded = pd.get_dummies(test_df, drop_first=True,dtype='int64')"
      ],
      "metadata": {
        "id": "rviT0rtHgGFR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define the target column\n",
        "# Replace 'target_column' with the actual name of the column in your DataFrame\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data first to avoid data leakage before scaling\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "9pAK_In0gHOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "MMvpsKwLgJrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define the target column\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert back to DataFrames to retain column names\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='rbf', random_state=303)  # 'rbf' is the default kernel\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy on the validation set\n",
        "y_pred = svm_classifier.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data with the same scaler used on the training data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "test_df_encoded_scaled = pd.DataFrame(test_df_encoded_scaled, columns=test_df_encoded.columns)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = svm_classifier.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Create a DataFrame with trip_ID and predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': predictions\n",
        "})\n",
        "\n",
        "# Save predictions to a new CSV file\n",
        "results_df.to_csv('predictions_results_svm.csv', index=False)\n",
        "print(\"Predictions saved to predictions_results_svm.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VePruqbgwLT",
        "outputId": "fa907084-18e3-4eae-dd80-f614d6c9759a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.7545454545454545\n",
            "Predictions saved to predictions_results_svm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the target column\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert back to DataFrames to retain column names\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "\n",
        "# Define the SVM classifier\n",
        "svm = SVC(random_state=303)\n",
        "\n",
        "# Define the parameter distribution for tuning\n",
        "param_dist = {\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Different kernel types\n",
        "    'C': np.logspace(-2, 2, 10),  # Regularization parameter (from 0.01 to 100)\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 5)),  # Kernel coefficient\n",
        "    'degree': [2, 3, 4, 5],  # Degree of polynomial kernel function (only for 'poly')\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5,  # Number of random combinations to try\n",
        "    scoring='accuracy',\n",
        "    cv=2,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=303\n",
        ")\n",
        "\n",
        "# Perform random search on training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", random_search.best_score_)\n",
        "\n",
        "# Use the best estimator to make predictions on validation data\n",
        "best_svm = random_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy with Best SVM:\", accuracy)\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data with the same scaler used on the training data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "test_df_encoded_scaled = pd.DataFrame(test_df_encoded_scaled, columns=test_df_encoded.columns)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_svm.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Create a DataFrame with trip_ID and predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': predictions\n",
        "})\n",
        "\n",
        "# Save predictions to a new CSV file\n",
        "results_df.to_csv('predictions_results_svm_tuned.csv', index=False)\n",
        "print(\"Predictions saved to predictions_results_svm_tuned.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "u2t9qzhtgz7o",
        "outputId": "abcc5deb-1f54-4377-d023-86d5ee5b07cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9e03a8742ed8>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Perform random search on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Print the best parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.replace(r'[\\[\\]<>,]', '', regex=True)\n",
        "    return df\n",
        "\n",
        "# Manually define SVM hyperparameters\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')  # You can adjust these values as needed\n",
        "\n",
        "# Train the SVM regressor\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "svr_predictions = svr_model.predict(X_val)\n",
        "svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "print(\"SVM Regressor Mean Absolute Error:\", svr_mae)\n",
        "\n",
        "# Predict on the test set\n",
        "svr_test_predictions = svr_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_only.csv', index=False)\n",
        "\n",
        "print(\"Submission for SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dY4Funphmyc",
        "outputId": "8040c66b-9f87-40ff-d820-13d72919e665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Regressor Mean Absolute Error: 0.3145443155252721\n",
            "Submission for SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.replace(r'[\\[\\]<>,]', '', regex=True)\n",
        "    return df\n",
        "\n",
        "# Manually define SVM hyperparameters\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')  # You can adjust these values as needed\n",
        "\n",
        "# Train the SVM regressor\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "svr_predictions = svr_model.predict(X_val)\n",
        "svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "print(\"SVM Regressor Mean Absolute Error:\", svr_mae)\n",
        "\n",
        "# Predict on the test set\n",
        "svr_test_predictions = svr_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "print(svr_output['category'])\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_only_tp.csv', index=False)\n",
        "\n",
        "print(\"Submission for SVM model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ChZgCvmGgg",
        "outputId": "b4bf5608-8ff6-49e5-87d0-20a2e3b28a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Regressor Mean Absolute Error: 0.3145443155252721\n",
            "0       1.140345\n",
            "1       0.002403\n",
            "2       0.227088\n",
            "3       0.219506\n",
            "4       0.296209\n",
            "          ...   \n",
            "5847    1.304772\n",
            "5848    0.065384\n",
            "5849    0.270208\n",
            "5850    1.156263\n",
            "5851    0.756091\n",
            "Name: category, Length: 5852, dtype: float64\n",
            "Submission for SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svr_output['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rfND8RIKmzqa",
        "outputId": "a38da53b-096b-4dd8-eefc-5ff3d3f9b38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "       ..\n",
              "5847    2\n",
              "5848    0\n",
              "5849    0\n",
              "5850    2\n",
              "5851    1\n",
              "Name: category, Length: 5852, dtype: category\n",
              "Categories (3, int64): [0 < 1 < 2]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5847</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5848</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5849</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5850</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5851</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5852 rows  1 columns</p>\n",
              "</div><br><label><b>dtype:</b> category</label>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define different kernels to test\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# Initialize variables to store the best kernel and corresponding MAE\n",
        "best_kernel = None\n",
        "best_mae = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Loop through each kernel and evaluate its performance\n",
        "for kernel in kernels:\n",
        "    print(f\"Training SVR with kernel: {kernel}\")\n",
        "    svr_model = SVR(kernel=kernel, C=1.0, gamma='scale')  # Keep other parameters fixed for now\n",
        "    svr_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    svr_predictions = svr_model.predict(X_val)\n",
        "    svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "    print(f\"Mean Absolute Error with kernel {kernel}: {svr_mae}\")\n",
        "\n",
        "    # Check if this kernel is the best so far\n",
        "    if svr_mae < best_mae:\n",
        "        best_mae = svr_mae\n",
        "        best_kernel = kernel\n",
        "        best_model = svr_model\n",
        "\n",
        "print(f\"Best kernel: {best_kernel} with MAE: {best_mae}\")\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "svr_test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_tuned.csv', index=False)\n",
        "\n",
        "print(\"Submission for tuned SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "oFTkTdeNlnex",
        "outputId": "4f98d801-669f-4e1c-dc90-24027a1417e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVR with kernel: linear\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-49baaa9e52dd>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training SVR with kernel: {kernel}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msvr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Keep other parameters fixed for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msvr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Predict on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_status_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_encoded['category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "AvykXu7Pls6i",
        "outputId": "ac75e2ea-e990-477f-a5a0-0c8ce52a2cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "0.0    6240\n",
              "1.0    4943\n",
              "2.0    1463\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>4943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>1463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define different kernels to test\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# Initialize variables to store the best kernel and corresponding MAE\n",
        "best_kernel = None\n",
        "best_mae = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Loop through each kernel and evaluate its performance\n",
        "for kernel in kernels:\n",
        "    print(f\"Training SVR with kernel: {kernel}\")\n",
        "    svr_model = SVR(kernel=kernel, C=1.0, gamma='scale')  # Keep other parameters fixed for now\n",
        "    svr_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    svr_predictions = svr_model.predict(X_val)\n",
        "    svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "    print(f\"Mean Absolute Error with kernel {kernel}: {svr_mae}\")\n",
        "\n",
        "    # Check if this kernel is the best so far\n",
        "    if svr_mae < best_mae:\n",
        "        best_mae = svr_mae\n",
        "        best_kernel = kernel\n",
        "        best_model = svr_model\n",
        "\n",
        "print(f\"Best kernel: {best_kernel} with MAE: {best_mae}\")\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "svr_test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_tuned.csv', index=False)\n",
        "\n",
        "print(\"Submission for tuned SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jTDbjHkjgc_",
        "outputId": "9d345241-8539-4bdb-a130-b15450069374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVR with kernel: linear\n",
            "Mean Absolute Error with kernel linear: 0.33002165030410735\n",
            "Training SVR with kernel: poly\n",
            "Mean Absolute Error with kernel poly: 0.3318558663207929\n",
            "Training SVR with kernel: rbf\n",
            "Mean Absolute Error with kernel rbf: 0.3145443155252721\n",
            "Training SVR with kernel: sigmoid\n",
            "Mean Absolute Error with kernel sigmoid: 13.60265789028028\n",
            "Best kernel: rbf with MAE: 0.3145443155252721\n",
            "Submission for tuned SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming `train_df_encoded` is your original dataset\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=303, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = log_reg.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data using the same scaler\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = log_reg.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_logreg.csv', index=False)\n",
        "print(\"Logistic Regression predictions saved to 'submission_logreg.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFM6j_Ckt14",
        "outputId": "4bd5afff-cbcb-4ae9-878b-71e33e5ba5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7573\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84      1248\n",
            "         1.0       0.70      0.72      0.71       989\n",
            "         2.0       0.70      0.42      0.52       293\n",
            "\n",
            "    accuracy                           0.76      2530\n",
            "   macro avg       0.74      0.67      0.69      2530\n",
            "weighted avg       0.75      0.76      0.75      2530\n",
            "\n",
            "Logistic Regression predictions saved to 'submission_logreg.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization terms\n",
        "    'C': [0.01, 0.1, 1, 10, 100],                  # Inverse regularization strength\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs'],      # Solvers for optimization\n",
        "    'max_iter': [100, 500, 1000]                   # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "log_reg = LogisticRegression(random_state=303)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "print(\"Starting Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred = best_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data using the same scaler\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_logreg_tuned.csv', index=False)\n",
        "print(\"Tuned Logistic Regression predictions saved to 'submission_logreg_tuned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "yT0eacBjnxyb",
        "outputId": "4d5f3caa-661b-443a-ffd3-9429bd2b1e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Grid Search...\n",
            "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c1ec4fc68053>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Fit GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Grid Search...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Get the best parameters and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_abort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# When workers are killed in a brutal manner, they cannot execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0;31m# is shutting down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_global_shutdown_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m                 \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m                 \u001b[0m_threads_wakeups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define hyperparameter space for RandomizedSearch\n",
        "param_dist = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization terms\n",
        "    'C': np.logspace(-4, 4, 20),                    # Wide range of regularization strength\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs'],       # Solvers for optimization\n",
        "    'max_iter': [100, 200, 500, 1000]               # Maximum iterations\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "log_reg = LogisticRegression(random_state=303)\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,               # Number of parameter combinations to try\n",
        "    scoring='accuracy',      # Metric for optimization\n",
        "    cv=3,                    # 3-fold cross-validation\n",
        "    n_jobs=-1,               # Use all available cores\n",
        "    random_state=303,        # For reproducibility\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "print(\"Starting Randomized Search...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred = best_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data using the same scaler\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_logreg_random_tuned.csv', index=False)\n",
        "print(\"Tuned Logistic Regression predictions saved to 'submission_logreg_random_tuned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8XVHvZJoP2v",
        "outputId": "3bbdfe99-2aef-4587-d6e3-67d1e218e0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Randomized Search...\n",
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "93 fits failed out of a total of 150.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "18 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "23 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "22 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.71510478        nan        nan        nan 0.74327798 0.74308027\n",
            "        nan 0.73843416        nan        nan 0.74031238        nan\n",
            "        nan 0.7394227         nan 0.73892843 0.73843416        nan\n",
            "        nan 0.73902728        nan        nan        nan 0.7394227\n",
            "        nan 0.73843416        nan        nan        nan        nan\n",
            "        nan 0.73032819        nan        nan        nan        nan\n",
            " 0.74031238 0.73922499 0.74278371        nan        nan 0.74278371\n",
            " 0.74308027 0.72004745        nan        nan        nan        nan\n",
            " 0.74308027        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 100, 'C': 10000.0}\n",
            "Validation Accuracy: 0.7569\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84      1248\n",
            "         1.0       0.70      0.72      0.71       989\n",
            "         2.0       0.70      0.42      0.52       293\n",
            "\n",
            "    accuracy                           0.76      2530\n",
            "   macro avg       0.74      0.67      0.69      2530\n",
            "weighted avg       0.75      0.76      0.75      2530\n",
            "\n",
            "Tuned Logistic Regression predictions saved to 'submission_logreg_random_tuned.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert target to categorical if its a classification task\n",
        "num_classes = len(y.unique())\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "# Build the neural network\n",
        "def build_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')  # Softmax for classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_model(input_dim=X_train.shape[1], num_classes=num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "y_pred_val = model.predict(X_val)\n",
        "y_pred_classes = y_pred_val.argmax(axis=1)\n",
        "y_val_classes = y_val.argmax(axis=1)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val_classes, y_pred_classes))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "test_pred_classes = test_predictions.argmax(axis=1)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_pred_classes\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn.csv', index=False)\n",
        "print(\"Neural Network predictions saved to 'submission_nn.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoVLQ6Ojo0H1",
        "outputId": "7fc35e1e-bb4c-491e-f099-c4c1bf33b9ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.6113 - loss: 0.9334 - val_accuracy: 0.7348 - val_loss: 0.6473\n",
            "Epoch 2/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7168 - loss: 0.6646 - val_accuracy: 0.7415 - val_loss: 0.6227\n",
            "Epoch 3/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7250 - loss: 0.6429 - val_accuracy: 0.7451 - val_loss: 0.6241\n",
            "Epoch 4/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7297 - loss: 0.6409 - val_accuracy: 0.7478 - val_loss: 0.6157\n",
            "Epoch 5/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7326 - loss: 0.6288 - val_accuracy: 0.7407 - val_loss: 0.6173\n",
            "Epoch 6/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7332 - loss: 0.6307 - val_accuracy: 0.7506 - val_loss: 0.6149\n",
            "Epoch 7/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7408 - loss: 0.6250 - val_accuracy: 0.7490 - val_loss: 0.6088\n",
            "Epoch 8/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7324 - loss: 0.6192 - val_accuracy: 0.7522 - val_loss: 0.6071\n",
            "Epoch 9/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7387 - loss: 0.6212 - val_accuracy: 0.7490 - val_loss: 0.6101\n",
            "Epoch 10/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7431 - loss: 0.6163 - val_accuracy: 0.7490 - val_loss: 0.6057\n",
            "Epoch 11/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7367 - loss: 0.6238 - val_accuracy: 0.7518 - val_loss: 0.6028\n",
            "Epoch 12/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7594 - loss: 0.5975 - val_accuracy: 0.7589 - val_loss: 0.6004\n",
            "Epoch 13/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7513 - loss: 0.5907 - val_accuracy: 0.7522 - val_loss: 0.6016\n",
            "Epoch 14/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7541 - loss: 0.5871 - val_accuracy: 0.7569 - val_loss: 0.5997\n",
            "Epoch 15/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7403 - loss: 0.6087 - val_accuracy: 0.7545 - val_loss: 0.6005\n",
            "Epoch 16/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7389 - loss: 0.6051 - val_accuracy: 0.7569 - val_loss: 0.6006\n",
            "Epoch 17/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.5928 - val_accuracy: 0.7585 - val_loss: 0.6013\n",
            "Epoch 18/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7527 - loss: 0.5996 - val_accuracy: 0.7490 - val_loss: 0.6016\n",
            "Epoch 19/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7549 - loss: 0.5902 - val_accuracy: 0.7549 - val_loss: 0.5990\n",
            "Epoch 20/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7480 - loss: 0.5943 - val_accuracy: 0.7601 - val_loss: 0.6008\n",
            "Epoch 21/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7525 - loss: 0.5921 - val_accuracy: 0.7561 - val_loss: 0.6008\n",
            "Epoch 22/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7472 - loss: 0.5962 - val_accuracy: 0.7518 - val_loss: 0.5995\n",
            "Epoch 23/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7539 - loss: 0.5885 - val_accuracy: 0.7498 - val_loss: 0.6010\n",
            "Epoch 24/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7555 - loss: 0.5743 - val_accuracy: 0.7569 - val_loss: 0.5983\n",
            "Epoch 25/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7532 - loss: 0.5780 - val_accuracy: 0.7581 - val_loss: 0.5982\n",
            "Epoch 26/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.5867 - val_accuracy: 0.7581 - val_loss: 0.5968\n",
            "Epoch 27/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7530 - loss: 0.5794 - val_accuracy: 0.7542 - val_loss: 0.5991\n",
            "Epoch 28/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.5817 - val_accuracy: 0.7565 - val_loss: 0.6001\n",
            "Epoch 29/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.5802 - val_accuracy: 0.7577 - val_loss: 0.6037\n",
            "Epoch 30/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.5829 - val_accuracy: 0.7534 - val_loss: 0.6020\n",
            "Epoch 31/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7590 - loss: 0.5706 - val_accuracy: 0.7526 - val_loss: 0.6040\n",
            "Epoch 32/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7470 - loss: 0.6026 - val_accuracy: 0.7534 - val_loss: 0.6037\n",
            "Epoch 33/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7489 - loss: 0.5871 - val_accuracy: 0.7514 - val_loss: 0.6076\n",
            "Epoch 34/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7564 - loss: 0.5679 - val_accuracy: 0.7439 - val_loss: 0.6038\n",
            "Epoch 35/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7482 - loss: 0.5908 - val_accuracy: 0.7542 - val_loss: 0.6051\n",
            "Epoch 36/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7553 - loss: 0.5837 - val_accuracy: 0.7518 - val_loss: 0.6051\n",
            "Epoch 37/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7625 - loss: 0.5784 - val_accuracy: 0.7526 - val_loss: 0.6006\n",
            "Epoch 38/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7624 - loss: 0.5723 - val_accuracy: 0.7526 - val_loss: 0.6022\n",
            "Epoch 39/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7523 - loss: 0.5846 - val_accuracy: 0.7557 - val_loss: 0.6046\n",
            "Epoch 40/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7545 - loss: 0.5816 - val_accuracy: 0.7545 - val_loss: 0.6019\n",
            "Epoch 41/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7546 - loss: 0.5755 - val_accuracy: 0.7530 - val_loss: 0.6020\n",
            "Epoch 42/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7699 - loss: 0.5741 - val_accuracy: 0.7514 - val_loss: 0.6016\n",
            "Epoch 43/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7507 - loss: 0.5853 - val_accuracy: 0.7553 - val_loss: 0.6066\n",
            "Epoch 44/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7592 - loss: 0.5682 - val_accuracy: 0.7557 - val_loss: 0.5998\n",
            "Epoch 45/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7514 - loss: 0.5772 - val_accuracy: 0.7561 - val_loss: 0.6045\n",
            "Epoch 46/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7511 - loss: 0.5808 - val_accuracy: 0.7534 - val_loss: 0.6054\n",
            "Epoch 47/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7623 - loss: 0.5628 - val_accuracy: 0.7526 - val_loss: 0.6021\n",
            "Epoch 48/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7688 - loss: 0.5647 - val_accuracy: 0.7538 - val_loss: 0.6061\n",
            "Epoch 49/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.5707 - val_accuracy: 0.7538 - val_loss: 0.6126\n",
            "Epoch 50/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7651 - loss: 0.5629 - val_accuracy: 0.7490 - val_loss: 0.6089\n",
            "Validation Accuracy: 0.7490\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84      1248\n",
            "           1       0.69      0.70      0.70       989\n",
            "           2       0.63      0.39      0.48       293\n",
            "\n",
            "    accuracy                           0.75      2530\n",
            "   macro avg       0.71      0.65      0.67      2530\n",
            "weighted avg       0.74      0.75      0.74      2530\n",
            "\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Neural Network predictions saved to 'submission_nn.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target (continuous)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Build the neural network\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned.csv', index=False)\n",
        "print(\"Neural Network predictions with bins saved to 'submission_nn_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Pq6cfrpNBi",
        "outputId": "b33ef8e7-f1af-4a0d-dbb2-01ea0d9324c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.7901 - mean_absolute_error: 0.6773 - val_loss: 0.2490 - val_mean_absolute_error: 0.3805\n",
            "Epoch 2/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.3358 - mean_absolute_error: 0.4497 - val_loss: 0.2292 - val_mean_absolute_error: 0.3643\n",
            "Epoch 3/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2757 - mean_absolute_error: 0.4048 - val_loss: 0.2240 - val_mean_absolute_error: 0.3414\n",
            "Epoch 4/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2664 - mean_absolute_error: 0.3897 - val_loss: 0.2205 - val_mean_absolute_error: 0.3502\n",
            "Epoch 5/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2460 - mean_absolute_error: 0.3754 - val_loss: 0.2198 - val_mean_absolute_error: 0.3555\n",
            "Epoch 6/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2458 - mean_absolute_error: 0.3764 - val_loss: 0.2178 - val_mean_absolute_error: 0.3418\n",
            "Epoch 7/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2450 - mean_absolute_error: 0.3747 - val_loss: 0.2160 - val_mean_absolute_error: 0.3339\n",
            "Epoch 8/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2312 - mean_absolute_error: 0.3599 - val_loss: 0.2163 - val_mean_absolute_error: 0.3251\n",
            "Epoch 9/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2429 - mean_absolute_error: 0.3725 - val_loss: 0.2133 - val_mean_absolute_error: 0.3315\n",
            "Epoch 10/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2321 - mean_absolute_error: 0.3604 - val_loss: 0.2187 - val_mean_absolute_error: 0.3527\n",
            "Epoch 11/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2263 - mean_absolute_error: 0.3585 - val_loss: 0.2118 - val_mean_absolute_error: 0.3353\n",
            "Epoch 12/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2308 - mean_absolute_error: 0.3628 - val_loss: 0.2139 - val_mean_absolute_error: 0.3290\n",
            "Epoch 13/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2273 - mean_absolute_error: 0.3592 - val_loss: 0.2205 - val_mean_absolute_error: 0.3287\n",
            "Epoch 14/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2258 - mean_absolute_error: 0.3512 - val_loss: 0.2128 - val_mean_absolute_error: 0.3267\n",
            "Epoch 15/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2282 - mean_absolute_error: 0.3567 - val_loss: 0.2143 - val_mean_absolute_error: 0.3217\n",
            "Epoch 16/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2264 - mean_absolute_error: 0.3545 - val_loss: 0.2146 - val_mean_absolute_error: 0.3291\n",
            "Epoch 17/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2285 - mean_absolute_error: 0.3556 - val_loss: 0.2115 - val_mean_absolute_error: 0.3297\n",
            "Epoch 18/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2229 - mean_absolute_error: 0.3506 - val_loss: 0.2195 - val_mean_absolute_error: 0.3277\n",
            "Epoch 19/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2220 - mean_absolute_error: 0.3500 - val_loss: 0.2120 - val_mean_absolute_error: 0.3261\n",
            "Epoch 20/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2170 - mean_absolute_error: 0.3480 - val_loss: 0.2141 - val_mean_absolute_error: 0.3393\n",
            "Epoch 21/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2141 - mean_absolute_error: 0.3433 - val_loss: 0.2144 - val_mean_absolute_error: 0.3404\n",
            "Epoch 22/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2169 - mean_absolute_error: 0.3502 - val_loss: 0.2132 - val_mean_absolute_error: 0.3204\n",
            "Epoch 23/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2159 - mean_absolute_error: 0.3416 - val_loss: 0.2164 - val_mean_absolute_error: 0.3266\n",
            "Epoch 24/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2188 - mean_absolute_error: 0.3418 - val_loss: 0.2088 - val_mean_absolute_error: 0.3315\n",
            "Epoch 25/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2164 - mean_absolute_error: 0.3437 - val_loss: 0.2134 - val_mean_absolute_error: 0.3260\n",
            "Epoch 26/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2196 - mean_absolute_error: 0.3488 - val_loss: 0.2127 - val_mean_absolute_error: 0.3285\n",
            "Epoch 27/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2120 - mean_absolute_error: 0.3390 - val_loss: 0.2125 - val_mean_absolute_error: 0.3349\n",
            "Epoch 28/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2088 - mean_absolute_error: 0.3347 - val_loss: 0.2124 - val_mean_absolute_error: 0.3293\n",
            "Epoch 29/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2148 - mean_absolute_error: 0.3423 - val_loss: 0.2146 - val_mean_absolute_error: 0.3323\n",
            "Epoch 30/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2184 - mean_absolute_error: 0.3475 - val_loss: 0.2140 - val_mean_absolute_error: 0.3245\n",
            "Epoch 31/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2102 - mean_absolute_error: 0.3362 - val_loss: 0.2101 - val_mean_absolute_error: 0.3310\n",
            "Epoch 32/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2141 - mean_absolute_error: 0.3416 - val_loss: 0.2089 - val_mean_absolute_error: 0.3383\n",
            "Epoch 33/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2115 - mean_absolute_error: 0.3408 - val_loss: 0.2094 - val_mean_absolute_error: 0.3385\n",
            "Epoch 34/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2050 - mean_absolute_error: 0.3379 - val_loss: 0.2111 - val_mean_absolute_error: 0.3178\n",
            "Epoch 35/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2125 - mean_absolute_error: 0.3340 - val_loss: 0.2118 - val_mean_absolute_error: 0.3248\n",
            "Epoch 36/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2071 - mean_absolute_error: 0.3334 - val_loss: 0.2106 - val_mean_absolute_error: 0.3309\n",
            "Epoch 37/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2091 - mean_absolute_error: 0.3352 - val_loss: 0.2134 - val_mean_absolute_error: 0.3322\n",
            "Epoch 38/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2128 - mean_absolute_error: 0.3400 - val_loss: 0.2097 - val_mean_absolute_error: 0.3315\n",
            "Epoch 39/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2027 - mean_absolute_error: 0.3278 - val_loss: 0.2109 - val_mean_absolute_error: 0.3318\n",
            "Epoch 40/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2055 - mean_absolute_error: 0.3338 - val_loss: 0.2099 - val_mean_absolute_error: 0.3275\n",
            "Epoch 41/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2050 - mean_absolute_error: 0.3357 - val_loss: 0.2090 - val_mean_absolute_error: 0.3330\n",
            "Epoch 42/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2111 - mean_absolute_error: 0.3386 - val_loss: 0.2116 - val_mean_absolute_error: 0.3379\n",
            "Epoch 43/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2102 - mean_absolute_error: 0.3397 - val_loss: 0.2140 - val_mean_absolute_error: 0.3346\n",
            "Epoch 44/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2068 - mean_absolute_error: 0.3344 - val_loss: 0.2114 - val_mean_absolute_error: 0.3281\n",
            "Epoch 45/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2115 - mean_absolute_error: 0.3410 - val_loss: 0.2118 - val_mean_absolute_error: 0.3331\n",
            "Epoch 46/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2123 - mean_absolute_error: 0.3399 - val_loss: 0.2113 - val_mean_absolute_error: 0.3191\n",
            "Epoch 47/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2096 - mean_absolute_error: 0.3347 - val_loss: 0.2117 - val_mean_absolute_error: 0.3358\n",
            "Epoch 48/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2136 - mean_absolute_error: 0.3365 - val_loss: 0.2129 - val_mean_absolute_error: 0.3352\n",
            "Epoch 49/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2001 - mean_absolute_error: 0.3262 - val_loss: 0.2119 - val_mean_absolute_error: 0.3441\n",
            "Epoch 50/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2034 - mean_absolute_error: 0.3326 - val_loss: 0.2092 - val_mean_absolute_error: 0.3261\n",
            "Validation MAE: 0.3261\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3261\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Neural Network predictions with bins saved to 'submission_nn_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target (continuous)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Optimized Neural Network Function\n",
        "def build_optimized_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),  # Reduced dropout for improved learning\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_optimized_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Add callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,  # Train up to 100 epochs with early stopping\n",
        "    batch_size=64,  # Larger batch size for faster computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2CNRFPqEI7",
        "outputId": "7295acf3-79e3-43e4-b31e-3335d698126b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.9423 - mean_absolute_error: 0.7227 - val_loss: 0.2957 - val_mean_absolute_error: 0.4530 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.3369 - mean_absolute_error: 0.4493 - val_loss: 0.2499 - val_mean_absolute_error: 0.3928 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2928 - mean_absolute_error: 0.4120 - val_loss: 0.2306 - val_mean_absolute_error: 0.3753 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2637 - mean_absolute_error: 0.3889 - val_loss: 0.2219 - val_mean_absolute_error: 0.3649 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2563 - mean_absolute_error: 0.3815 - val_loss: 0.2211 - val_mean_absolute_error: 0.3559 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2429 - mean_absolute_error: 0.3714 - val_loss: 0.2246 - val_mean_absolute_error: 0.3483 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2380 - mean_absolute_error: 0.3643 - val_loss: 0.2161 - val_mean_absolute_error: 0.3426 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2359 - mean_absolute_error: 0.3630 - val_loss: 0.2198 - val_mean_absolute_error: 0.3363 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2371 - mean_absolute_error: 0.3583 - val_loss: 0.2183 - val_mean_absolute_error: 0.3441 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2367 - mean_absolute_error: 0.3611 - val_loss: 0.2165 - val_mean_absolute_error: 0.3385 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2243 - mean_absolute_error: 0.3524 - val_loss: 0.2201 - val_mean_absolute_error: 0.3312 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m150/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2258 - mean_absolute_error: 0.3499\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2257 - mean_absolute_error: 0.3500 - val_loss: 0.2198 - val_mean_absolute_error: 0.3303 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2247 - mean_absolute_error: 0.3498 - val_loss: 0.2150 - val_mean_absolute_error: 0.3343 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2231 - mean_absolute_error: 0.3472 - val_loss: 0.2160 - val_mean_absolute_error: 0.3413 - learning_rate: 5.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2181 - mean_absolute_error: 0.3500 - val_loss: 0.2125 - val_mean_absolute_error: 0.3370 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2228 - mean_absolute_error: 0.3500 - val_loss: 0.2158 - val_mean_absolute_error: 0.3296 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2172 - mean_absolute_error: 0.3429 - val_loss: 0.2131 - val_mean_absolute_error: 0.3459 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2245 - mean_absolute_error: 0.3534 - val_loss: 0.2141 - val_mean_absolute_error: 0.3366 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2281 - mean_absolute_error: 0.3532 - val_loss: 0.2141 - val_mean_absolute_error: 0.3316 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m146/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2200 - mean_absolute_error: 0.3465\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2197 - mean_absolute_error: 0.3463 - val_loss: 0.2153 - val_mean_absolute_error: 0.3377 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2106 - mean_absolute_error: 0.3394 - val_loss: 0.2126 - val_mean_absolute_error: 0.3345 - learning_rate: 2.5000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2118 - mean_absolute_error: 0.3387 - val_loss: 0.2137 - val_mean_absolute_error: 0.3336 - learning_rate: 2.5000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2116 - mean_absolute_error: 0.3394 - val_loss: 0.2133 - val_mean_absolute_error: 0.3346 - learning_rate: 2.5000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2134 - mean_absolute_error: 0.3413 - val_loss: 0.2128 - val_mean_absolute_error: 0.3333 - learning_rate: 2.5000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m155/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2082 - mean_absolute_error: 0.3326\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2083 - mean_absolute_error: 0.3327 - val_loss: 0.2126 - val_mean_absolute_error: 0.3311 - learning_rate: 2.5000e-04\n",
            "Epoch 25: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Validation MAE: 0.3370\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3370\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_2.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izknmyLOq9V4",
        "outputId": "90e5a132-603d-4ecd-8efd-89729cadf2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - loss: 0.7267 - mean_absolute_error: 0.6435 - val_loss: 0.2527 - val_mean_absolute_error: 0.3577 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.3204 - mean_absolute_error: 0.4369 - val_loss: 0.2531 - val_mean_absolute_error: 0.3610 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.2790 - mean_absolute_error: 0.3995 - val_loss: 0.2386 - val_mean_absolute_error: 0.3388 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2543 - mean_absolute_error: 0.3789 - val_loss: 0.2237 - val_mean_absolute_error: 0.3314 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2424 - mean_absolute_error: 0.3651 - val_loss: 0.2231 - val_mean_absolute_error: 0.3292 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2404 - mean_absolute_error: 0.3703 - val_loss: 0.2218 - val_mean_absolute_error: 0.3301 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.2375 - mean_absolute_error: 0.3672 - val_loss: 0.2172 - val_mean_absolute_error: 0.3361 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2352 - mean_absolute_error: 0.3611 - val_loss: 0.2150 - val_mean_absolute_error: 0.3347 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2253 - mean_absolute_error: 0.3532 - val_loss: 0.2165 - val_mean_absolute_error: 0.3275 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2306 - mean_absolute_error: 0.3540 - val_loss: 0.2143 - val_mean_absolute_error: 0.3353 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2261 - mean_absolute_error: 0.3511 - val_loss: 0.2137 - val_mean_absolute_error: 0.3271 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2182 - mean_absolute_error: 0.3499 - val_loss: 0.2141 - val_mean_absolute_error: 0.3283 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2139 - mean_absolute_error: 0.3431 - val_loss: 0.2142 - val_mean_absolute_error: 0.3313 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2168 - mean_absolute_error: 0.3438 - val_loss: 0.2159 - val_mean_absolute_error: 0.3238 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2200 - mean_absolute_error: 0.3454 - val_loss: 0.2132 - val_mean_absolute_error: 0.3319 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2124 - mean_absolute_error: 0.3394 - val_loss: 0.2209 - val_mean_absolute_error: 0.3313 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2152 - mean_absolute_error: 0.3418 - val_loss: 0.2136 - val_mean_absolute_error: 0.3267 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2132 - mean_absolute_error: 0.3394 - val_loss: 0.2184 - val_mean_absolute_error: 0.3370 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2197 - mean_absolute_error: 0.3487 - val_loss: 0.2114 - val_mean_absolute_error: 0.3304 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2222 - mean_absolute_error: 0.3527 - val_loss: 0.2173 - val_mean_absolute_error: 0.3273 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2078 - mean_absolute_error: 0.3357 - val_loss: 0.2124 - val_mean_absolute_error: 0.3441 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2069 - mean_absolute_error: 0.3370 - val_loss: 0.2141 - val_mean_absolute_error: 0.3332 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2151 - mean_absolute_error: 0.3418 - val_loss: 0.2188 - val_mean_absolute_error: 0.3226 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m315/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2125 - mean_absolute_error: 0.3409\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2125 - mean_absolute_error: 0.3409 - val_loss: 0.2175 - val_mean_absolute_error: 0.3297 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2082 - mean_absolute_error: 0.3341 - val_loss: 0.2113 - val_mean_absolute_error: 0.3244 - learning_rate: 2.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1953 - mean_absolute_error: 0.3243 - val_loss: 0.2113 - val_mean_absolute_error: 0.3240 - learning_rate: 2.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1984 - mean_absolute_error: 0.3253 - val_loss: 0.2121 - val_mean_absolute_error: 0.3266 - learning_rate: 2.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2062 - mean_absolute_error: 0.3327 - val_loss: 0.2124 - val_mean_absolute_error: 0.3212 - learning_rate: 2.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2043 - mean_absolute_error: 0.3268 - val_loss: 0.2125 - val_mean_absolute_error: 0.3223 - learning_rate: 2.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m308/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2055 - mean_absolute_error: 0.3277\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2054 - mean_absolute_error: 0.3278 - val_loss: 0.2117 - val_mean_absolute_error: 0.3288 - learning_rate: 2.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1958 - mean_absolute_error: 0.3262 - val_loss: 0.2115 - val_mean_absolute_error: 0.3252 - learning_rate: 4.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2099 - mean_absolute_error: 0.3371 - val_loss: 0.2118 - val_mean_absolute_error: 0.3242 - learning_rate: 4.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2009 - mean_absolute_error: 0.3257 - val_loss: 0.2119 - val_mean_absolute_error: 0.3245 - learning_rate: 4.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1985 - mean_absolute_error: 0.3256 - val_loss: 0.2122 - val_mean_absolute_error: 0.3230 - learning_rate: 4.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m316/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1944 - mean_absolute_error: 0.3219\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1944 - mean_absolute_error: 0.3219 - val_loss: 0.2123 - val_mean_absolute_error: 0.3221 - learning_rate: 4.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1997 - mean_absolute_error: 0.3269 - val_loss: 0.2123 - val_mean_absolute_error: 0.3220 - learning_rate: 8.0000e-06\n",
            "Epoch 37/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1961 - mean_absolute_error: 0.3209 - val_loss: 0.2121 - val_mean_absolute_error: 0.3222 - learning_rate: 8.0000e-06\n",
            "Epoch 38/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1978 - mean_absolute_error: 0.3220 - val_loss: 0.2121 - val_mean_absolute_error: 0.3220 - learning_rate: 8.0000e-06\n",
            "Epoch 39/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1991 - mean_absolute_error: 0.3271 - val_loss: 0.2121 - val_mean_absolute_error: 0.3227 - learning_rate: 8.0000e-06\n",
            "Epoch 40/200\n",
            "\u001b[1m316/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1937 - mean_absolute_error: 0.3216\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1937 - mean_absolute_error: 0.3216 - val_loss: 0.2122 - val_mean_absolute_error: 0.3224 - learning_rate: 8.0000e-06\n",
            "Epoch 41/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2001 - mean_absolute_error: 0.3270 - val_loss: 0.2121 - val_mean_absolute_error: 0.3228 - learning_rate: 1.6000e-06\n",
            "Epoch 41: early stopping\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "Validation MAE: 0.3240\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3240\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.23, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_test_size_changed.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_test_size_changed.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdIwlzxCrrpg",
        "outputId": "2e610dc7-867f-4e39-a8d3-a7a761ae1287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.6993 - mean_absolute_error: 0.6398 - val_loss: 0.2417 - val_mean_absolute_error: 0.3496 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3305 - mean_absolute_error: 0.4404 - val_loss: 0.2378 - val_mean_absolute_error: 0.3482 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2800 - mean_absolute_error: 0.4032 - val_loss: 0.2227 - val_mean_absolute_error: 0.3501 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2581 - mean_absolute_error: 0.3862 - val_loss: 0.2286 - val_mean_absolute_error: 0.3654 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2370 - mean_absolute_error: 0.3669 - val_loss: 0.2181 - val_mean_absolute_error: 0.3417 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2359 - mean_absolute_error: 0.3655 - val_loss: 0.2131 - val_mean_absolute_error: 0.3539 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2325 - mean_absolute_error: 0.3614 - val_loss: 0.2139 - val_mean_absolute_error: 0.3338 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2321 - mean_absolute_error: 0.3596 - val_loss: 0.2121 - val_mean_absolute_error: 0.3269 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2211 - mean_absolute_error: 0.3528 - val_loss: 0.2168 - val_mean_absolute_error: 0.3430 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2272 - mean_absolute_error: 0.3556 - val_loss: 0.2120 - val_mean_absolute_error: 0.3171 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2312 - mean_absolute_error: 0.3572 - val_loss: 0.2109 - val_mean_absolute_error: 0.3365 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2255 - mean_absolute_error: 0.3524 - val_loss: 0.2103 - val_mean_absolute_error: 0.3270 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2148 - mean_absolute_error: 0.3397 - val_loss: 0.2133 - val_mean_absolute_error: 0.3215 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2199 - mean_absolute_error: 0.3449 - val_loss: 0.2099 - val_mean_absolute_error: 0.3229 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2223 - mean_absolute_error: 0.3475 - val_loss: 0.2108 - val_mean_absolute_error: 0.3207 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2194 - mean_absolute_error: 0.3420 - val_loss: 0.2124 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2126 - mean_absolute_error: 0.3418 - val_loss: 0.2127 - val_mean_absolute_error: 0.3159 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2236 - mean_absolute_error: 0.3488 - val_loss: 0.2124 - val_mean_absolute_error: 0.3266 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m297/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2212 - mean_absolute_error: 0.3478\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2211 - mean_absolute_error: 0.3477 - val_loss: 0.2122 - val_mean_absolute_error: 0.3233 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2153 - mean_absolute_error: 0.3415 - val_loss: 0.2124 - val_mean_absolute_error: 0.3156 - learning_rate: 2.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2045 - mean_absolute_error: 0.3308 - val_loss: 0.2091 - val_mean_absolute_error: 0.3178 - learning_rate: 2.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2080 - mean_absolute_error: 0.3336 - val_loss: 0.2105 - val_mean_absolute_error: 0.3161 - learning_rate: 2.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2121 - mean_absolute_error: 0.3371 - val_loss: 0.2092 - val_mean_absolute_error: 0.3217 - learning_rate: 2.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1998 - mean_absolute_error: 0.3277 - val_loss: 0.2099 - val_mean_absolute_error: 0.3217 - learning_rate: 2.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2044 - mean_absolute_error: 0.3349 - val_loss: 0.2091 - val_mean_absolute_error: 0.3197 - learning_rate: 2.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m299/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2033 - mean_absolute_error: 0.3291\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2033 - mean_absolute_error: 0.3292 - val_loss: 0.2104 - val_mean_absolute_error: 0.3198 - learning_rate: 2.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.2053 - mean_absolute_error: 0.3302 - val_loss: 0.2089 - val_mean_absolute_error: 0.3187 - learning_rate: 4.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2025 - mean_absolute_error: 0.3308 - val_loss: 0.2097 - val_mean_absolute_error: 0.3182 - learning_rate: 4.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1996 - mean_absolute_error: 0.3268 - val_loss: 0.2097 - val_mean_absolute_error: 0.3179 - learning_rate: 4.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2087 - mean_absolute_error: 0.3372 - val_loss: 0.2102 - val_mean_absolute_error: 0.3156 - learning_rate: 4.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2011 - mean_absolute_error: 0.3247 - val_loss: 0.2096 - val_mean_absolute_error: 0.3172 - learning_rate: 4.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m302/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2037 - mean_absolute_error: 0.3291\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2037 - mean_absolute_error: 0.3291 - val_loss: 0.2099 - val_mean_absolute_error: 0.3163 - learning_rate: 4.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2029 - mean_absolute_error: 0.3294 - val_loss: 0.2100 - val_mean_absolute_error: 0.3172 - learning_rate: 8.0000e-06\n",
            "Epoch 34/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2027 - mean_absolute_error: 0.3298 - val_loss: 0.2097 - val_mean_absolute_error: 0.3167 - learning_rate: 8.0000e-06\n",
            "Epoch 35/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2049 - mean_absolute_error: 0.3333 - val_loss: 0.2097 - val_mean_absolute_error: 0.3171 - learning_rate: 8.0000e-06\n",
            "Epoch 36/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1973 - mean_absolute_error: 0.3240 - val_loss: 0.2099 - val_mean_absolute_error: 0.3173 - learning_rate: 8.0000e-06\n",
            "Epoch 37/200\n",
            "\u001b[1m304/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2004 - mean_absolute_error: 0.3280\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2004 - mean_absolute_error: 0.3280 - val_loss: 0.2098 - val_mean_absolute_error: 0.3169 - learning_rate: 8.0000e-06\n",
            "Epoch 38/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2021 - mean_absolute_error: 0.3306 - val_loss: 0.2098 - val_mean_absolute_error: 0.3170 - learning_rate: 1.6000e-06\n",
            "Epoch 39/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2052 - mean_absolute_error: 0.3304 - val_loss: 0.2098 - val_mean_absolute_error: 0.3170 - learning_rate: 1.6000e-06\n",
            "Epoch 40/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2054 - mean_absolute_error: 0.3328 - val_loss: 0.2094 - val_mean_absolute_error: 0.3174 - learning_rate: 1.6000e-06\n",
            "Epoch 41/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2045 - mean_absolute_error: 0.3324 - val_loss: 0.2096 - val_mean_absolute_error: 0.3173 - learning_rate: 1.6000e-06\n",
            "Epoch 42/200\n",
            "\u001b[1m300/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2010 - mean_absolute_error: 0.3302\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2010 - mean_absolute_error: 0.3301 - val_loss: 0.2098 - val_mean_absolute_error: 0.3170 - learning_rate: 1.6000e-06\n",
            "Epoch 42: early stopping\n",
            "Restoring model weights from the end of the best epoch: 27.\n",
            "Validation MAE: 0.3187\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3187\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_test_size_changed.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Best Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Xavier initialization for linear output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_he.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_he.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf_uHC5XHSzk",
        "outputId": "b0fcb53c-32ea-42bc-c5cf-4456b6ea9d25"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 1.2529 - mean_absolute_error: 0.8255 - val_loss: 0.2871 - val_mean_absolute_error: 0.4145 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3873 - mean_absolute_error: 0.4816 - val_loss: 0.2441 - val_mean_absolute_error: 0.3748 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.3107 - mean_absolute_error: 0.4268 - val_loss: 0.2318 - val_mean_absolute_error: 0.3677 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2715 - mean_absolute_error: 0.3971 - val_loss: 0.2302 - val_mean_absolute_error: 0.3477 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.2614 - mean_absolute_error: 0.3848 - val_loss: 0.2210 - val_mean_absolute_error: 0.3254 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.2497 - mean_absolute_error: 0.3776 - val_loss: 0.2184 - val_mean_absolute_error: 0.3326 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2430 - mean_absolute_error: 0.3700 - val_loss: 0.2265 - val_mean_absolute_error: 0.3269 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2394 - mean_absolute_error: 0.3664 - val_loss: 0.2129 - val_mean_absolute_error: 0.3270 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2338 - mean_absolute_error: 0.3607 - val_loss: 0.2258 - val_mean_absolute_error: 0.3194 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2253 - mean_absolute_error: 0.3541 - val_loss: 0.2211 - val_mean_absolute_error: 0.3538 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2303 - mean_absolute_error: 0.3565 - val_loss: 0.2171 - val_mean_absolute_error: 0.3199 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2267 - mean_absolute_error: 0.3517 - val_loss: 0.2115 - val_mean_absolute_error: 0.3332 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2207 - mean_absolute_error: 0.3512 - val_loss: 0.2159 - val_mean_absolute_error: 0.3357 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2221 - mean_absolute_error: 0.3500 - val_loss: 0.2146 - val_mean_absolute_error: 0.3277 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2230 - mean_absolute_error: 0.3498 - val_loss: 0.2145 - val_mean_absolute_error: 0.3349 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2266 - mean_absolute_error: 0.3527 - val_loss: 0.2104 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2189 - mean_absolute_error: 0.3499 - val_loss: 0.2231 - val_mean_absolute_error: 0.3294 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2262 - mean_absolute_error: 0.3560 - val_loss: 0.2158 - val_mean_absolute_error: 0.3319 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2116 - mean_absolute_error: 0.3404 - val_loss: 0.2128 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2164 - mean_absolute_error: 0.3491 - val_loss: 0.2113 - val_mean_absolute_error: 0.3245 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m301/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2181 - mean_absolute_error: 0.3451\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2182 - mean_absolute_error: 0.3452 - val_loss: 0.2132 - val_mean_absolute_error: 0.3262 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2142 - mean_absolute_error: 0.3418 - val_loss: 0.2104 - val_mean_absolute_error: 0.3263 - learning_rate: 2.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2157 - mean_absolute_error: 0.3442 - val_loss: 0.2097 - val_mean_absolute_error: 0.3237 - learning_rate: 2.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2113 - mean_absolute_error: 0.3397 - val_loss: 0.2099 - val_mean_absolute_error: 0.3205 - learning_rate: 2.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2174 - mean_absolute_error: 0.3442 - val_loss: 0.2112 - val_mean_absolute_error: 0.3161 - learning_rate: 2.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2144 - mean_absolute_error: 0.3398 - val_loss: 0.2114 - val_mean_absolute_error: 0.3170 - learning_rate: 2.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2116 - mean_absolute_error: 0.3389 - val_loss: 0.2101 - val_mean_absolute_error: 0.3262 - learning_rate: 2.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m315/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2115 - mean_absolute_error: 0.3451\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2114 - mean_absolute_error: 0.3450 - val_loss: 0.2119 - val_mean_absolute_error: 0.3193 - learning_rate: 2.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2098 - mean_absolute_error: 0.3360 - val_loss: 0.2100 - val_mean_absolute_error: 0.3207 - learning_rate: 4.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2130 - mean_absolute_error: 0.3389 - val_loss: 0.2105 - val_mean_absolute_error: 0.3181 - learning_rate: 4.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2004 - mean_absolute_error: 0.3271 - val_loss: 0.2101 - val_mean_absolute_error: 0.3208 - learning_rate: 4.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2135 - mean_absolute_error: 0.3411 - val_loss: 0.2107 - val_mean_absolute_error: 0.3195 - learning_rate: 4.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m313/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2067 - mean_absolute_error: 0.3319\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2067 - mean_absolute_error: 0.3320 - val_loss: 0.2100 - val_mean_absolute_error: 0.3211 - learning_rate: 4.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2076 - mean_absolute_error: 0.3332 - val_loss: 0.2101 - val_mean_absolute_error: 0.3202 - learning_rate: 8.0000e-06\n",
            "Epoch 35/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2126 - mean_absolute_error: 0.3414 - val_loss: 0.2103 - val_mean_absolute_error: 0.3197 - learning_rate: 8.0000e-06\n",
            "Epoch 36/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2036 - mean_absolute_error: 0.3310 - val_loss: 0.2102 - val_mean_absolute_error: 0.3200 - learning_rate: 8.0000e-06\n",
            "Epoch 37/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2051 - mean_absolute_error: 0.3319 - val_loss: 0.2104 - val_mean_absolute_error: 0.3190 - learning_rate: 8.0000e-06\n",
            "Epoch 38/200\n",
            "\u001b[1m304/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2039 - mean_absolute_error: 0.3352\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2040 - mean_absolute_error: 0.3352 - val_loss: 0.2102 - val_mean_absolute_error: 0.3192 - learning_rate: 8.0000e-06\n",
            "Epoch 38: early stopping\n",
            "Restoring model weights from the end of the best epoch: 23.\n",
            "Validation MAE: 0.3237\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3237\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_he.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),  # Increased dropout for robustness\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Linear for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=150,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=64,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_3.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3nEduOeHeCK",
        "outputId": "d7b150bd-84d3-4dcd-c7a0-7afcaf686c0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - loss: 1.4066 - mean_absolute_error: 0.8577 - val_loss: 0.4432 - val_mean_absolute_error: 0.4340 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.4999 - mean_absolute_error: 0.4993 - val_loss: 0.3830 - val_mean_absolute_error: 0.3862 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.4073 - mean_absolute_error: 0.4370 - val_loss: 0.3309 - val_mean_absolute_error: 0.3381 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.3674 - mean_absolute_error: 0.4030 - val_loss: 0.3131 - val_mean_absolute_error: 0.3299 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3407 - mean_absolute_error: 0.3826 - val_loss: 0.3091 - val_mean_absolute_error: 0.3316 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.3266 - mean_absolute_error: 0.3728 - val_loss: 0.3054 - val_mean_absolute_error: 0.3245 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.3233 - mean_absolute_error: 0.3683 - val_loss: 0.2972 - val_mean_absolute_error: 0.3255 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.3288 - mean_absolute_error: 0.3778 - val_loss: 0.3020 - val_mean_absolute_error: 0.3223 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3166 - mean_absolute_error: 0.3622 - val_loss: 0.2981 - val_mean_absolute_error: 0.3221 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3096 - mean_absolute_error: 0.3579 - val_loss: 0.2886 - val_mean_absolute_error: 0.3327 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3068 - mean_absolute_error: 0.3590 - val_loss: 0.2883 - val_mean_absolute_error: 0.3254 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3051 - mean_absolute_error: 0.3599 - val_loss: 0.2960 - val_mean_absolute_error: 0.3295 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2971 - mean_absolute_error: 0.3544 - val_loss: 0.2848 - val_mean_absolute_error: 0.3209 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2918 - mean_absolute_error: 0.3493 - val_loss: 0.2856 - val_mean_absolute_error: 0.3181 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.2963 - mean_absolute_error: 0.3574 - val_loss: 0.2852 - val_mean_absolute_error: 0.3202 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2863 - mean_absolute_error: 0.3472 - val_loss: 0.2776 - val_mean_absolute_error: 0.3204 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2819 - mean_absolute_error: 0.3479 - val_loss: 0.2768 - val_mean_absolute_error: 0.3188 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2845 - mean_absolute_error: 0.3499 - val_loss: 0.2747 - val_mean_absolute_error: 0.3157 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2800 - mean_absolute_error: 0.3451 - val_loss: 0.2729 - val_mean_absolute_error: 0.3214 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2708 - mean_absolute_error: 0.3417 - val_loss: 0.2756 - val_mean_absolute_error: 0.3276 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2785 - mean_absolute_error: 0.3475 - val_loss: 0.2693 - val_mean_absolute_error: 0.3310 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2783 - mean_absolute_error: 0.3575 - val_loss: 0.2694 - val_mean_absolute_error: 0.3295 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2695 - mean_absolute_error: 0.3475 - val_loss: 0.2628 - val_mean_absolute_error: 0.3154 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2748 - mean_absolute_error: 0.3515 - val_loss: 0.2588 - val_mean_absolute_error: 0.3305 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.2644 - mean_absolute_error: 0.3467 - val_loss: 0.2576 - val_mean_absolute_error: 0.3227 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2649 - mean_absolute_error: 0.3437 - val_loss: 0.2587 - val_mean_absolute_error: 0.3294 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.2620 - mean_absolute_error: 0.3449 - val_loss: 0.2567 - val_mean_absolute_error: 0.3285 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2576 - mean_absolute_error: 0.3418 - val_loss: 0.2541 - val_mean_absolute_error: 0.3225 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2542 - mean_absolute_error: 0.3388 - val_loss: 0.2512 - val_mean_absolute_error: 0.3346 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2570 - mean_absolute_error: 0.3441 - val_loss: 0.2512 - val_mean_absolute_error: 0.3235 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2546 - mean_absolute_error: 0.3421 - val_loss: 0.2476 - val_mean_absolute_error: 0.3310 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2409 - mean_absolute_error: 0.3345 - val_loss: 0.2506 - val_mean_absolute_error: 0.3244 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2492 - mean_absolute_error: 0.3390 - val_loss: 0.2472 - val_mean_absolute_error: 0.3245 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2503 - mean_absolute_error: 0.3415 - val_loss: 0.2442 - val_mean_absolute_error: 0.3245 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2526 - mean_absolute_error: 0.3420 - val_loss: 0.2421 - val_mean_absolute_error: 0.3213 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2411 - mean_absolute_error: 0.3365 - val_loss: 0.2417 - val_mean_absolute_error: 0.3176 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2490 - mean_absolute_error: 0.3404 - val_loss: 0.2438 - val_mean_absolute_error: 0.3254 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2421 - mean_absolute_error: 0.3367 - val_loss: 0.2394 - val_mean_absolute_error: 0.3242 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2419 - mean_absolute_error: 0.3456 - val_loss: 0.2373 - val_mean_absolute_error: 0.3272 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.2340 - mean_absolute_error: 0.3340 - val_loss: 0.2375 - val_mean_absolute_error: 0.3267 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2421 - mean_absolute_error: 0.3394 - val_loss: 0.2374 - val_mean_absolute_error: 0.3306 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2412 - mean_absolute_error: 0.3402 - val_loss: 0.2396 - val_mean_absolute_error: 0.3322 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2342 - mean_absolute_error: 0.3332 - val_loss: 0.2355 - val_mean_absolute_error: 0.3207 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2335 - mean_absolute_error: 0.3344 - val_loss: 0.2354 - val_mean_absolute_error: 0.3257 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2345 - mean_absolute_error: 0.3372 - val_loss: 0.2367 - val_mean_absolute_error: 0.3188 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2426 - mean_absolute_error: 0.3403 - val_loss: 0.2332 - val_mean_absolute_error: 0.3241 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2269 - mean_absolute_error: 0.3279 - val_loss: 0.2357 - val_mean_absolute_error: 0.3369 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2310 - mean_absolute_error: 0.3364 - val_loss: 0.2335 - val_mean_absolute_error: 0.3343 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2361 - mean_absolute_error: 0.3424 - val_loss: 0.2351 - val_mean_absolute_error: 0.3246 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2306 - mean_absolute_error: 0.3347 - val_loss: 0.2352 - val_mean_absolute_error: 0.3261 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2283 - mean_absolute_error: 0.3296 - val_loss: 0.2307 - val_mean_absolute_error: 0.3272 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2299 - mean_absolute_error: 0.3351 - val_loss: 0.2304 - val_mean_absolute_error: 0.3236 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2214 - mean_absolute_error: 0.3265 - val_loss: 0.2306 - val_mean_absolute_error: 0.3227 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2197 - mean_absolute_error: 0.3247 - val_loss: 0.2305 - val_mean_absolute_error: 0.3245 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2268 - mean_absolute_error: 0.3307 - val_loss: 0.2307 - val_mean_absolute_error: 0.3226 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2143 - mean_absolute_error: 0.3207 - val_loss: 0.2310 - val_mean_absolute_error: 0.3257 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/150\n",
            "\u001b[1m152/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2210 - mean_absolute_error: 0.3281\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2210 - mean_absolute_error: 0.3281 - val_loss: 0.2310 - val_mean_absolute_error: 0.3223 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2198 - mean_absolute_error: 0.3246 - val_loss: 0.2311 - val_mean_absolute_error: 0.3215 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2197 - mean_absolute_error: 0.3246 - val_loss: 0.2310 - val_mean_absolute_error: 0.3228 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.2193 - mean_absolute_error: 0.3247 - val_loss: 0.2309 - val_mean_absolute_error: 0.3232 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2172 - mean_absolute_error: 0.3259 - val_loss: 0.2313 - val_mean_absolute_error: 0.3237 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/150\n",
            "\u001b[1m153/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2235 - mean_absolute_error: 0.3268\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2232 - mean_absolute_error: 0.3267 - val_loss: 0.2312 - val_mean_absolute_error: 0.3214 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2194 - mean_absolute_error: 0.3227 - val_loss: 0.2313 - val_mean_absolute_error: 0.3227 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2142 - mean_absolute_error: 0.3189 - val_loss: 0.2311 - val_mean_absolute_error: 0.3244 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2270 - mean_absolute_error: 0.3304 - val_loss: 0.2316 - val_mean_absolute_error: 0.3215 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2203 - mean_absolute_error: 0.3250 - val_loss: 0.2311 - val_mean_absolute_error: 0.3215 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/150\n",
            "\u001b[1m151/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2218 - mean_absolute_error: 0.3228\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2217 - mean_absolute_error: 0.3228 - val_loss: 0.2311 - val_mean_absolute_error: 0.3224 - learning_rate: 5.0000e-05\n",
            "Epoch 67: early stopping\n",
            "Restoring model weights from the end of the best epoch: 52.\n",
            "Validation MAE: 0.3236\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3236\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlL3iH3hI611",
        "outputId": "1671c331-18ac-4420-9d88-fae482e380cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.6 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna-integration[tfkeras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rAMJ_uNJCTp",
        "outputId": "f560047a-6e08-4886-ac6a-a94cf2cc8a19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-integration[tfkeras]\n",
            "  Downloading optuna_integration-4.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (4.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (2.17.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.37.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[tfkeras]) (1.3.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->optuna-integration[tfkeras]) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[tfkeras]) (3.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.1.2)\n",
            "Downloading optuna_integration-4.1.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    n_units_1 = trial.suggest_int('n_units_1', 128, 512, step=64)  # Layer 1 units\n",
        "    n_units_2 = trial.suggest_int('n_units_2', 64, 256, step=32)   # Layer 2 units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True)   # L2 regularization\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16) # Batch size\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),\n",
        "        Dense(n_units_1, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_units_2, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Regression output\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[TFKerasPruningCallback(trial, 'val_loss')],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return val_mae  # Minimize MAE\n",
        "\n",
        "# Run the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Output the best hyperparameters\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Train the final model with best hyperparameters\n",
        "best_params = study.best_params\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(best_params['n_units_1'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(best_params['n_units_2'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(1, activation='linear', kernel_initializer='glorot_uniform')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the optimized model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the optimized model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Optimized Model Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "y_val_pred = model.predict(X_val)\n",
        "test_predictions = model.predict(scaler.transform(test_df_encoded))\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "})\n",
        "results_df.to_csv('submission_nn_optimized_optuna.csv', index=False)\n",
        "print(\"Predictions saved to 'submission_nn_optimized_optuna.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8ToLVaINgB",
        "outputId": "824ca716-2a1f-4240-a6a7-18495c01a464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-28 05:09:08,114] A new study created in memory with name: no-name-d4f77e75-4418-4302-b660-1fd1e30a72c1\n",
            "[I 2024-11-28 05:09:44,957] Trial 0 finished with value: 0.3428192138671875 and parameters: {'n_units_1': 512, 'n_units_2': 224, 'dropout_rate': 0.30000000000000004, 'l2_reg': 0.0009612830523963407, 'learning_rate': 0.0003695540570307106, 'batch_size': 112}. Best is trial 0 with value: 0.3428192138671875.\n",
            "[I 2024-11-28 05:10:08,751] Trial 1 finished with value: 0.3376930058002472 and parameters: {'n_units_1': 384, 'n_units_2': 96, 'dropout_rate': 0.2, 'l2_reg': 0.0005416555466629026, 'learning_rate': 0.00034288290055904853, 'batch_size': 96}. Best is trial 1 with value: 0.3376930058002472.\n",
            "[I 2024-11-28 05:10:31,640] Trial 2 finished with value: 0.34121254086494446 and parameters: {'n_units_1': 512, 'n_units_2': 160, 'dropout_rate': 0.4, 'l2_reg': 1.0300315275292269e-05, 'learning_rate': 0.00033122767660977035, 'batch_size': 96}. Best is trial 1 with value: 0.3376930058002472.\n",
            "[I 2024-11-28 05:11:05,319] Trial 3 finished with value: 0.3361949026584625 and parameters: {'n_units_1': 128, 'n_units_2': 128, 'dropout_rate': 0.5, 'l2_reg': 0.0007940494547879258, 'learning_rate': 0.0003935394209012036, 'batch_size': 64}. Best is trial 3 with value: 0.3361949026584625.\n",
            "[I 2024-11-28 05:11:43,632] Trial 4 finished with value: 0.34464889764785767 and parameters: {'n_units_1': 192, 'n_units_2': 160, 'dropout_rate': 0.30000000000000004, 'l2_reg': 0.0005521152985390827, 'learning_rate': 0.00010576636439951068, 'batch_size': 48}. Best is trial 3 with value: 0.3361949026584625.\n",
            "[I 2024-11-28 05:12:34,991] Trial 5 finished with value: 0.3375972807407379 and parameters: {'n_units_1': 128, 'n_units_2': 64, 'dropout_rate': 0.2, 'l2_reg': 0.0001002765672695575, 'learning_rate': 0.004309454834125598, 'batch_size': 32}. Best is trial 3 with value: 0.3361949026584625.\n",
            "[I 2024-11-28 05:13:01,091] Trial 6 finished with value: 0.3379319906234741 and parameters: {'n_units_1': 448, 'n_units_2': 192, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.83325675424722e-05, 'learning_rate': 0.0004240425005889459, 'batch_size': 128}. Best is trial 3 with value: 0.3361949026584625.\n",
            "[I 2024-11-28 05:13:23,928] Trial 7 finished with value: 0.35423362255096436 and parameters: {'n_units_1': 192, 'n_units_2': 96, 'dropout_rate': 0.5, 'l2_reg': 6.43328168560843e-05, 'learning_rate': 0.000510396702632131, 'batch_size': 96}. Best is trial 3 with value: 0.3361949026584625.\n",
            "[I 2024-11-28 05:13:32,849] Trial 8 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:13:48,141] Trial 9 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-11-28 05:14:21,482] Trial 10 finished with value: 0.3218255639076233 and parameters: {'n_units_1': 256, 'n_units_2': 256, 'dropout_rate': 0.4, 'l2_reg': 0.00017213585638857593, 'learning_rate': 0.008549734341079515, 'batch_size': 64}. Best is trial 10 with value: 0.3218255639076233.\n",
            "[I 2024-11-28 05:14:27,530] Trial 11 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-11-28 05:14:39,090] Trial 12 pruned. Trial was pruned at epoch 4.\n",
            "[I 2024-11-28 05:14:54,111] Trial 13 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:15:18,507] Trial 14 finished with value: 0.3306784927845001 and parameters: {'n_units_1': 320, 'n_units_2': 128, 'dropout_rate': 0.4, 'l2_reg': 0.00023874641993860596, 'learning_rate': 0.009541141491996816, 'batch_size': 80}. Best is trial 10 with value: 0.3218255639076233.\n",
            "[I 2024-11-28 05:15:26,127] Trial 15 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:15:35,342] Trial 16 pruned. Trial was pruned at epoch 7.\n",
            "[I 2024-11-28 05:15:49,390] Trial 17 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-11-28 05:16:01,751] Trial 18 pruned. Trial was pruned at epoch 12.\n",
            "[I 2024-11-28 05:16:09,798] Trial 19 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-11-28 05:16:18,562] Trial 20 pruned. Trial was pruned at epoch 4.\n",
            "[I 2024-11-28 05:16:24,973] Trial 21 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:16:32,308] Trial 22 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:16:43,889] Trial 23 pruned. Trial was pruned at epoch 14.\n",
            "[I 2024-11-28 05:16:51,432] Trial 24 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:17:38,161] Trial 25 finished with value: 0.34520313143730164 and parameters: {'n_units_1': 320, 'n_units_2': 128, 'dropout_rate': 0.4, 'l2_reg': 2.527180919293691e-05, 'learning_rate': 0.000616129320683873, 'batch_size': 32}. Best is trial 10 with value: 0.3218255639076233.\n",
            "[I 2024-11-28 05:17:50,505] Trial 26 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-11-28 05:17:59,250] Trial 27 pruned. Trial was pruned at epoch 3.\n",
            "[I 2024-11-28 05:18:05,961] Trial 28 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:18:18,074] Trial 29 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:18:29,821] Trial 30 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-11-28 05:20:07,678] Trial 31 finished with value: 0.3180537819862366 and parameters: {'n_units_1': 128, 'n_units_2': 64, 'dropout_rate': 0.2, 'l2_reg': 9.357376776002758e-05, 'learning_rate': 0.005000038438440505, 'batch_size': 16}. Best is trial 31 with value: 0.3180537819862366.\n",
            "[I 2024-11-28 05:21:08,182] Trial 32 finished with value: 0.3735964298248291 and parameters: {'n_units_1': 128, 'n_units_2': 64, 'dropout_rate': 0.2, 'l2_reg': 9.206936208592156e-05, 'learning_rate': 0.009949219462664765, 'batch_size': 32}. Best is trial 31 with value: 0.3180537819862366.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tmisnh0vI4Ad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}