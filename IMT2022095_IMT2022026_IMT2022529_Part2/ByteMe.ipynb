{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OcFhvpILfPsY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "35IusksEfjH7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['category'].fillna(train_df['category'].median(), inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N1zHYbUfkVI",
        "outputId": "ccd579e8-5dc8-4ff1-c57a-2669582b2231"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e96754ba7e74>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['category'].fillna(train_df['category'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns = ['trip_ID','travelling_with','trip_purpose','first_time_visitor','source_of_info','weather_at_arrival','tour_arrangement','special_requirements'])\n",
        "test_df = test_df.drop(columns = ['trip_ID','travelling_with','trip_purpose','first_time_visitor','source_of_info','weather_at_arrival','tour_arrangement','special_requirements'])"
      ],
      "metadata": {
        "id": "4NFXKt3hfl6C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_nights'] = train_df['mainland_nights'] + train_df['island_nights']\n",
        "test_df['total_nights'] = test_df['mainland_nights'] + test_df['island_nights']"
      ],
      "metadata": {
        "id": "8a33LKcRfnUK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "\n",
        "# Function to apply target encoding with K-Fold\n",
        "def target_encode_column(train_df, test_df, target, col, n_splits=5):\n",
        "    # Create Series to store the target-encoded values for train and test\n",
        "    train_encoded = pd.Series(index=train_df.index, dtype='float64')\n",
        "    test_encoded = pd.Series(index=test_df.index, dtype='float64')\n",
        "\n",
        "    # Set up K-Fold for target encoding on the train set\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(train_df):\n",
        "        # Train and validation folds\n",
        "        train_fold, valid_fold = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n",
        "\n",
        "        # Calculate target mean per category in train fold\n",
        "        means = train_fold.groupby(col)[target].mean()\n",
        "\n",
        "        # Map these means to the validation fold\n",
        "        train_encoded.iloc[valid_idx] = valid_fold[col].map(means)\n",
        "\n",
        "    # Fill missing values in train_encoded with the overall target mean\n",
        "    train_encoded.fillna(train_df[target].mean(), inplace=True)\n",
        "\n",
        "    # Map the target encoding means to the test set\n",
        "    category_means = train_df.groupby(col)[target].mean()\n",
        "    test_encoded = test_df[col].map(category_means)\n",
        "\n",
        "    # Fill missing values in test_encoded with the overall target mean\n",
        "    test_encoded.fillna(train_df[target].mean(), inplace=True)\n",
        "\n",
        "    return train_encoded, test_encoded\n",
        "\n",
        "# Apply target encoding on 'visitor_nation'\n",
        "train_df['visitor_nation_encoded'], test_df['visitor_nation_encoded'] = target_encode_column(train_df, test_df, target='category', col='visitor_nation')\n",
        "\n",
        "# Drop the original 'visitor_nation' column if not needed further\n",
        "train_df = train_df.drop(columns=['visitor_nation'])\n",
        "test_df = test_df.drop(columns=['visitor_nation'])\n",
        "\n",
        "# Display first few rows to verify the encoding\n",
        "train_df[['visitor_nation_encoded', 'category']].head(), test_df[['visitor_nation_encoded']].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm-47e3sfomt",
        "outputId": "71fcc2e3-944c-4f56-ba06-140ab912a065"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   visitor_nation_encoded  category\n",
              " 0                0.270869       1.0\n",
              " 1                1.253133       2.0\n",
              " 2                0.696833       2.0\n",
              " 3                0.282976       0.0\n",
              " 4                0.284483       0.0,\n",
              "    visitor_nation_encoded\n",
              " 0                1.500000\n",
              " 1                0.436170\n",
              " 2                0.500000\n",
              " 3                0.823529\n",
              " 4                0.428986)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_people'] = train_df['female_count'] + train_df['male_count']\n",
        "test_df['total_people'] = test_df['female_count'] + test_df['male_count']"
      ],
      "metadata": {
        "id": "WniEfkbffqZv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Average group size\n",
        "train_df['average_group_size'] = train_df['total_people'] / train_df['total_nights']\n",
        "test_df['average_group_size'] = test_df['total_people'] / test_df['total_nights']\n",
        "\n",
        "# 2. Trip duration categories\n",
        "train_df['trip_duration_category'] = pd.cut(train_df['total_nights'], bins=[0, 3, 7, float('inf')], labels=['short', 'medium', 'long'])\n",
        "test_df['trip_duration_category'] = pd.cut(test_df['total_nights'], bins=[0, 3, 7, float('inf')], labels=['short', 'medium', 'long'])\n",
        "\n",
        "# 3. Visitor demographics (family and group indicators)\n",
        "train_df['is_family'] = ((train_df['female_count'] > 0) & (train_df['male_count'] > 0)).astype(int)\n",
        "test_df['is_family'] = ((test_df['female_count'] > 0) & (test_df['male_count'] > 0)).astype(int)\n",
        "\n",
        "train_df['is_group'] = (train_df['total_people'] > 2).astype(int)\n",
        "test_df['is_group'] = (test_df['total_people'] > 2).astype(int)\n",
        "\n",
        "# 4. Days in each type of location (mainland/island ratios)\n",
        "train_df['mainland_ratio'] = train_df['mainland_nights'] / train_df['total_nights']\n",
        "train_df['island_ratio'] = train_df['island_nights'] / train_df['total_nights']\n",
        "\n",
        "test_df['mainland_ratio'] = test_df['mainland_nights'] / test_df['total_nights']\n",
        "test_df['island_ratio'] = test_df['island_nights'] / test_df['total_nights']"
      ],
      "metadata": {
        "id": "vV0aoFVTfshI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns = ['female_count','male_count','mainland_nights','island_nights'])\n",
        "test_df = test_df.drop(columns = ['female_count','male_count','mainland_nights','island_nights'])"
      ],
      "metadata": {
        "id": "LCtpZlhAft54"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['key_activity'] = train_df['key_activity'].replace('Widlife Tourism', 'Wildlife Tourism')\n",
        "test_df['key_activity'] = test_df['key_activity'].replace('Widlife Tourism', 'Wildlife Tourism')"
      ],
      "metadata": {
        "id": "JbKMpNGMfvjB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_key_activity = train_df['key_activity'].mode()[0]\n",
        "\n",
        "train_df['key_activity'].fillna(mode_key_activity, inplace=True)\n",
        "test_df['key_activity'].fillna(mode_key_activity, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCu8C07Qfwv2",
        "outputId": "2dc01d33-e806-4c5d-cdd9-696aaf4fa58b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-a221c41d21a0>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['key_activity'].fillna(mode_key_activity, inplace=True)\n",
            "<ipython-input-11-a221c41d21a0>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['key_activity'].fillna(mode_key_activity, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define mappings for range values to approximate numeric values\n",
        "days_before_booked_map = {\n",
        "    '1-7': 4,\n",
        "    '8-14': 11,\n",
        "    '15-30': 22.5,\n",
        "    '31-60': 45.5,\n",
        "    '61-90': 75.5,\n",
        "    '90+': 90\n",
        "}\n",
        "\n",
        "tour_length_map = {\n",
        "    '1-6': 3.5,\n",
        "    '7-14': 10.5,\n",
        "    '15-29': 22,\n",
        "    '30+': 30\n",
        "}\n",
        "\n",
        "# Apply mappings to convert ranges to numeric values\n",
        "train_df['days_before_booked_num'] = train_df['days_before_booked'].map(days_before_booked_map)\n",
        "test_df['days_before_booked_num'] = test_df['days_before_booked'].map(days_before_booked_map)\n",
        "\n",
        "train_df['tour_length_num'] = train_df['tour_length'].map(tour_length_map)\n",
        "test_df['tour_length_num'] = test_df['tour_length'].map(tour_length_map)\n",
        "\n",
        "# Fill missing values with median of each column\n",
        "train_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
        "test_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
        "\n",
        "train_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
        "test_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
        "\n",
        "# Drop the original columns if not needed\n",
        "train_df = train_df.drop(columns=['days_before_booked', 'tour_length'])\n",
        "test_df = test_df.drop(columns=['days_before_booked', 'tour_length'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRX6UkT5f2YQ",
        "outputId": "2d0ce1ee-7934-4b70-812b-1376a0bf506d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0f46c9e2cb81>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['days_before_booked_num'].fillna(train_df['days_before_booked_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n",
            "<ipython-input-12-0f46c9e2cb81>:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['tour_length_num'].fillna(train_df['tour_length_num'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for column in ['transport_package_international', 'package_accomodation', 'food_package', 'insurance_package']:\n",
        "    most_frequent = train_df[column].mode()[0]\n",
        "    train_df[column].fillna(most_frequent, inplace=True)\n",
        "    test_df[column].fillna(most_frequent, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmtvt3Swf4P0",
        "outputId": "790d8986-6983-4d01-be53-42ec3c79c0c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a54ac0f19e68>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df[column].fillna(most_frequent, inplace=True)\n",
            "<ipython-input-13-a54ac0f19e68>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df[column].fillna(most_frequent, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
        "test_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
        "\n",
        "train_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n",
        "test_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSMgUwSLf7J8",
        "outputId": "8c72908f-d1c0-4d4b-cd41-c19b8661ce64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-758df23ed5c5>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['total_people'].fillna(train_df['total_people'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n",
            "<ipython-input-14-758df23ed5c5>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df['average_group_size'].fillna(train_df['average_group_size'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "Tuuy0_jcf-RM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "train_df['age_bracket'] = train_df['age_bracket'].replace({'<18': 'Below 25', '18-24': 'Below 25'})\n",
        "test_df['age_bracket'] = test_df['age_bracket'].replace({'<18': 'Below 25', '18-24': 'Below 25'})\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['age_bracket_encoded'] = label_encoder.fit_transform(train_df['age_bracket'])\n",
        "test_df['age_bracket_encoded'] = label_encoder.transform(test_df['age_bracket'])"
      ],
      "metadata": {
        "id": "fw8nxTmkgBNe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.drop(columns=['age_bracket'])\n",
        "test_df = test_df.drop(columns=['age_bracket'])"
      ],
      "metadata": {
        "id": "oYt-ndzsgDEY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_encoded = pd.get_dummies(train_df, drop_first=True,dtype='int64')"
      ],
      "metadata": {
        "id": "SiGNqNoTgEYR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_encoded = pd.get_dummies(test_df, drop_first=True,dtype='int64')"
      ],
      "metadata": {
        "id": "rviT0rtHgGFR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define the target column\n",
        "# Replace 'target_column' with the actual name of the column in your DataFrame\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data first to avoid data leakage before scaling\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "9pAK_In0gHOz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "MMvpsKwLgJrN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define the target column\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert back to DataFrames to retain column names\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='rbf', random_state=303)  # 'rbf' is the default kernel\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy on the validation set\n",
        "y_pred = svm_classifier.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data with the same scaler used on the training data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "test_df_encoded_scaled = pd.DataFrame(test_df_encoded_scaled, columns=test_df_encoded.columns)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = svm_classifier.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Create a DataFrame with trip_ID and predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': predictions\n",
        "})\n",
        "\n",
        "# Save predictions to a new CSV file\n",
        "results_df.to_csv('predictions_results_svm.csv', index=False)\n",
        "print(\"Predictions saved to predictions_results_svm.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VePruqbgwLT",
        "outputId": "8f2d97b1-317b-45d8-a4c7-b2e7ee7f12f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.7545454545454545\n",
            "Predictions saved to predictions_results_svm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Define the target column\n",
        "# X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "# y = train_df_encoded['category']  # Target\n",
        "\n",
        "# # Split the data\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# # Initialize and apply the scaler\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_val = scaler.transform(X_val)\n",
        "\n",
        "# # Convert back to DataFrames to retain column names\n",
        "# X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "# X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "\n",
        "# # Define the SVM classifier\n",
        "# svm = SVC(random_state=303)\n",
        "\n",
        "# # Define the parameter distribution for tuning\n",
        "# param_dist = {\n",
        "#     'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Different kernel types\n",
        "#     'C': np.logspace(-2, 2, 10),  # Regularization parameter (from 0.01 to 100)\n",
        "#     'gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 5)),  # Kernel coefficient\n",
        "#     'degree': [2, 3, 4, 5],  # Degree of polynomial kernel function (only for 'poly')\n",
        "# }\n",
        "\n",
        "# # Initialize RandomizedSearchCV\n",
        "# random_search = RandomizedSearchCV(\n",
        "#     estimator=svm,\n",
        "#     param_distributions=param_dist,\n",
        "#     n_iter=5,  # Number of random combinations to try\n",
        "#     scoring='accuracy',\n",
        "#     cv=2,\n",
        "#     verbose=2,\n",
        "#     n_jobs=-1,\n",
        "#     random_state=303\n",
        "# )\n",
        "\n",
        "# # Perform random search on training data\n",
        "# random_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best parameters and best score\n",
        "# print(\"Best Parameters:\", random_search.best_params_)\n",
        "# print(\"Best Cross-Validation Score:\", random_search.best_score_)\n",
        "\n",
        "# # Use the best estimator to make predictions on validation data\n",
        "# best_svm = random_search.best_estimator_\n",
        "# y_pred = best_svm.predict(X_val)\n",
        "# accuracy = accuracy_score(y_val, y_pred)\n",
        "# print(\"Validation Accuracy with Best SVM:\", accuracy)\n",
        "\n",
        "# # Load the test data\n",
        "# temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# # Scale the test data with the same scaler used on the training data\n",
        "# test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "# test_df_encoded_scaled = pd.DataFrame(test_df_encoded_scaled, columns=test_df_encoded.columns)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# predictions = best_svm.predict(test_df_encoded_scaled)\n",
        "\n",
        "# # Create a DataFrame with trip_ID and predictions\n",
        "# results_df = pd.DataFrame({\n",
        "#     'trip_ID': temp_test_df['trip_ID'],\n",
        "#     'category': predictions\n",
        "# })\n",
        "\n",
        "# # Save predictions to a new CSV file\n",
        "# results_df.to_csv('predictions_results_svm_tuned.csv', index=False)\n",
        "# print(\"Predictions saved to predictions_results_svm_tuned.csv\")\n"
      ],
      "metadata": {
        "id": "u2t9qzhtgz7o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.replace(r'[\\[\\]<>,]', '', regex=True)\n",
        "    return df\n",
        "\n",
        "# Manually define SVM hyperparameters\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')  # You can adjust these values as needed\n",
        "\n",
        "# Train the SVM regressor\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "svr_predictions = svr_model.predict(X_val)\n",
        "svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "print(\"SVM Regressor Mean Absolute Error:\", svr_mae)\n",
        "\n",
        "# Predict on the test set\n",
        "svr_test_predictions = svr_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_only.csv', index=False)\n",
        "\n",
        "print(\"Submission for SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dY4Funphmyc",
        "outputId": "b200dc95-aae0-43b6-a178-76321e436756"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Regressor Mean Absolute Error: 0.3145443155252721\n",
            "Submission for SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.replace(r'[\\[\\]<>,]', '', regex=True)\n",
        "    return df\n",
        "\n",
        "# Manually define SVM hyperparameters\n",
        "svr_model = SVR(kernel='rbf', C=1.0, gamma='scale')  # You can adjust these values as needed\n",
        "\n",
        "# Train the SVM regressor\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "svr_predictions = svr_model.predict(X_val)\n",
        "svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "print(\"SVM Regressor Mean Absolute Error:\", svr_mae)\n",
        "\n",
        "# Predict on the test set\n",
        "svr_test_predictions = svr_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "print(svr_output['category'])\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_only_tp.csv', index=False)\n",
        "\n",
        "print(\"Submission for SVM model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ChZgCvmGgg",
        "outputId": "1570a1c1-367e-433d-efe9-ff6662d36054"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Regressor Mean Absolute Error: 0.3145443155252721\n",
            "0       1.140345\n",
            "1       0.002403\n",
            "2       0.227088\n",
            "3       0.219506\n",
            "4       0.296209\n",
            "          ...   \n",
            "5847    1.304772\n",
            "5848    0.065384\n",
            "5849    0.270208\n",
            "5850    1.156263\n",
            "5851    0.756091\n",
            "Name: category, Length: 5852, dtype: float64\n",
            "Submission for SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svr_output['category']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rfND8RIKmzqa",
        "outputId": "4ff99149-d8c7-43fb-9c8c-3e7d70dad082"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "       ..\n",
              "5847    2\n",
              "5848    0\n",
              "5849    0\n",
              "5850    1\n",
              "5851    1\n",
              "Name: category, Length: 5852, dtype: category\n",
              "Categories (3, int64): [0 < 1 < 2]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5847</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5848</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5849</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5850</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5851</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5852 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> category</label>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVR\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define different kernels to test\n",
        "# kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# # Initialize variables to store the best kernel and corresponding MAE\n",
        "# best_kernel = None\n",
        "# best_mae = float('inf')\n",
        "# best_model = None\n",
        "\n",
        "# # Loop through each kernel and evaluate its performance\n",
        "# for kernel in kernels:\n",
        "#     print(f\"Training SVR with kernel: {kernel}\")\n",
        "#     svr_model = SVR(kernel=kernel, C=1.0, gamma='scale')  # Keep other parameters fixed for now\n",
        "#     svr_model.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict on the validation set\n",
        "#     svr_predictions = svr_model.predict(X_val)\n",
        "#     svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "#     print(f\"Mean Absolute Error with kernel {kernel}: {svr_mae}\")\n",
        "\n",
        "#     # Check if this kernel is the best so far\n",
        "#     if svr_mae < best_mae:\n",
        "#         best_mae = svr_mae\n",
        "#         best_kernel = kernel\n",
        "#         best_model = svr_model\n",
        "\n",
        "# print(f\"Best kernel: {best_kernel} with MAE: {best_mae}\")\n",
        "\n",
        "# # Predict on the test set using the best model\n",
        "# svr_test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# # Load the test dataset\n",
        "# temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# # SVM Predictions on test set\n",
        "# svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# # Apply binning to convert regression outputs into categories\n",
        "# svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# # Save predictions to a CSV file\n",
        "# svr_output.to_csv('submission_svr_tuned.csv', index=False)\n",
        "\n",
        "# print(\"Submission for tuned SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "oFTkTdeNlnex"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_encoded['category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "AvykXu7Pls6i",
        "outputId": "e949e356-5cd5-4b73-88c8-c6b845e54443"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "0.0    6240\n",
              "1.0    4943\n",
              "2.0    1463\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>6240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>4943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>1463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define different kernels to test\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# Initialize variables to store the best kernel and corresponding MAE\n",
        "best_kernel = None\n",
        "best_mae = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Loop through each kernel and evaluate its performance\n",
        "for kernel in kernels:\n",
        "    print(f\"Training SVR with kernel: {kernel}\")\n",
        "    svr_model = SVR(kernel=kernel, C=1.0, gamma='scale')  # Keep other parameters fixed for now\n",
        "    svr_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the validation set\n",
        "    svr_predictions = svr_model.predict(X_val)\n",
        "    svr_mae = mean_absolute_error(y_val, svr_predictions)\n",
        "    print(f\"Mean Absolute Error with kernel {kernel}: {svr_mae}\")\n",
        "\n",
        "    # Check if this kernel is the best so far\n",
        "    if svr_mae < best_mae:\n",
        "        best_mae = svr_mae\n",
        "        best_kernel = kernel\n",
        "        best_model = svr_model\n",
        "\n",
        "print(f\"Best kernel: {best_kernel} with MAE: {best_mae}\")\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "svr_test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Load the test dataset\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# SVM Predictions on test set\n",
        "svr_output = pd.DataFrame({'trip_ID': temp_test_df['trip_ID'], 'category': svr_test_predictions})\n",
        "\n",
        "# Apply binning to convert regression outputs into categories\n",
        "svr_output['category'] = pd.cut(svr_output['category'], bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "svr_output.to_csv('submission_svr_tuned.csv', index=False)\n",
        "\n",
        "print(\"Submission for tuned SVM model saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jTDbjHkjgc_",
        "outputId": "e7ff29fe-48a3-4147-875c-2aaf7ad09033"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVR with kernel: linear\n",
            "Mean Absolute Error with kernel linear: 0.33002165030410735\n",
            "Training SVR with kernel: poly\n",
            "Mean Absolute Error with kernel poly: 0.3318558663207929\n",
            "Training SVR with kernel: rbf\n",
            "Mean Absolute Error with kernel rbf: 0.3145443155252721\n",
            "Training SVR with kernel: sigmoid\n",
            "Mean Absolute Error with kernel sigmoid: 13.60265789028028\n",
            "Best kernel: rbf with MAE: 0.3145443155252721\n",
            "Submission for tuned SVM model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming `train_df_encoded` is your original dataset\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=303, max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = log_reg.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data using the same scaler\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = log_reg.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_logreg.csv', index=False)\n",
        "print(\"Logistic Regression predictions saved to 'submission_logreg.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFM6j_Ckt14",
        "outputId": "30e5be07-82cf-41f1-d87e-8fa360137fb0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7573\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84      1248\n",
            "         1.0       0.70      0.72      0.71       989\n",
            "         2.0       0.70      0.42      0.52       293\n",
            "\n",
            "    accuracy                           0.76      2530\n",
            "   macro avg       0.74      0.67      0.69      2530\n",
            "weighted avg       0.75      0.76      0.75      2530\n",
            "\n",
            "Logistic Regression predictions saved to 'submission_logreg.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define features and target\n",
        "# X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "# y = train_df_encoded['category']  # Target\n",
        "\n",
        "# # Split the data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# # Initialize and apply the scaler\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_val = scaler.transform(X_val)\n",
        "\n",
        "# # Define hyperparameter grid\n",
        "# param_grid = {\n",
        "#     'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization terms\n",
        "#     'C': [0.01, 0.1, 1, 10, 100],                  # Inverse regularization strength\n",
        "#     'solver': ['saga', 'liblinear', 'lbfgs'],      # Solvers for optimization\n",
        "#     'max_iter': [100, 500, 1000]                   # Maximum number of iterations\n",
        "# }\n",
        "\n",
        "# # Initialize Logistic Regression\n",
        "# log_reg = LogisticRegression(random_state=303)\n",
        "\n",
        "# # Perform Grid Search\n",
        "# grid_search = GridSearchCV(\n",
        "#     estimator=log_reg,\n",
        "#     param_grid=param_grid,\n",
        "#     scoring='accuracy',\n",
        "#     cv=3,  # 3-fold cross-validation\n",
        "#     n_jobs=-1,\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# # Fit GridSearchCV\n",
        "# print(\"Starting Grid Search...\")\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Get the best parameters and model\n",
        "# best_params = grid_search.best_params_\n",
        "# best_model = grid_search.best_estimator_\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# # Evaluate on the validation set\n",
        "# y_pred = best_model.predict(X_val)\n",
        "# accuracy = accuracy_score(y_val, y_pred)\n",
        "# print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# # Load the test data\n",
        "# temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# # Scale the test data using the same scaler\n",
        "# test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# # Predict on the test set\n",
        "# test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# # Save predictions to a CSV file\n",
        "# results_df = pd.DataFrame({\n",
        "#     'trip_ID': temp_test_df['trip_ID'],\n",
        "#     'category': test_predictions\n",
        "# })\n",
        "\n",
        "# results_df.to_csv('submission_logreg_tuned.csv', index=False)\n",
        "# print(\"Tuned Logistic Regression predictions saved to 'submission_logreg_tuned.csv'.\")\n"
      ],
      "metadata": {
        "id": "yT0eacBjnxyb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Initialize and apply the scaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define hyperparameter space for RandomizedSearch\n",
        "param_dist = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],  # Regularization terms\n",
        "    'C': np.logspace(-4, 4, 20),                    # Wide range of regularization strength\n",
        "    'solver': ['saga', 'liblinear', 'lbfgs'],       # Solvers for optimization\n",
        "    'max_iter': [100, 200, 500, 1000]               # Maximum iterations\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "log_reg = LogisticRegression(random_state=303)\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,               # Number of parameter combinations to try\n",
        "    scoring='accuracy',      # Metric for optimization\n",
        "    cv=3,                    # 3-fold cross-validation\n",
        "    n_jobs=-1,               # Use all available cores\n",
        "    random_state=303,        # For reproducibility\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "print(\"Starting Randomized Search...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "y_pred = best_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data using the same scaler\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = best_model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_logreg_random_tuned.csv', index=False)\n",
        "print(\"Tuned Logistic Regression predictions saved to 'submission_logreg_random_tuned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8XVHvZJoP2v",
        "outputId": "db1545b2-96e4-4157-b4c8-5c9a789ce4d7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Randomized Search...\n",
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "93 fits failed out of a total of 150.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "18 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "26 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "19 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "12 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.71510478        nan        nan        nan 0.74327798 0.74308027\n",
            "        nan 0.73843416        nan        nan 0.74031238        nan\n",
            "        nan 0.7394227         nan 0.73892843 0.73843416        nan\n",
            "        nan 0.73902728        nan        nan        nan 0.7394227\n",
            "        nan 0.73843416        nan        nan        nan        nan\n",
            "        nan 0.73032819        nan        nan        nan        nan\n",
            " 0.74031238 0.73922499 0.74278371        nan        nan 0.74278371\n",
            " 0.74308027 0.72004745        nan        nan        nan        nan\n",
            " 0.74308027        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 100, 'C': 10000.0}\n",
            "Validation Accuracy: 0.7569\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.87      0.84      1248\n",
            "         1.0       0.70      0.72      0.71       989\n",
            "         2.0       0.70      0.42      0.52       293\n",
            "\n",
            "    accuracy                           0.76      2530\n",
            "   macro avg       0.74      0.67      0.69      2530\n",
            "weighted avg       0.75      0.76      0.75      2530\n",
            "\n",
            "Tuned Logistic Regression predictions saved to 'submission_logreg_random_tuned.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Convert target to categorical if it’s a classification task\n",
        "num_classes = len(y.unique())\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "# Build the neural network\n",
        "def build_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')  # Softmax for classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_model(input_dim=X_train.shape[1], num_classes=num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "y_pred_val = model.predict(X_val)\n",
        "y_pred_classes = y_pred_val.argmax(axis=1)\n",
        "y_val_classes = y_val.argmax(axis=1)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val_classes, y_pred_classes))\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "test_pred_classes = test_predictions.argmax(axis=1)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_pred_classes\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn.csv', index=False)\n",
        "print(\"Neural Network predictions saved to 'submission_nn.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoVLQ6Ojo0H1",
        "outputId": "4a3ba555-8768-4953-c4d2-a400707af21c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.6200 - loss: 0.8960 - val_accuracy: 0.7360 - val_loss: 0.6349\n",
            "Epoch 2/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7176 - loss: 0.6656 - val_accuracy: 0.7395 - val_loss: 0.6163\n",
            "Epoch 3/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7284 - loss: 0.6354 - val_accuracy: 0.7383 - val_loss: 0.6143\n",
            "Epoch 4/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7277 - loss: 0.6381 - val_accuracy: 0.7498 - val_loss: 0.6127\n",
            "Epoch 5/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7272 - loss: 0.6364 - val_accuracy: 0.7439 - val_loss: 0.6068\n",
            "Epoch 6/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7367 - loss: 0.6301 - val_accuracy: 0.7510 - val_loss: 0.6064\n",
            "Epoch 7/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7427 - loss: 0.6164 - val_accuracy: 0.7439 - val_loss: 0.6047\n",
            "Epoch 8/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7459 - loss: 0.6030 - val_accuracy: 0.7502 - val_loss: 0.6053\n",
            "Epoch 9/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7458 - loss: 0.6103 - val_accuracy: 0.7522 - val_loss: 0.6007\n",
            "Epoch 10/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7407 - loss: 0.6158 - val_accuracy: 0.7522 - val_loss: 0.6035\n",
            "Epoch 11/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7344 - loss: 0.6138 - val_accuracy: 0.7451 - val_loss: 0.6053\n",
            "Epoch 12/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7512 - loss: 0.5967 - val_accuracy: 0.7526 - val_loss: 0.6052\n",
            "Epoch 13/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7475 - loss: 0.5989 - val_accuracy: 0.7518 - val_loss: 0.6036\n",
            "Epoch 14/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.6033 - val_accuracy: 0.7474 - val_loss: 0.6065\n",
            "Epoch 15/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7402 - loss: 0.6126 - val_accuracy: 0.7466 - val_loss: 0.6050\n",
            "Epoch 16/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7460 - loss: 0.5965 - val_accuracy: 0.7553 - val_loss: 0.6053\n",
            "Epoch 17/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7484 - loss: 0.6060 - val_accuracy: 0.7502 - val_loss: 0.6029\n",
            "Epoch 18/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.5891 - val_accuracy: 0.7518 - val_loss: 0.6035\n",
            "Epoch 19/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7425 - loss: 0.6049 - val_accuracy: 0.7526 - val_loss: 0.6030\n",
            "Epoch 20/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.5966 - val_accuracy: 0.7526 - val_loss: 0.6030\n",
            "Epoch 21/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7460 - loss: 0.5893 - val_accuracy: 0.7530 - val_loss: 0.6021\n",
            "Epoch 22/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7509 - loss: 0.5830 - val_accuracy: 0.7538 - val_loss: 0.6022\n",
            "Epoch 23/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.5948 - val_accuracy: 0.7494 - val_loss: 0.6026\n",
            "Epoch 24/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.5896 - val_accuracy: 0.7522 - val_loss: 0.6046\n",
            "Epoch 25/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.5994 - val_accuracy: 0.7538 - val_loss: 0.6050\n",
            "Epoch 26/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.5825 - val_accuracy: 0.7569 - val_loss: 0.5975\n",
            "Epoch 27/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7532 - loss: 0.5871 - val_accuracy: 0.7542 - val_loss: 0.6000\n",
            "Epoch 28/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.5824 - val_accuracy: 0.7474 - val_loss: 0.6077\n",
            "Epoch 29/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7505 - loss: 0.5914 - val_accuracy: 0.7573 - val_loss: 0.6039\n",
            "Epoch 30/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7605 - loss: 0.5714 - val_accuracy: 0.7534 - val_loss: 0.6022\n",
            "Epoch 31/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.5694 - val_accuracy: 0.7502 - val_loss: 0.6073\n",
            "Epoch 32/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7607 - loss: 0.5682 - val_accuracy: 0.7514 - val_loss: 0.6024\n",
            "Epoch 33/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.5727 - val_accuracy: 0.7514 - val_loss: 0.6095\n",
            "Epoch 34/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.5818 - val_accuracy: 0.7518 - val_loss: 0.5991\n",
            "Epoch 35/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.5777 - val_accuracy: 0.7458 - val_loss: 0.6050\n",
            "Epoch 36/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7505 - loss: 0.5885 - val_accuracy: 0.7518 - val_loss: 0.6043\n",
            "Epoch 37/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7500 - loss: 0.5813 - val_accuracy: 0.7549 - val_loss: 0.6015\n",
            "Epoch 38/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7537 - loss: 0.5736 - val_accuracy: 0.7518 - val_loss: 0.6059\n",
            "Epoch 39/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7644 - loss: 0.5661 - val_accuracy: 0.7549 - val_loss: 0.6018\n",
            "Epoch 40/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7507 - loss: 0.5818 - val_accuracy: 0.7538 - val_loss: 0.6076\n",
            "Epoch 41/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.5693 - val_accuracy: 0.7478 - val_loss: 0.6039\n",
            "Epoch 42/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7610 - loss: 0.5764 - val_accuracy: 0.7518 - val_loss: 0.6044\n",
            "Epoch 43/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.5708 - val_accuracy: 0.7502 - val_loss: 0.6066\n",
            "Epoch 44/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7620 - loss: 0.5663 - val_accuracy: 0.7549 - val_loss: 0.6048\n",
            "Epoch 45/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.5551 - val_accuracy: 0.7510 - val_loss: 0.6034\n",
            "Epoch 46/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7651 - loss: 0.5642 - val_accuracy: 0.7538 - val_loss: 0.6035\n",
            "Epoch 47/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.5744 - val_accuracy: 0.7581 - val_loss: 0.6040\n",
            "Epoch 48/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7630 - loss: 0.5698 - val_accuracy: 0.7510 - val_loss: 0.6030\n",
            "Epoch 49/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7616 - loss: 0.5766 - val_accuracy: 0.7565 - val_loss: 0.6090\n",
            "Epoch 50/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7644 - loss: 0.5630 - val_accuracy: 0.7565 - val_loss: 0.6080\n",
            "Validation Accuracy: 0.7565\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84      1248\n",
            "           1       0.71      0.70      0.70       989\n",
            "           2       0.62      0.45      0.52       293\n",
            "\n",
            "    accuracy                           0.76      2530\n",
            "   macro avg       0.71      0.68      0.69      2530\n",
            "weighted avg       0.75      0.76      0.75      2530\n",
            "\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Neural Network predictions saved to 'submission_nn.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target (continuous)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Build the neural network\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned.csv', index=False)\n",
        "print(\"Neural Network predictions with bins saved to 'submission_nn_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Pq6cfrpNBi",
        "outputId": "90a2056b-cf0d-46b6-e0fd-4520b9c815c0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 1.0566 - mean_absolute_error: 0.7698 - val_loss: 0.2441 - val_mean_absolute_error: 0.3807\n",
            "Epoch 2/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3407 - mean_absolute_error: 0.4502 - val_loss: 0.2293 - val_mean_absolute_error: 0.3655\n",
            "Epoch 3/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2904 - mean_absolute_error: 0.4146 - val_loss: 0.2320 - val_mean_absolute_error: 0.3466\n",
            "Epoch 4/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2748 - mean_absolute_error: 0.3969 - val_loss: 0.2286 - val_mean_absolute_error: 0.3418\n",
            "Epoch 5/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2514 - mean_absolute_error: 0.3768 - val_loss: 0.2158 - val_mean_absolute_error: 0.3404\n",
            "Epoch 6/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2549 - mean_absolute_error: 0.3796 - val_loss: 0.2148 - val_mean_absolute_error: 0.3515\n",
            "Epoch 7/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2475 - mean_absolute_error: 0.3740 - val_loss: 0.2132 - val_mean_absolute_error: 0.3401\n",
            "Epoch 8/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2496 - mean_absolute_error: 0.3760 - val_loss: 0.2113 - val_mean_absolute_error: 0.3425\n",
            "Epoch 9/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2300 - mean_absolute_error: 0.3614 - val_loss: 0.2171 - val_mean_absolute_error: 0.3456\n",
            "Epoch 10/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2391 - mean_absolute_error: 0.3659 - val_loss: 0.2140 - val_mean_absolute_error: 0.3333\n",
            "Epoch 11/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2279 - mean_absolute_error: 0.3569 - val_loss: 0.2201 - val_mean_absolute_error: 0.3495\n",
            "Epoch 12/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2278 - mean_absolute_error: 0.3526 - val_loss: 0.2121 - val_mean_absolute_error: 0.3308\n",
            "Epoch 13/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2321 - mean_absolute_error: 0.3595 - val_loss: 0.2138 - val_mean_absolute_error: 0.3305\n",
            "Epoch 14/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2300 - mean_absolute_error: 0.3580 - val_loss: 0.2148 - val_mean_absolute_error: 0.3558\n",
            "Epoch 15/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2327 - mean_absolute_error: 0.3611 - val_loss: 0.2171 - val_mean_absolute_error: 0.3373\n",
            "Epoch 16/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2166 - mean_absolute_error: 0.3424 - val_loss: 0.2102 - val_mean_absolute_error: 0.3486\n",
            "Epoch 17/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2237 - mean_absolute_error: 0.3494 - val_loss: 0.2101 - val_mean_absolute_error: 0.3392\n",
            "Epoch 18/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2221 - mean_absolute_error: 0.3519 - val_loss: 0.2118 - val_mean_absolute_error: 0.3477\n",
            "Epoch 19/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2268 - mean_absolute_error: 0.3515 - val_loss: 0.2117 - val_mean_absolute_error: 0.3250\n",
            "Epoch 20/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2113 - mean_absolute_error: 0.3360 - val_loss: 0.2113 - val_mean_absolute_error: 0.3400\n",
            "Epoch 21/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2143 - mean_absolute_error: 0.3392 - val_loss: 0.2124 - val_mean_absolute_error: 0.3291\n",
            "Epoch 22/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2235 - mean_absolute_error: 0.3470 - val_loss: 0.2144 - val_mean_absolute_error: 0.3322\n",
            "Epoch 23/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2188 - mean_absolute_error: 0.3445 - val_loss: 0.2108 - val_mean_absolute_error: 0.3466\n",
            "Epoch 24/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2250 - mean_absolute_error: 0.3494 - val_loss: 0.2154 - val_mean_absolute_error: 0.3355\n",
            "Epoch 25/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2237 - mean_absolute_error: 0.3526 - val_loss: 0.2128 - val_mean_absolute_error: 0.3323\n",
            "Epoch 26/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2152 - mean_absolute_error: 0.3425 - val_loss: 0.2127 - val_mean_absolute_error: 0.3251\n",
            "Epoch 27/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2092 - mean_absolute_error: 0.3323 - val_loss: 0.2163 - val_mean_absolute_error: 0.3384\n",
            "Epoch 28/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2120 - mean_absolute_error: 0.3419 - val_loss: 0.2121 - val_mean_absolute_error: 0.3297\n",
            "Epoch 29/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2149 - mean_absolute_error: 0.3436 - val_loss: 0.2114 - val_mean_absolute_error: 0.3354\n",
            "Epoch 30/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2128 - mean_absolute_error: 0.3405 - val_loss: 0.2120 - val_mean_absolute_error: 0.3381\n",
            "Epoch 31/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2233 - mean_absolute_error: 0.3477 - val_loss: 0.2129 - val_mean_absolute_error: 0.3341\n",
            "Epoch 32/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2105 - mean_absolute_error: 0.3356 - val_loss: 0.2120 - val_mean_absolute_error: 0.3197\n",
            "Epoch 33/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2126 - mean_absolute_error: 0.3370 - val_loss: 0.2115 - val_mean_absolute_error: 0.3304\n",
            "Epoch 34/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2238 - mean_absolute_error: 0.3544 - val_loss: 0.2109 - val_mean_absolute_error: 0.3229\n",
            "Epoch 35/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2120 - mean_absolute_error: 0.3369 - val_loss: 0.2149 - val_mean_absolute_error: 0.3280\n",
            "Epoch 36/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2050 - mean_absolute_error: 0.3289 - val_loss: 0.2100 - val_mean_absolute_error: 0.3291\n",
            "Epoch 37/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2070 - mean_absolute_error: 0.3327 - val_loss: 0.2129 - val_mean_absolute_error: 0.3296\n",
            "Epoch 38/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1985 - mean_absolute_error: 0.3229 - val_loss: 0.2102 - val_mean_absolute_error: 0.3437\n",
            "Epoch 39/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2135 - mean_absolute_error: 0.3426 - val_loss: 0.2147 - val_mean_absolute_error: 0.3252\n",
            "Epoch 40/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2049 - mean_absolute_error: 0.3295 - val_loss: 0.2083 - val_mean_absolute_error: 0.3317\n",
            "Epoch 41/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2064 - mean_absolute_error: 0.3349 - val_loss: 0.2104 - val_mean_absolute_error: 0.3260\n",
            "Epoch 42/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2049 - mean_absolute_error: 0.3296 - val_loss: 0.2103 - val_mean_absolute_error: 0.3201\n",
            "Epoch 43/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2049 - mean_absolute_error: 0.3299 - val_loss: 0.2111 - val_mean_absolute_error: 0.3191\n",
            "Epoch 44/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2090 - mean_absolute_error: 0.3340 - val_loss: 0.2088 - val_mean_absolute_error: 0.3273\n",
            "Epoch 45/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2161 - mean_absolute_error: 0.3434 - val_loss: 0.2130 - val_mean_absolute_error: 0.3288\n",
            "Epoch 46/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2059 - mean_absolute_error: 0.3326 - val_loss: 0.2097 - val_mean_absolute_error: 0.3243\n",
            "Epoch 47/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1995 - mean_absolute_error: 0.3234 - val_loss: 0.2075 - val_mean_absolute_error: 0.3397\n",
            "Epoch 48/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2117 - mean_absolute_error: 0.3364 - val_loss: 0.2143 - val_mean_absolute_error: 0.3290\n",
            "Epoch 49/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2079 - mean_absolute_error: 0.3314 - val_loss: 0.2115 - val_mean_absolute_error: 0.3140\n",
            "Epoch 50/50\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2067 - mean_absolute_error: 0.3301 - val_loss: 0.2121 - val_mean_absolute_error: 0.3454\n",
            "Validation MAE: 0.3454\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3454\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Neural Network predictions with bins saved to 'submission_nn_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target (continuous)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Optimized Neural Network Function\n",
        "def build_optimized_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),  # Reduced dropout for improved learning\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_optimized_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Add callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,  # Train up to 100 epochs with early stopping\n",
        "    batch_size=64,  # Larger batch size for faster computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2CNRFPqEI7",
        "outputId": "40f3f622-09c5-4dcd-f19b-780d00cce817"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 16ms/step - loss: 1.0936 - mean_absolute_error: 0.7790 - val_loss: 0.2735 - val_mean_absolute_error: 0.4197 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.3544 - mean_absolute_error: 0.4601 - val_loss: 0.2485 - val_mean_absolute_error: 0.3984 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3101 - mean_absolute_error: 0.4255 - val_loss: 0.2254 - val_mean_absolute_error: 0.3539 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2799 - mean_absolute_error: 0.4029 - val_loss: 0.2283 - val_mean_absolute_error: 0.3734 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2625 - mean_absolute_error: 0.3887 - val_loss: 0.2285 - val_mean_absolute_error: 0.3627 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2570 - mean_absolute_error: 0.3826 - val_loss: 0.2218 - val_mean_absolute_error: 0.3483 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2448 - mean_absolute_error: 0.3737 - val_loss: 0.2185 - val_mean_absolute_error: 0.3540 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2442 - mean_absolute_error: 0.3736 - val_loss: 0.2235 - val_mean_absolute_error: 0.3382 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2347 - mean_absolute_error: 0.3602 - val_loss: 0.2225 - val_mean_absolute_error: 0.3648 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2360 - mean_absolute_error: 0.3649 - val_loss: 0.2221 - val_mean_absolute_error: 0.3485 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2286 - mean_absolute_error: 0.3550 - val_loss: 0.2158 - val_mean_absolute_error: 0.3655 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2282 - mean_absolute_error: 0.3560 - val_loss: 0.2137 - val_mean_absolute_error: 0.3426 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2311 - mean_absolute_error: 0.3594 - val_loss: 0.2114 - val_mean_absolute_error: 0.3437 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2135 - mean_absolute_error: 0.3441 - val_loss: 0.2151 - val_mean_absolute_error: 0.3400 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2168 - mean_absolute_error: 0.3457 - val_loss: 0.2160 - val_mean_absolute_error: 0.3354 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2271 - mean_absolute_error: 0.3549 - val_loss: 0.2165 - val_mean_absolute_error: 0.3467 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2252 - mean_absolute_error: 0.3502 - val_loss: 0.2136 - val_mean_absolute_error: 0.3536 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m147/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2248 - mean_absolute_error: 0.3535\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2246 - mean_absolute_error: 0.3531 - val_loss: 0.2164 - val_mean_absolute_error: 0.3381 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2151 - mean_absolute_error: 0.3400 - val_loss: 0.2131 - val_mean_absolute_error: 0.3285 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2111 - mean_absolute_error: 0.3348 - val_loss: 0.2150 - val_mean_absolute_error: 0.3308 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2102 - mean_absolute_error: 0.3346 - val_loss: 0.2115 - val_mean_absolute_error: 0.3352 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2168 - mean_absolute_error: 0.3395 - val_loss: 0.2107 - val_mean_absolute_error: 0.3314 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2103 - mean_absolute_error: 0.3382 - val_loss: 0.2114 - val_mean_absolute_error: 0.3370 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2184 - mean_absolute_error: 0.3442 - val_loss: 0.2142 - val_mean_absolute_error: 0.3242 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2153 - mean_absolute_error: 0.3428 - val_loss: 0.2118 - val_mean_absolute_error: 0.3329 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2176 - mean_absolute_error: 0.3440 - val_loss: 0.2122 - val_mean_absolute_error: 0.3332 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m157/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2169 - mean_absolute_error: 0.3404\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2168 - mean_absolute_error: 0.3403 - val_loss: 0.2141 - val_mean_absolute_error: 0.3219 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2144 - mean_absolute_error: 0.3336 - val_loss: 0.2119 - val_mean_absolute_error: 0.3351 - learning_rate: 2.5000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2116 - mean_absolute_error: 0.3398 - val_loss: 0.2114 - val_mean_absolute_error: 0.3311 - learning_rate: 2.5000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2098 - mean_absolute_error: 0.3345 - val_loss: 0.2116 - val_mean_absolute_error: 0.3272 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2089 - mean_absolute_error: 0.3324 - val_loss: 0.2138 - val_mean_absolute_error: 0.3261 - learning_rate: 2.5000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m142/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2113 - mean_absolute_error: 0.3343\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2112 - mean_absolute_error: 0.3344 - val_loss: 0.2118 - val_mean_absolute_error: 0.3276 - learning_rate: 2.5000e-04\n",
            "Epoch 32: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n",
            "Validation MAE: 0.3314\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3314\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_2.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izknmyLOq9V4",
        "outputId": "2f617301-76dd-46f6-a497-86f8e03719d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.7111 - mean_absolute_error: 0.6463 - val_loss: 0.2788 - val_mean_absolute_error: 0.3782 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.3225 - mean_absolute_error: 0.4358 - val_loss: 0.2370 - val_mean_absolute_error: 0.3413 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2773 - mean_absolute_error: 0.3993 - val_loss: 0.2274 - val_mean_absolute_error: 0.3327 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2542 - mean_absolute_error: 0.3803 - val_loss: 0.2250 - val_mean_absolute_error: 0.3260 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2427 - mean_absolute_error: 0.3687 - val_loss: 0.2245 - val_mean_absolute_error: 0.3259 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2307 - mean_absolute_error: 0.3595 - val_loss: 0.2204 - val_mean_absolute_error: 0.3318 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2265 - mean_absolute_error: 0.3540 - val_loss: 0.2230 - val_mean_absolute_error: 0.3267 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2286 - mean_absolute_error: 0.3560 - val_loss: 0.2162 - val_mean_absolute_error: 0.3430 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2313 - mean_absolute_error: 0.3628 - val_loss: 0.2329 - val_mean_absolute_error: 0.3305 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2311 - mean_absolute_error: 0.3571 - val_loss: 0.2164 - val_mean_absolute_error: 0.3220 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2247 - mean_absolute_error: 0.3543 - val_loss: 0.2166 - val_mean_absolute_error: 0.3209 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2171 - mean_absolute_error: 0.3461 - val_loss: 0.2151 - val_mean_absolute_error: 0.3253 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2170 - mean_absolute_error: 0.3464 - val_loss: 0.2154 - val_mean_absolute_error: 0.3172 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2179 - mean_absolute_error: 0.3436 - val_loss: 0.2204 - val_mean_absolute_error: 0.3261 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2147 - mean_absolute_error: 0.3421 - val_loss: 0.2111 - val_mean_absolute_error: 0.3262 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2221 - mean_absolute_error: 0.3536 - val_loss: 0.2194 - val_mean_absolute_error: 0.3169 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2190 - mean_absolute_error: 0.3437 - val_loss: 0.2173 - val_mean_absolute_error: 0.3225 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.2177 - mean_absolute_error: 0.3454 - val_loss: 0.2184 - val_mean_absolute_error: 0.3195 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2193 - mean_absolute_error: 0.3418 - val_loss: 0.2125 - val_mean_absolute_error: 0.3297 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.2085 - mean_absolute_error: 0.3336 - val_loss: 0.2106 - val_mean_absolute_error: 0.3283 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2092 - mean_absolute_error: 0.3362 - val_loss: 0.2119 - val_mean_absolute_error: 0.3273 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2177 - mean_absolute_error: 0.3468 - val_loss: 0.2143 - val_mean_absolute_error: 0.3274 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2142 - mean_absolute_error: 0.3427 - val_loss: 0.2160 - val_mean_absolute_error: 0.3140 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2160 - mean_absolute_error: 0.3378 - val_loss: 0.2131 - val_mean_absolute_error: 0.3395 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m301/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2052 - mean_absolute_error: 0.3327\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2056 - mean_absolute_error: 0.3331 - val_loss: 0.2146 - val_mean_absolute_error: 0.3201 - learning_rate: 0.0010\n",
            "Epoch 26/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2015 - mean_absolute_error: 0.3255 - val_loss: 0.2109 - val_mean_absolute_error: 0.3278 - learning_rate: 2.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2053 - mean_absolute_error: 0.3325 - val_loss: 0.2111 - val_mean_absolute_error: 0.3246 - learning_rate: 2.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2055 - mean_absolute_error: 0.3280 - val_loss: 0.2106 - val_mean_absolute_error: 0.3234 - learning_rate: 2.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2009 - mean_absolute_error: 0.3275 - val_loss: 0.2113 - val_mean_absolute_error: 0.3189 - learning_rate: 2.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m302/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1994 - mean_absolute_error: 0.3254\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1996 - mean_absolute_error: 0.3256 - val_loss: 0.2112 - val_mean_absolute_error: 0.3255 - learning_rate: 2.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2037 - mean_absolute_error: 0.3339 - val_loss: 0.2124 - val_mean_absolute_error: 0.3229 - learning_rate: 4.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1978 - mean_absolute_error: 0.3241 - val_loss: 0.2119 - val_mean_absolute_error: 0.3219 - learning_rate: 4.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1949 - mean_absolute_error: 0.3236 - val_loss: 0.2118 - val_mean_absolute_error: 0.3217 - learning_rate: 4.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1957 - mean_absolute_error: 0.3218 - val_loss: 0.2123 - val_mean_absolute_error: 0.3223 - learning_rate: 4.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m289/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2081 - mean_absolute_error: 0.3332\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2076 - mean_absolute_error: 0.3328 - val_loss: 0.2121 - val_mean_absolute_error: 0.3209 - learning_rate: 4.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1999 - mean_absolute_error: 0.3245 - val_loss: 0.2121 - val_mean_absolute_error: 0.3208 - learning_rate: 8.0000e-06\n",
            "Epoch 37/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2020 - mean_absolute_error: 0.3290 - val_loss: 0.2122 - val_mean_absolute_error: 0.3208 - learning_rate: 8.0000e-06\n",
            "Epoch 38/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2003 - mean_absolute_error: 0.3245 - val_loss: 0.2120 - val_mean_absolute_error: 0.3215 - learning_rate: 8.0000e-06\n",
            "Epoch 39/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2054 - mean_absolute_error: 0.3304 - val_loss: 0.2121 - val_mean_absolute_error: 0.3214 - learning_rate: 8.0000e-06\n",
            "Epoch 40/200\n",
            "\u001b[1m304/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1995 - mean_absolute_error: 0.3258\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1995 - mean_absolute_error: 0.3258 - val_loss: 0.2123 - val_mean_absolute_error: 0.3212 - learning_rate: 8.0000e-06\n",
            "Epoch 41/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1968 - mean_absolute_error: 0.3243 - val_loss: 0.2123 - val_mean_absolute_error: 0.3203 - learning_rate: 1.6000e-06\n",
            "Epoch 42/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2061 - mean_absolute_error: 0.3309 - val_loss: 0.2123 - val_mean_absolute_error: 0.3208 - learning_rate: 1.6000e-06\n",
            "Epoch 43/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2036 - mean_absolute_error: 0.3295 - val_loss: 0.2121 - val_mean_absolute_error: 0.3213 - learning_rate: 1.6000e-06\n",
            "Epoch 43: early stopping\n",
            "Restoring model weights from the end of the best epoch: 28.\n",
            "Validation MAE: 0.3234\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3234\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.23, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_test_size_changed.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_test_size_changed.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdIwlzxCrrpg",
        "outputId": "184a9950-4035-471e-c5da-5c02cc140e9f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.8022 - mean_absolute_error: 0.6848 - val_loss: 0.2781 - val_mean_absolute_error: 0.3835 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3421 - mean_absolute_error: 0.4556 - val_loss: 0.2416 - val_mean_absolute_error: 0.3562 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2863 - mean_absolute_error: 0.4103 - val_loss: 0.2261 - val_mean_absolute_error: 0.3447 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2588 - mean_absolute_error: 0.3882 - val_loss: 0.2165 - val_mean_absolute_error: 0.3317 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2396 - mean_absolute_error: 0.3701 - val_loss: 0.2167 - val_mean_absolute_error: 0.3351 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2386 - mean_absolute_error: 0.3696 - val_loss: 0.2170 - val_mean_absolute_error: 0.3223 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2329 - mean_absolute_error: 0.3599 - val_loss: 0.2131 - val_mean_absolute_error: 0.3268 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2358 - mean_absolute_error: 0.3640 - val_loss: 0.2239 - val_mean_absolute_error: 0.3258 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2243 - mean_absolute_error: 0.3545 - val_loss: 0.2116 - val_mean_absolute_error: 0.3268 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2222 - mean_absolute_error: 0.3512 - val_loss: 0.2082 - val_mean_absolute_error: 0.3304 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2244 - mean_absolute_error: 0.3557 - val_loss: 0.2127 - val_mean_absolute_error: 0.3298 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2229 - mean_absolute_error: 0.3489 - val_loss: 0.2093 - val_mean_absolute_error: 0.3189 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2108 - mean_absolute_error: 0.3388 - val_loss: 0.2129 - val_mean_absolute_error: 0.3228 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2193 - mean_absolute_error: 0.3493 - val_loss: 0.2144 - val_mean_absolute_error: 0.3256 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m303/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2207 - mean_absolute_error: 0.3500\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2207 - mean_absolute_error: 0.3500 - val_loss: 0.2126 - val_mean_absolute_error: 0.3179 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2140 - mean_absolute_error: 0.3398 - val_loss: 0.2092 - val_mean_absolute_error: 0.3217 - learning_rate: 2.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2103 - mean_absolute_error: 0.3350 - val_loss: 0.2084 - val_mean_absolute_error: 0.3237 - learning_rate: 2.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2162 - mean_absolute_error: 0.3454 - val_loss: 0.2106 - val_mean_absolute_error: 0.3164 - learning_rate: 2.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2149 - mean_absolute_error: 0.3408 - val_loss: 0.2099 - val_mean_absolute_error: 0.3238 - learning_rate: 2.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m279/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2131 - mean_absolute_error: 0.3407\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2129 - mean_absolute_error: 0.3406 - val_loss: 0.2112 - val_mean_absolute_error: 0.3158 - learning_rate: 2.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2026 - mean_absolute_error: 0.3318 - val_loss: 0.2099 - val_mean_absolute_error: 0.3188 - learning_rate: 4.0000e-05\n",
            "Epoch 22/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2108 - mean_absolute_error: 0.3367 - val_loss: 0.2093 - val_mean_absolute_error: 0.3217 - learning_rate: 4.0000e-05\n",
            "Epoch 23/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2111 - mean_absolute_error: 0.3431 - val_loss: 0.2098 - val_mean_absolute_error: 0.3181 - learning_rate: 4.0000e-05\n",
            "Epoch 24/200\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2216 - mean_absolute_error: 0.3432 - val_loss: 0.2098 - val_mean_absolute_error: 0.3185 - learning_rate: 4.0000e-05\n",
            "Epoch 25/200\n",
            "\u001b[1m299/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2211 - mean_absolute_error: 0.3471\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2208 - mean_absolute_error: 0.3469 - val_loss: 0.2098 - val_mean_absolute_error: 0.3170 - learning_rate: 4.0000e-05\n",
            "Epoch 25: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Validation MAE: 0.3304\n",
            "\u001b[1m91/91\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3304\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_test_size_changed.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Best Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal()),  # He initialization\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Xavier initialization for linear output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_he.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_he.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf_uHC5XHSzk",
        "outputId": "92bcb28b-362b-4289-bb42-060db60e9e65"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 1.1264 - mean_absolute_error: 0.7903 - val_loss: 0.2817 - val_mean_absolute_error: 0.3946 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3931 - mean_absolute_error: 0.4880 - val_loss: 0.2595 - val_mean_absolute_error: 0.3825 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3229 - mean_absolute_error: 0.4357 - val_loss: 0.2296 - val_mean_absolute_error: 0.3496 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2768 - mean_absolute_error: 0.4025 - val_loss: 0.2246 - val_mean_absolute_error: 0.3552 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2573 - mean_absolute_error: 0.3836 - val_loss: 0.2243 - val_mean_absolute_error: 0.3479 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2418 - mean_absolute_error: 0.3683 - val_loss: 0.2265 - val_mean_absolute_error: 0.3437 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2423 - mean_absolute_error: 0.3703 - val_loss: 0.2174 - val_mean_absolute_error: 0.3300 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2343 - mean_absolute_error: 0.3615 - val_loss: 0.2217 - val_mean_absolute_error: 0.3457 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2412 - mean_absolute_error: 0.3698 - val_loss: 0.2162 - val_mean_absolute_error: 0.3240 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2251 - mean_absolute_error: 0.3497 - val_loss: 0.2150 - val_mean_absolute_error: 0.3318 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2341 - mean_absolute_error: 0.3644 - val_loss: 0.2177 - val_mean_absolute_error: 0.3269 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2192 - mean_absolute_error: 0.3516 - val_loss: 0.2142 - val_mean_absolute_error: 0.3311 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2249 - mean_absolute_error: 0.3525 - val_loss: 0.2164 - val_mean_absolute_error: 0.3389 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2245 - mean_absolute_error: 0.3537 - val_loss: 0.2133 - val_mean_absolute_error: 0.3334 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2170 - mean_absolute_error: 0.3513 - val_loss: 0.2182 - val_mean_absolute_error: 0.3178 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2185 - mean_absolute_error: 0.3438 - val_loss: 0.2119 - val_mean_absolute_error: 0.3276 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2188 - mean_absolute_error: 0.3444 - val_loss: 0.2139 - val_mean_absolute_error: 0.3247 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2169 - mean_absolute_error: 0.3424 - val_loss: 0.2165 - val_mean_absolute_error: 0.3285 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2246 - mean_absolute_error: 0.3490 - val_loss: 0.2146 - val_mean_absolute_error: 0.3222 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2209 - mean_absolute_error: 0.3486 - val_loss: 0.2118 - val_mean_absolute_error: 0.3336 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m302/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2199 - mean_absolute_error: 0.3505\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2198 - mean_absolute_error: 0.3503 - val_loss: 0.2192 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2135 - mean_absolute_error: 0.3431 - val_loss: 0.2142 - val_mean_absolute_error: 0.3217 - learning_rate: 2.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2091 - mean_absolute_error: 0.3406 - val_loss: 0.2142 - val_mean_absolute_error: 0.3232 - learning_rate: 2.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2091 - mean_absolute_error: 0.3373 - val_loss: 0.2142 - val_mean_absolute_error: 0.3249 - learning_rate: 2.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2144 - mean_absolute_error: 0.3434 - val_loss: 0.2144 - val_mean_absolute_error: 0.3242 - learning_rate: 2.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2170 - mean_absolute_error: 0.3459\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2170 - mean_absolute_error: 0.3459 - val_loss: 0.2148 - val_mean_absolute_error: 0.3233 - learning_rate: 2.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2014 - mean_absolute_error: 0.3302 - val_loss: 0.2137 - val_mean_absolute_error: 0.3230 - learning_rate: 4.0000e-05\n",
            "Epoch 28/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2083 - mean_absolute_error: 0.3376 - val_loss: 0.2136 - val_mean_absolute_error: 0.3239 - learning_rate: 4.0000e-05\n",
            "Epoch 29/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2085 - mean_absolute_error: 0.3357 - val_loss: 0.2138 - val_mean_absolute_error: 0.3218 - learning_rate: 4.0000e-05\n",
            "Epoch 30/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2026 - mean_absolute_error: 0.3286 - val_loss: 0.2132 - val_mean_absolute_error: 0.3247 - learning_rate: 4.0000e-05\n",
            "Epoch 31/200\n",
            "\u001b[1m316/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2061 - mean_absolute_error: 0.3357\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2061 - mean_absolute_error: 0.3357 - val_loss: 0.2134 - val_mean_absolute_error: 0.3224 - learning_rate: 4.0000e-05\n",
            "Epoch 32/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2084 - mean_absolute_error: 0.3352 - val_loss: 0.2135 - val_mean_absolute_error: 0.3227 - learning_rate: 8.0000e-06\n",
            "Epoch 33/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2127 - mean_absolute_error: 0.3376 - val_loss: 0.2134 - val_mean_absolute_error: 0.3228 - learning_rate: 8.0000e-06\n",
            "Epoch 34/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2098 - mean_absolute_error: 0.3371 - val_loss: 0.2134 - val_mean_absolute_error: 0.3239 - learning_rate: 8.0000e-06\n",
            "Epoch 35/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2057 - mean_absolute_error: 0.3337 - val_loss: 0.2135 - val_mean_absolute_error: 0.3237 - learning_rate: 8.0000e-06\n",
            "Epoch 35: early stopping\n",
            "Restoring model weights from the end of the best epoch: 20.\n",
            "Validation MAE: 0.3336\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3336\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_binned_optimized_he.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),  # Increased dropout for robustness\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Linear for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=150,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=64,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_3.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3nEduOeHeCK",
        "outputId": "30b97d6c-36fd-4cfa-b393-ab8718b29ec5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - loss: 1.1997 - mean_absolute_error: 0.7937 - val_loss: 0.4428 - val_mean_absolute_error: 0.3918 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4764 - mean_absolute_error: 0.4799 - val_loss: 0.3491 - val_mean_absolute_error: 0.3564 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3922 - mean_absolute_error: 0.4244 - val_loss: 0.3185 - val_mean_absolute_error: 0.3332 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3597 - mean_absolute_error: 0.3975 - val_loss: 0.3109 - val_mean_absolute_error: 0.3310 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3429 - mean_absolute_error: 0.3846 - val_loss: 0.3196 - val_mean_absolute_error: 0.3356 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3270 - mean_absolute_error: 0.3712 - val_loss: 0.3058 - val_mean_absolute_error: 0.3238 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3215 - mean_absolute_error: 0.3654 - val_loss: 0.2979 - val_mean_absolute_error: 0.3221 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3236 - mean_absolute_error: 0.3704 - val_loss: 0.2996 - val_mean_absolute_error: 0.3252 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3100 - mean_absolute_error: 0.3590 - val_loss: 0.2917 - val_mean_absolute_error: 0.3272 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3019 - mean_absolute_error: 0.3526 - val_loss: 0.2994 - val_mean_absolute_error: 0.3218 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3049 - mean_absolute_error: 0.3579 - val_loss: 0.3031 - val_mean_absolute_error: 0.3246 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3013 - mean_absolute_error: 0.3530 - val_loss: 0.2875 - val_mean_absolute_error: 0.3162 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3004 - mean_absolute_error: 0.3547 - val_loss: 0.2828 - val_mean_absolute_error: 0.3339 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2917 - mean_absolute_error: 0.3502 - val_loss: 0.2829 - val_mean_absolute_error: 0.3366 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2887 - mean_absolute_error: 0.3480 - val_loss: 0.2797 - val_mean_absolute_error: 0.3293 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2909 - mean_absolute_error: 0.3522 - val_loss: 0.2781 - val_mean_absolute_error: 0.3245 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2855 - mean_absolute_error: 0.3475 - val_loss: 0.2743 - val_mean_absolute_error: 0.3385 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - mean_absolute_error: 0.3466 - val_loss: 0.2731 - val_mean_absolute_error: 0.3249 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2817 - mean_absolute_error: 0.3523 - val_loss: 0.2745 - val_mean_absolute_error: 0.3212 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2749 - mean_absolute_error: 0.3480 - val_loss: 0.2679 - val_mean_absolute_error: 0.3258 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2722 - mean_absolute_error: 0.3485 - val_loss: 0.2687 - val_mean_absolute_error: 0.3243 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2679 - mean_absolute_error: 0.3429 - val_loss: 0.2775 - val_mean_absolute_error: 0.3337 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2690 - mean_absolute_error: 0.3458 - val_loss: 0.2655 - val_mean_absolute_error: 0.3181 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2634 - mean_absolute_error: 0.3432 - val_loss: 0.2589 - val_mean_absolute_error: 0.3305 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2672 - mean_absolute_error: 0.3476 - val_loss: 0.2586 - val_mean_absolute_error: 0.3268 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2507 - mean_absolute_error: 0.3336 - val_loss: 0.2559 - val_mean_absolute_error: 0.3330 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2554 - mean_absolute_error: 0.3411 - val_loss: 0.2538 - val_mean_absolute_error: 0.3292 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2648 - mean_absolute_error: 0.3541 - val_loss: 0.2514 - val_mean_absolute_error: 0.3326 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2602 - mean_absolute_error: 0.3494 - val_loss: 0.2488 - val_mean_absolute_error: 0.3241 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2478 - mean_absolute_error: 0.3373 - val_loss: 0.2492 - val_mean_absolute_error: 0.3313 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2540 - mean_absolute_error: 0.3468 - val_loss: 0.2450 - val_mean_absolute_error: 0.3324 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2444 - mean_absolute_error: 0.3380 - val_loss: 0.2520 - val_mean_absolute_error: 0.3277 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2437 - mean_absolute_error: 0.3355 - val_loss: 0.2448 - val_mean_absolute_error: 0.3248 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2508 - mean_absolute_error: 0.3480 - val_loss: 0.2452 - val_mean_absolute_error: 0.3217 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2461 - mean_absolute_error: 0.3409 - val_loss: 0.2404 - val_mean_absolute_error: 0.3294 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2407 - mean_absolute_error: 0.3356 - val_loss: 0.2433 - val_mean_absolute_error: 0.3339 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2443 - mean_absolute_error: 0.3421 - val_loss: 0.2433 - val_mean_absolute_error: 0.3173 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2459 - mean_absolute_error: 0.3412 - val_loss: 0.2413 - val_mean_absolute_error: 0.3283 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2355 - mean_absolute_error: 0.3354 - val_loss: 0.2425 - val_mean_absolute_error: 0.3141 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2289 - mean_absolute_error: 0.3261 - val_loss: 0.2384 - val_mean_absolute_error: 0.3232 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2370 - mean_absolute_error: 0.3373 - val_loss: 0.2481 - val_mean_absolute_error: 0.3292 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2419 - mean_absolute_error: 0.3385 - val_loss: 0.2350 - val_mean_absolute_error: 0.3178 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2279 - mean_absolute_error: 0.3311 - val_loss: 0.2372 - val_mean_absolute_error: 0.3430 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2377 - mean_absolute_error: 0.3431 - val_loss: 0.2321 - val_mean_absolute_error: 0.3281 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2315 - mean_absolute_error: 0.3339 - val_loss: 0.2365 - val_mean_absolute_error: 0.3362 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2318 - mean_absolute_error: 0.3339 - val_loss: 0.2333 - val_mean_absolute_error: 0.3380 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2318 - mean_absolute_error: 0.3382 - val_loss: 0.2336 - val_mean_absolute_error: 0.3364 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2297 - mean_absolute_error: 0.3337 - val_loss: 0.2335 - val_mean_absolute_error: 0.3312 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/150\n",
            "\u001b[1m155/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2229 - mean_absolute_error: 0.3296\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2231 - mean_absolute_error: 0.3297 - val_loss: 0.2330 - val_mean_absolute_error: 0.3279 - learning_rate: 5.0000e-04\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2231 - mean_absolute_error: 0.3284 - val_loss: 0.2334 - val_mean_absolute_error: 0.3335 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2190 - mean_absolute_error: 0.3260 - val_loss: 0.2334 - val_mean_absolute_error: 0.3269 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2263 - mean_absolute_error: 0.3313 - val_loss: 0.2313 - val_mean_absolute_error: 0.3276 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2248 - mean_absolute_error: 0.3312 - val_loss: 0.2316 - val_mean_absolute_error: 0.3276 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2197 - mean_absolute_error: 0.3255 - val_loss: 0.2313 - val_mean_absolute_error: 0.3266 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2242 - mean_absolute_error: 0.3297 - val_loss: 0.2313 - val_mean_absolute_error: 0.3244 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2268 - mean_absolute_error: 0.3308 - val_loss: 0.2314 - val_mean_absolute_error: 0.3266 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/150\n",
            "\u001b[1m156/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2207 - mean_absolute_error: 0.3249\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2207 - mean_absolute_error: 0.3250 - val_loss: 0.2315 - val_mean_absolute_error: 0.3255 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2181 - mean_absolute_error: 0.3271 - val_loss: 0.2312 - val_mean_absolute_error: 0.3234 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2164 - mean_absolute_error: 0.3216 - val_loss: 0.2310 - val_mean_absolute_error: 0.3267 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2199 - mean_absolute_error: 0.3272 - val_loss: 0.2312 - val_mean_absolute_error: 0.3256 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2188 - mean_absolute_error: 0.3254 - val_loss: 0.2312 - val_mean_absolute_error: 0.3246 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2225 - mean_absolute_error: 0.3275 - val_loss: 0.2316 - val_mean_absolute_error: 0.3251 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2193 - mean_absolute_error: 0.3270 - val_loss: 0.2314 - val_mean_absolute_error: 0.3243 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2155 - mean_absolute_error: 0.3216 - val_loss: 0.2308 - val_mean_absolute_error: 0.3274 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2156 - mean_absolute_error: 0.3244 - val_loss: 0.2314 - val_mean_absolute_error: 0.3257 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2177 - mean_absolute_error: 0.3261 - val_loss: 0.2309 - val_mean_absolute_error: 0.3238 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2131 - mean_absolute_error: 0.3207 - val_loss: 0.2309 - val_mean_absolute_error: 0.3285 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 68/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2205 - mean_absolute_error: 0.3306 - val_loss: 0.2308 - val_mean_absolute_error: 0.3255 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 69/150\n",
            "\u001b[1m154/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2134 - mean_absolute_error: 0.3222\n",
            "Epoch 69: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2135 - mean_absolute_error: 0.3223 - val_loss: 0.2308 - val_mean_absolute_error: 0.3231 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 70/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2107 - mean_absolute_error: 0.3182 - val_loss: 0.2308 - val_mean_absolute_error: 0.3249 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 71/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2162 - mean_absolute_error: 0.3223 - val_loss: 0.2310 - val_mean_absolute_error: 0.3203 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 72/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2134 - mean_absolute_error: 0.3183 - val_loss: 0.2308 - val_mean_absolute_error: 0.3220 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 73/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2106 - mean_absolute_error: 0.3160 - val_loss: 0.2307 - val_mean_absolute_error: 0.3239 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 74/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2116 - mean_absolute_error: 0.3200 - val_loss: 0.2306 - val_mean_absolute_error: 0.3243 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 75/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2208 - mean_absolute_error: 0.3254 - val_loss: 0.2301 - val_mean_absolute_error: 0.3251 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 76/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2238 - mean_absolute_error: 0.3312 - val_loss: 0.2305 - val_mean_absolute_error: 0.3223 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 77/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2093 - mean_absolute_error: 0.3183 - val_loss: 0.2300 - val_mean_absolute_error: 0.3291 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 78/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2125 - mean_absolute_error: 0.3235 - val_loss: 0.2305 - val_mean_absolute_error: 0.3237 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 79/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2124 - mean_absolute_error: 0.3215 - val_loss: 0.2301 - val_mean_absolute_error: 0.3222 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 80/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2170 - mean_absolute_error: 0.3245 - val_loss: 0.2301 - val_mean_absolute_error: 0.3236 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 81/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2223 - mean_absolute_error: 0.3281 - val_loss: 0.2301 - val_mean_absolute_error: 0.3225 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 82/150\n",
            "\u001b[1m147/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2135 - mean_absolute_error: 0.3188\n",
            "Epoch 82: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2134 - mean_absolute_error: 0.3188 - val_loss: 0.2303 - val_mean_absolute_error: 0.3224 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 83/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2177 - mean_absolute_error: 0.3229 - val_loss: 0.2310 - val_mean_absolute_error: 0.3216 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 84/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2168 - mean_absolute_error: 0.3245 - val_loss: 0.2306 - val_mean_absolute_error: 0.3210 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 85/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2145 - mean_absolute_error: 0.3204 - val_loss: 0.2306 - val_mean_absolute_error: 0.3244 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 86/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2136 - mean_absolute_error: 0.3241 - val_loss: 0.2305 - val_mean_absolute_error: 0.3245 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 87/150\n",
            "\u001b[1m144/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2132 - mean_absolute_error: 0.3213\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2131 - mean_absolute_error: 0.3212 - val_loss: 0.2306 - val_mean_absolute_error: 0.3232 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 88/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2108 - mean_absolute_error: 0.3174 - val_loss: 0.2308 - val_mean_absolute_error: 0.3210 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 89/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2171 - mean_absolute_error: 0.3247 - val_loss: 0.2309 - val_mean_absolute_error: 0.3206 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 90/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2140 - mean_absolute_error: 0.3217 - val_loss: 0.2301 - val_mean_absolute_error: 0.3235 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 91/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2127 - mean_absolute_error: 0.3217 - val_loss: 0.2306 - val_mean_absolute_error: 0.3239 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 92/150\n",
            "\u001b[1m139/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2101 - mean_absolute_error: 0.3177\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2103 - mean_absolute_error: 0.3179 - val_loss: 0.2309 - val_mean_absolute_error: 0.3231 - learning_rate: 5.0000e-05\n",
            "Epoch 92: early stopping\n",
            "Restoring model weights from the end of the best epoch: 77.\n",
            "Validation MAE: 0.3291\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3291\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlL3iH3hI611",
        "outputId": "d45e30c6-0601-4d37-c1eb-79429fff2a5a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna-integration[tfkeras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rAMJ_uNJCTp",
        "outputId": "e9fdeef0-115b-446d-db1d-e5e4d88e5431"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna-integration[tfkeras]\n",
            "  Downloading optuna_integration-4.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (4.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from optuna-integration[tfkeras]) (2.17.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna->optuna-integration[tfkeras]) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->optuna-integration[tfkeras]) (0.37.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[tfkeras]) (1.3.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->optuna-integration[tfkeras]) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->optuna-integration[tfkeras]) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[tfkeras]) (3.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->optuna-integration[tfkeras]) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->optuna-integration[tfkeras]) (0.1.2)\n",
            "Downloading optuna_integration-4.1.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna-integration\n",
            "Successfully installed optuna-integration-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    n_units_1 = trial.suggest_int('n_units_1', 128, 512, step=64)  # Layer 1 units\n",
        "    n_units_2 = trial.suggest_int('n_units_2', 64, 256, step=32)   # Layer 2 units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True)   # L2 regularization\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16) # Batch size\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),\n",
        "        Dense(n_units_1, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_units_2, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Regression output\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[TFKerasPruningCallback(trial, 'val_loss')],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return val_mae  # Minimize MAE\n",
        "\n",
        "# Run the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Output the best hyperparameters\n",
        "print(\"Best hyperparameters: \", study.best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFPGGgQBAh1l",
        "outputId": "8bbdc6cd-efe1-47b3-9d61-676602a64821"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-12 14:22:49,548] A new study created in memory with name: no-name-a628e602-3bcc-41ee-9ceb-72523dccc22d\n",
            "[I 2024-12-12 14:24:33,763] Trial 0 finished with value: 0.3653591573238373 and parameters: {'n_units_1': 320, 'n_units_2': 160, 'dropout_rate': 0.4, 'l2_reg': 0.0003259157180374716, 'learning_rate': 0.0014099941051248235, 'batch_size': 16}. Best is trial 0 with value: 0.3653591573238373.\n",
            "[I 2024-12-12 14:25:10,872] Trial 1 finished with value: 0.3498423993587494 and parameters: {'n_units_1': 256, 'n_units_2': 224, 'dropout_rate': 0.5, 'l2_reg': 3.9718448509398066e-05, 'learning_rate': 0.005554251373957166, 'batch_size': 64}. Best is trial 1 with value: 0.3498423993587494.\n",
            "[I 2024-12-12 14:25:39,110] Trial 2 finished with value: 0.36205679178237915 and parameters: {'n_units_1': 384, 'n_units_2': 256, 'dropout_rate': 0.5, 'l2_reg': 0.00019216480186897566, 'learning_rate': 0.0007084974131171848, 'batch_size': 80}. Best is trial 1 with value: 0.3498423993587494.\n",
            "[I 2024-12-12 14:27:14,180] Trial 3 finished with value: 0.3321791887283325 and parameters: {'n_units_1': 192, 'n_units_2': 160, 'dropout_rate': 0.2, 'l2_reg': 5.7645677782459946e-05, 'learning_rate': 0.0027018121867177555, 'batch_size': 16}. Best is trial 3 with value: 0.3321791887283325.\n",
            "[I 2024-12-12 14:27:50,647] Trial 4 finished with value: 0.344661146402359 and parameters: {'n_units_1': 192, 'n_units_2': 96, 'dropout_rate': 0.5, 'l2_reg': 4.218578953903089e-05, 'learning_rate': 0.00011415794944477178, 'batch_size': 48}. Best is trial 3 with value: 0.3321791887283325.\n",
            "[I 2024-12-12 14:29:18,912] Trial 5 finished with value: 0.3491482734680176 and parameters: {'n_units_1': 128, 'n_units_2': 128, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.0459145916192682e-05, 'learning_rate': 0.009278739344417062, 'batch_size': 16}. Best is trial 3 with value: 0.3321791887283325.\n",
            "[I 2024-12-12 14:30:51,457] Trial 6 finished with value: 0.3498231768608093 and parameters: {'n_units_1': 128, 'n_units_2': 224, 'dropout_rate': 0.5, 'l2_reg': 2.476972676524827e-05, 'learning_rate': 0.0007735052609547114, 'batch_size': 16}. Best is trial 3 with value: 0.3321791887283325.\n",
            "[I 2024-12-12 14:30:59,126] Trial 7 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:31:51,799] Trial 8 finished with value: 0.32887136936187744 and parameters: {'n_units_1': 320, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.2397514410225523e-05, 'learning_rate': 0.008679412699164835, 'batch_size': 32}. Best is trial 8 with value: 0.32887136936187744.\n",
            "[I 2024-12-12 14:31:59,704] Trial 9 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:32:06,199] Trial 10 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:32:13,880] Trial 11 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:32:20,651] Trial 12 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:32:28,982] Trial 13 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:33:31,109] Trial 14 finished with value: 0.33859094977378845 and parameters: {'n_units_1': 384, 'n_units_2': 128, 'dropout_rate': 0.2, 'l2_reg': 1.7110471202218524e-05, 'learning_rate': 0.0025099684792704324, 'batch_size': 32}. Best is trial 8 with value: 0.32887136936187744.\n",
            "[I 2024-12-12 14:33:37,501] Trial 15 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:33:45,174] Trial 16 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:34:15,309] Trial 17 finished with value: 0.33213984966278076 and parameters: {'n_units_1': 192, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.9319955019796497e-05, 'learning_rate': 0.005146524747690117, 'batch_size': 64}. Best is trial 8 with value: 0.32887136936187744.\n",
            "[I 2024-12-12 14:34:38,181] Trial 18 finished with value: 0.3226611316204071 and parameters: {'n_units_1': 512, 'n_units_2': 64, 'dropout_rate': 0.4, 'l2_reg': 1.840072210469368e-05, 'learning_rate': 0.006284763778670258, 'batch_size': 96}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:34:45,616] Trial 19 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:34:52,670] Trial 20 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:35:28,198] Trial 21 finished with value: 0.3396289646625519 and parameters: {'n_units_1': 448, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 2.0493760514261696e-05, 'learning_rate': 0.004504395201301381, 'batch_size': 64}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:35:52,466] Trial 22 finished with value: 0.34219229221343994 and parameters: {'n_units_1': 320, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.734465319334723e-05, 'learning_rate': 0.005963809873811612, 'batch_size': 96}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:36:14,845] Trial 23 finished with value: 0.32805025577545166 and parameters: {'n_units_1': 256, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.1507162841617474e-05, 'learning_rate': 0.007188425424429534, 'batch_size': 112}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:36:37,997] Trial 24 finished with value: 0.32951897382736206 and parameters: {'n_units_1': 256, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.1720136411750285e-05, 'learning_rate': 0.0075544688785707855, 'batch_size': 96}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:36:44,643] Trial 25 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:37:10,849] Trial 26 finished with value: 0.33654093742370605 and parameters: {'n_units_1': 320, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.0680610801515982e-05, 'learning_rate': 0.0036015369588429215, 'batch_size': 112}. Best is trial 18 with value: 0.3226611316204071.\n",
            "[I 2024-12-12 14:37:18,301] Trial 27 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:37:24,763] Trial 28 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:37:32,194] Trial 29 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:37:44,189] Trial 30 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-12-12 14:38:07,912] Trial 31 finished with value: 0.31894582509994507 and parameters: {'n_units_1': 256, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.3250308702490959e-05, 'learning_rate': 0.007344257177436144, 'batch_size': 96}. Best is trial 31 with value: 0.31894582509994507.\n",
            "[I 2024-12-12 14:38:33,114] Trial 32 finished with value: 0.317579060792923 and parameters: {'n_units_1': 256, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.5037403615742857e-05, 'learning_rate': 0.004862158014853811, 'batch_size': 80}. Best is trial 32 with value: 0.317579060792923.\n",
            "[I 2024-12-12 14:38:39,718] Trial 33 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:39:04,229] Trial 34 finished with value: 0.344426691532135 and parameters: {'n_units_1': 192, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 2.1349684517003198e-05, 'learning_rate': 0.0061967649692798174, 'batch_size': 80}. Best is trial 32 with value: 0.317579060792923.\n",
            "[I 2024-12-12 14:39:16,973] Trial 35 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-12-12 14:39:24,517] Trial 36 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:39:36,304] Trial 37 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:39:42,700] Trial 38 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:39:50,262] Trial 39 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:39:57,460] Trial 40 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-12-12 14:40:19,984] Trial 41 finished with value: 0.33963248133659363 and parameters: {'n_units_1': 320, 'n_units_2': 64, 'dropout_rate': 0.4, 'l2_reg': 1.3181482127407277e-05, 'learning_rate': 0.009900487642278619, 'batch_size': 96}. Best is trial 32 with value: 0.317579060792923.\n",
            "[I 2024-12-12 14:40:27,853] Trial 42 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-12-12 14:41:00,062] Trial 43 finished with value: 0.34304511547088623 and parameters: {'n_units_1': 320, 'n_units_2': 64, 'dropout_rate': 0.4, 'l2_reg': 1.0065374081269615e-05, 'learning_rate': 0.004000169577731821, 'batch_size': 64}. Best is trial 32 with value: 0.317579060792923.\n",
            "[I 2024-12-12 14:41:12,753] Trial 44 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-12-12 14:41:20,329] Trial 45 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:41:33,169] Trial 46 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:41:39,347] Trial 47 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:41:47,312] Trial 48 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:41:59,970] Trial 49 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'n_units_1': 256, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.5037403615742857e-05, 'learning_rate': 0.004862158014853811, 'batch_size': 80}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final model with best hyperparameters\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "best_params = study.best_params\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(best_params['n_units_1'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(best_params['n_units_2'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(1, activation='linear', kernel_initializer='glorot_uniform')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the optimized model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the optimized model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Optimized Model Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_optuna_1.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_optuna_1.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfWN9X9-E-Mv",
        "outputId": "9fcfb848-9080-47be-c34e-e7b6e2481331"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 1.6213 - mean_absolute_error: 0.9176 - val_loss: 0.2320 - val_mean_absolute_error: 0.3498\n",
            "Epoch 2/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 0.2802 - mean_absolute_error: 0.3953 - val_loss: 0.2340 - val_mean_absolute_error: 0.3481\n",
            "Epoch 3/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2615 - mean_absolute_error: 0.3850 - val_loss: 0.2273 - val_mean_absolute_error: 0.3626\n",
            "Epoch 4/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2516 - mean_absolute_error: 0.3781 - val_loss: 0.2334 - val_mean_absolute_error: 0.3669\n",
            "Epoch 5/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2565 - mean_absolute_error: 0.3809 - val_loss: 0.2236 - val_mean_absolute_error: 0.3559\n",
            "Epoch 6/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2498 - mean_absolute_error: 0.3723 - val_loss: 0.2287 - val_mean_absolute_error: 0.3363\n",
            "Epoch 7/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2441 - mean_absolute_error: 0.3716 - val_loss: 0.2313 - val_mean_absolute_error: 0.3638\n",
            "Epoch 8/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2460 - mean_absolute_error: 0.3688 - val_loss: 0.2371 - val_mean_absolute_error: 0.3837\n",
            "Epoch 9/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2557 - mean_absolute_error: 0.3823 - val_loss: 0.2226 - val_mean_absolute_error: 0.3484\n",
            "Epoch 10/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2435 - mean_absolute_error: 0.3661 - val_loss: 0.2204 - val_mean_absolute_error: 0.3365\n",
            "Epoch 11/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2397 - mean_absolute_error: 0.3629 - val_loss: 0.2247 - val_mean_absolute_error: 0.3571\n",
            "Epoch 12/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2471 - mean_absolute_error: 0.3696 - val_loss: 0.2280 - val_mean_absolute_error: 0.3611\n",
            "Epoch 13/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2362 - mean_absolute_error: 0.3611 - val_loss: 0.2194 - val_mean_absolute_error: 0.3410\n",
            "Epoch 14/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2430 - mean_absolute_error: 0.3646 - val_loss: 0.2208 - val_mean_absolute_error: 0.3522\n",
            "Epoch 15/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2359 - mean_absolute_error: 0.3608 - val_loss: 0.2257 - val_mean_absolute_error: 0.3508\n",
            "Epoch 16/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2309 - mean_absolute_error: 0.3561 - val_loss: 0.2275 - val_mean_absolute_error: 0.3581\n",
            "Epoch 17/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2377 - mean_absolute_error: 0.3606 - val_loss: 0.2194 - val_mean_absolute_error: 0.3383\n",
            "Epoch 18/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2298 - mean_absolute_error: 0.3585 - val_loss: 0.2294 - val_mean_absolute_error: 0.3552\n",
            "Epoch 19/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2437 - mean_absolute_error: 0.3639 - val_loss: 0.2228 - val_mean_absolute_error: 0.3468\n",
            "Epoch 20/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2319 - mean_absolute_error: 0.3594 - val_loss: 0.2202 - val_mean_absolute_error: 0.3473\n",
            "Epoch 21/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2322 - mean_absolute_error: 0.3597 - val_loss: 0.2198 - val_mean_absolute_error: 0.3260\n",
            "Epoch 22/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2389 - mean_absolute_error: 0.3621 - val_loss: 0.2274 - val_mean_absolute_error: 0.3602\n",
            "Epoch 23/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2353 - mean_absolute_error: 0.3549 - val_loss: 0.2214 - val_mean_absolute_error: 0.3390\n",
            "Epoch 24/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2329 - mean_absolute_error: 0.3553 - val_loss: 0.2206 - val_mean_absolute_error: 0.3479\n",
            "Epoch 25/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2277 - mean_absolute_error: 0.3537 - val_loss: 0.2168 - val_mean_absolute_error: 0.3285\n",
            "Epoch 26/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2305 - mean_absolute_error: 0.3532 - val_loss: 0.2208 - val_mean_absolute_error: 0.3457\n",
            "Epoch 27/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2290 - mean_absolute_error: 0.3475 - val_loss: 0.2193 - val_mean_absolute_error: 0.3272\n",
            "Epoch 28/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2288 - mean_absolute_error: 0.3494 - val_loss: 0.2185 - val_mean_absolute_error: 0.3371\n",
            "Epoch 29/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2255 - mean_absolute_error: 0.3500 - val_loss: 0.2195 - val_mean_absolute_error: 0.3402\n",
            "Epoch 30/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2204 - mean_absolute_error: 0.3444 - val_loss: 0.2175 - val_mean_absolute_error: 0.3325\n",
            "Epoch 31/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2165 - mean_absolute_error: 0.3383 - val_loss: 0.2188 - val_mean_absolute_error: 0.3429\n",
            "Epoch 32/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2294 - mean_absolute_error: 0.3508 - val_loss: 0.2184 - val_mean_absolute_error: 0.3349\n",
            "Epoch 33/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2274 - mean_absolute_error: 0.3463 - val_loss: 0.2187 - val_mean_absolute_error: 0.3327\n",
            "Epoch 34/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2210 - mean_absolute_error: 0.3415 - val_loss: 0.2215 - val_mean_absolute_error: 0.3462\n",
            "Epoch 35/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2241 - mean_absolute_error: 0.3446 - val_loss: 0.2183 - val_mean_absolute_error: 0.3284\n",
            "Epoch 36/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2245 - mean_absolute_error: 0.3461 - val_loss: 0.2179 - val_mean_absolute_error: 0.3405\n",
            "Epoch 37/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2264 - mean_absolute_error: 0.3519 - val_loss: 0.2163 - val_mean_absolute_error: 0.3359\n",
            "Epoch 38/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2209 - mean_absolute_error: 0.3440 - val_loss: 0.2213 - val_mean_absolute_error: 0.3303\n",
            "Epoch 39/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2152 - mean_absolute_error: 0.3398 - val_loss: 0.2187 - val_mean_absolute_error: 0.3437\n",
            "Epoch 40/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2217 - mean_absolute_error: 0.3473 - val_loss: 0.2167 - val_mean_absolute_error: 0.3392\n",
            "Epoch 41/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2176 - mean_absolute_error: 0.3416 - val_loss: 0.2193 - val_mean_absolute_error: 0.3458\n",
            "Epoch 42/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2255 - mean_absolute_error: 0.3442 - val_loss: 0.2169 - val_mean_absolute_error: 0.3414\n",
            "Epoch 43/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2213 - mean_absolute_error: 0.3418 - val_loss: 0.2167 - val_mean_absolute_error: 0.3415\n",
            "Epoch 44/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2217 - mean_absolute_error: 0.3487 - val_loss: 0.2197 - val_mean_absolute_error: 0.3236\n",
            "Epoch 45/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2235 - mean_absolute_error: 0.3469 - val_loss: 0.2225 - val_mean_absolute_error: 0.3389\n",
            "Epoch 46/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2181 - mean_absolute_error: 0.3383 - val_loss: 0.2204 - val_mean_absolute_error: 0.3291\n",
            "Epoch 47/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2217 - mean_absolute_error: 0.3433 - val_loss: 0.2203 - val_mean_absolute_error: 0.3302\n",
            "Epoch 48/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2221 - mean_absolute_error: 0.3446 - val_loss: 0.2167 - val_mean_absolute_error: 0.3360\n",
            "Epoch 49/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2288 - mean_absolute_error: 0.3544 - val_loss: 0.2193 - val_mean_absolute_error: 0.3320\n",
            "Epoch 50/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2136 - mean_absolute_error: 0.3368 - val_loss: 0.2156 - val_mean_absolute_error: 0.3429\n",
            "Epoch 51/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2184 - mean_absolute_error: 0.3412 - val_loss: 0.2213 - val_mean_absolute_error: 0.3164\n",
            "Epoch 52/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2241 - mean_absolute_error: 0.3436 - val_loss: 0.2176 - val_mean_absolute_error: 0.3226\n",
            "Epoch 53/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2221 - mean_absolute_error: 0.3438 - val_loss: 0.2197 - val_mean_absolute_error: 0.3368\n",
            "Epoch 54/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2216 - mean_absolute_error: 0.3450 - val_loss: 0.2186 - val_mean_absolute_error: 0.3274\n",
            "Epoch 55/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2221 - mean_absolute_error: 0.3419 - val_loss: 0.2191 - val_mean_absolute_error: 0.3348\n",
            "Epoch 56/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2124 - mean_absolute_error: 0.3357 - val_loss: 0.2166 - val_mean_absolute_error: 0.3264\n",
            "Epoch 57/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2224 - mean_absolute_error: 0.3427 - val_loss: 0.2212 - val_mean_absolute_error: 0.3318\n",
            "Epoch 58/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2222 - mean_absolute_error: 0.3429 - val_loss: 0.2188 - val_mean_absolute_error: 0.3498\n",
            "Epoch 59/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2122 - mean_absolute_error: 0.3321 - val_loss: 0.2184 - val_mean_absolute_error: 0.3432\n",
            "Epoch 60/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2158 - mean_absolute_error: 0.3394 - val_loss: 0.2186 - val_mean_absolute_error: 0.3301\n",
            "Epoch 61/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2222 - mean_absolute_error: 0.3451 - val_loss: 0.2264 - val_mean_absolute_error: 0.3427\n",
            "Epoch 62/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2228 - mean_absolute_error: 0.3454 - val_loss: 0.2215 - val_mean_absolute_error: 0.3485\n",
            "Epoch 63/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2206 - mean_absolute_error: 0.3419 - val_loss: 0.2182 - val_mean_absolute_error: 0.3342\n",
            "Epoch 64/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2201 - mean_absolute_error: 0.3429 - val_loss: 0.2196 - val_mean_absolute_error: 0.3446\n",
            "Epoch 65/200\n",
            "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2242 - mean_absolute_error: 0.3421 - val_loss: 0.2206 - val_mean_absolute_error: 0.3272\n",
            "Epoch 65: early stopping\n",
            "Restoring model weights from the end of the best epoch: 50.\n",
            "Optimized Model Validation MAE: 0.3429\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_optuna_1.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_optuna_1.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_optuna_1.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNcKrgrDGA70",
        "outputId": "85e03909-39cb-4c0c-f70d-74f239d7a8ff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_optuna_1.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Define Optuna objective function\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    n_units_1 = trial.suggest_int('n_units_1', 128, 512, step=64)  # Layer 1 units\n",
        "    n_units_2 = trial.suggest_int('n_units_2', 64, 256, step=32)   # Layer 2 units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5, step=0.1)\n",
        "    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True)   # L2 regularization\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16) # Batch size\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),\n",
        "        Dense(n_units_1, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(n_units_2, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Regression output\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[TFKerasPruningCallback(trial, 'val_loss')],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return val_mae  # Minimize MAE\n",
        "\n",
        "# Run the Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Output the best hyperparameters\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "\n",
        "# Train the final model with best hyperparameters\n",
        "best_params = study.best_params\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(best_params['n_units_1'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(best_params['n_units_2'], activation='relu', kernel_initializer=HeNormal(),\n",
        "          kernel_regularizer=l2(best_params['l2_reg'])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(best_params['dropout_rate']),\n",
        "    Dense(1, activation='linear', kernel_initializer='glorot_uniform')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Train the optimized model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the optimized model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Optimized Model Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "# Save predictions to a CSV file\n",
        "# y_val_pred = model.predict(X_val)\n",
        "test_predictions = model.predict(scaler.transform(test_df_encoded))\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "})\n",
        "results_df.to_csv('submission_nn_optimized_optuna.csv', index=False)\n",
        "print(\"Predictions saved to 'submission_nn_optimized_optuna.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8ToLVaINgB",
        "outputId": "c3c3d5a3-1452-4ad1-d425-41908b6dc015"
      },
      "execution_count": 45,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-12 14:43:02,040] A new study created in memory with name: no-name-2afb2d74-16c0-463f-8464-189e9213e59d\n",
            "[I 2024-12-12 14:43:25,631] Trial 0 finished with value: 0.3466989994049072 and parameters: {'n_units_1': 512, 'n_units_2': 224, 'dropout_rate': 0.2, 'l2_reg': 0.00023698386675070167, 'learning_rate': 0.0002824327603169769, 'batch_size': 112}. Best is trial 0 with value: 0.3466989994049072.\n",
            "[I 2024-12-12 14:43:47,735] Trial 1 finished with value: 0.3290501832962036 and parameters: {'n_units_1': 448, 'n_units_2': 192, 'dropout_rate': 0.4, 'l2_reg': 0.00037344148426143127, 'learning_rate': 0.008739953981494632, 'batch_size': 112}. Best is trial 1 with value: 0.3290501832962036.\n",
            "[I 2024-12-12 14:45:11,804] Trial 2 finished with value: 0.3258601725101471 and parameters: {'n_units_1': 192, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 0.0002245534380362104, 'learning_rate': 0.0014372804894736701, 'batch_size': 16}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:45:50,793] Trial 3 finished with value: 0.3309251070022583 and parameters: {'n_units_1': 384, 'n_units_2': 128, 'dropout_rate': 0.2, 'l2_reg': 6.588429907333823e-05, 'learning_rate': 0.001895377696078378, 'batch_size': 48}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:47:25,574] Trial 4 finished with value: 0.3498185873031616 and parameters: {'n_units_1': 128, 'n_units_2': 128, 'dropout_rate': 0.5, 'l2_reg': 6.834370881835379e-05, 'learning_rate': 0.0002792811136666863, 'batch_size': 16}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:47:33,031] Trial 5 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-12-12 14:47:56,258] Trial 6 pruned. Trial was pruned at epoch 34.\n",
            "[I 2024-12-12 14:48:04,321] Trial 7 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:48:17,464] Trial 8 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:48:29,160] Trial 9 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:50:02,970] Trial 10 finished with value: 0.32930701971054077 and parameters: {'n_units_1': 192, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 3.068342801291862e-05, 'learning_rate': 0.0007593634688123992, 'batch_size': 16}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:50:10,621] Trial 11 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:50:18,236] Trial 12 pruned. Trial was pruned at epoch 5.\n",
            "[I 2024-12-12 14:50:42,504] Trial 13 finished with value: 0.3369716703891754 and parameters: {'n_units_1': 320, 'n_units_2': 192, 'dropout_rate': 0.4, 'l2_reg': 1.1193500400510183e-05, 'learning_rate': 0.00884621566058811, 'batch_size': 80}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:50:49,877] Trial 14 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:51:02,330] Trial 15 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:51:08,764] Trial 16 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:51:17,570] Trial 17 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-12-12 14:51:25,191] Trial 18 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:51:31,743] Trial 19 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:51:39,568] Trial 20 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:53:17,962] Trial 21 finished with value: 0.33490490913391113 and parameters: {'n_units_1': 192, 'n_units_2': 64, 'dropout_rate': 0.30000000000000004, 'l2_reg': 2.779899496474119e-05, 'learning_rate': 0.000665953512333868, 'batch_size': 16}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:53:24,119] Trial 22 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:54:53,031] Trial 23 finished with value: 0.33766236901283264 and parameters: {'n_units_1': 192, 'n_units_2': 96, 'dropout_rate': 0.30000000000000004, 'l2_reg': 1.394918911435747e-05, 'learning_rate': 0.0003211356354651317, 'batch_size': 16}. Best is trial 2 with value: 0.3258601725101471.\n",
            "[I 2024-12-12 14:55:14,394] Trial 24 pruned. Trial was pruned at epoch 15.\n",
            "[I 2024-12-12 14:55:22,782] Trial 25 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:55:59,868] Trial 26 finished with value: 0.3235953748226166 and parameters: {'n_units_1': 128, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.8237511717032932e-05, 'learning_rate': 0.0023467274045567386, 'batch_size': 64}. Best is trial 26 with value: 0.3235953748226166.\n",
            "[I 2024-12-12 14:56:30,565] Trial 27 finished with value: 0.33608636260032654 and parameters: {'n_units_1': 128, 'n_units_2': 160, 'dropout_rate': 0.5, 'l2_reg': 1.7502408089324282e-05, 'learning_rate': 0.0022504306553265555, 'batch_size': 64}. Best is trial 26 with value: 0.3235953748226166.\n",
            "[I 2024-12-12 14:56:38,759] Trial 28 pruned. Trial was pruned at epoch 2.\n",
            "[I 2024-12-12 14:56:45,083] Trial 29 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:56:52,756] Trial 30 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:57:31,059] Trial 31 finished with value: 0.3374730944633484 and parameters: {'n_units_1': 192, 'n_units_2': 64, 'dropout_rate': 0.4, 'l2_reg': 2.2203563199903815e-05, 'learning_rate': 0.001514408535838363, 'batch_size': 48}. Best is trial 26 with value: 0.3235953748226166.\n",
            "[I 2024-12-12 14:57:37,241] Trial 32 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:59:18,860] Trial 33 finished with value: 0.3538702130317688 and parameters: {'n_units_1': 128, 'n_units_2': 64, 'dropout_rate': 0.4, 'l2_reg': 1.0144108375839064e-05, 'learning_rate': 0.0011139434335656355, 'batch_size': 16}. Best is trial 26 with value: 0.3235953748226166.\n",
            "[I 2024-12-12 14:59:26,507] Trial 34 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:59:33,744] Trial 35 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:59:39,963] Trial 36 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 14:59:48,113] Trial 37 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:03,737] Trial 38 pruned. Trial was pruned at epoch 1.\n",
            "[I 2024-12-12 15:00:10,168] Trial 39 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:16,942] Trial 40 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:24,646] Trial 41 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:30,865] Trial 42 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:39,624] Trial 43 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:00:49,896] Trial 44 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:01:01,571] Trial 45 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:01:10,014] Trial 46 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:01:17,789] Trial 47 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:01:24,419] Trial 48 pruned. Trial was pruned at epoch 0.\n",
            "[I 2024-12-12 15:01:31,758] Trial 49 pruned. Trial was pruned at epoch 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:  {'n_units_1': 128, 'n_units_2': 96, 'dropout_rate': 0.4, 'l2_reg': 1.8237511717032932e-05, 'learning_rate': 0.0023467274045567386, 'batch_size': 64}\n",
            "Epoch 1/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 1.6119 - mean_absolute_error: 0.9470 - val_loss: 0.2406 - val_mean_absolute_error: 0.3590\n",
            "Epoch 2/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3404 - mean_absolute_error: 0.4429 - val_loss: 0.2298 - val_mean_absolute_error: 0.3543\n",
            "Epoch 3/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2760 - mean_absolute_error: 0.3950 - val_loss: 0.2251 - val_mean_absolute_error: 0.3560\n",
            "Epoch 4/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2613 - mean_absolute_error: 0.3844 - val_loss: 0.2247 - val_mean_absolute_error: 0.3519\n",
            "Epoch 5/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2522 - mean_absolute_error: 0.3784 - val_loss: 0.2243 - val_mean_absolute_error: 0.3552\n",
            "Epoch 6/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2602 - mean_absolute_error: 0.3848 - val_loss: 0.2244 - val_mean_absolute_error: 0.3395\n",
            "Epoch 7/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2490 - mean_absolute_error: 0.3759 - val_loss: 0.2234 - val_mean_absolute_error: 0.3454\n",
            "Epoch 8/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2573 - mean_absolute_error: 0.3824 - val_loss: 0.2237 - val_mean_absolute_error: 0.3473\n",
            "Epoch 9/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2485 - mean_absolute_error: 0.3723 - val_loss: 0.2206 - val_mean_absolute_error: 0.3453\n",
            "Epoch 10/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2541 - mean_absolute_error: 0.3807 - val_loss: 0.2214 - val_mean_absolute_error: 0.3443\n",
            "Epoch 11/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2494 - mean_absolute_error: 0.3741 - val_loss: 0.2211 - val_mean_absolute_error: 0.3563\n",
            "Epoch 12/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2364 - mean_absolute_error: 0.3634 - val_loss: 0.2234 - val_mean_absolute_error: 0.3543\n",
            "Epoch 13/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2431 - mean_absolute_error: 0.3722 - val_loss: 0.2205 - val_mean_absolute_error: 0.3462\n",
            "Epoch 14/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2408 - mean_absolute_error: 0.3675 - val_loss: 0.2213 - val_mean_absolute_error: 0.3519\n",
            "Epoch 15/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2425 - mean_absolute_error: 0.3699 - val_loss: 0.2236 - val_mean_absolute_error: 0.3406\n",
            "Epoch 16/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2413 - mean_absolute_error: 0.3671 - val_loss: 0.2181 - val_mean_absolute_error: 0.3383\n",
            "Epoch 17/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2474 - mean_absolute_error: 0.3709 - val_loss: 0.2188 - val_mean_absolute_error: 0.3384\n",
            "Epoch 18/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2383 - mean_absolute_error: 0.3647 - val_loss: 0.2181 - val_mean_absolute_error: 0.3471\n",
            "Epoch 19/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2360 - mean_absolute_error: 0.3610 - val_loss: 0.2178 - val_mean_absolute_error: 0.3427\n",
            "Epoch 20/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2409 - mean_absolute_error: 0.3659 - val_loss: 0.2183 - val_mean_absolute_error: 0.3478\n",
            "Epoch 21/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2309 - mean_absolute_error: 0.3579 - val_loss: 0.2208 - val_mean_absolute_error: 0.3335\n",
            "Epoch 22/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2337 - mean_absolute_error: 0.3574 - val_loss: 0.2189 - val_mean_absolute_error: 0.3592\n",
            "Epoch 23/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2411 - mean_absolute_error: 0.3695 - val_loss: 0.2181 - val_mean_absolute_error: 0.3305\n",
            "Epoch 24/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2356 - mean_absolute_error: 0.3612 - val_loss: 0.2191 - val_mean_absolute_error: 0.3428\n",
            "Epoch 25/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2420 - mean_absolute_error: 0.3667 - val_loss: 0.2185 - val_mean_absolute_error: 0.3573\n",
            "Epoch 26/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2379 - mean_absolute_error: 0.3671 - val_loss: 0.2197 - val_mean_absolute_error: 0.3392\n",
            "Epoch 27/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2280 - mean_absolute_error: 0.3538 - val_loss: 0.2183 - val_mean_absolute_error: 0.3409\n",
            "Epoch 28/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2356 - mean_absolute_error: 0.3582 - val_loss: 0.2224 - val_mean_absolute_error: 0.3502\n",
            "Epoch 29/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2238 - mean_absolute_error: 0.3521 - val_loss: 0.2255 - val_mean_absolute_error: 0.3699\n",
            "Epoch 30/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2343 - mean_absolute_error: 0.3606 - val_loss: 0.2164 - val_mean_absolute_error: 0.3292\n",
            "Epoch 31/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2340 - mean_absolute_error: 0.3586 - val_loss: 0.2271 - val_mean_absolute_error: 0.3630\n",
            "Epoch 32/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2285 - mean_absolute_error: 0.3554 - val_loss: 0.2152 - val_mean_absolute_error: 0.3440\n",
            "Epoch 33/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2308 - mean_absolute_error: 0.3599 - val_loss: 0.2178 - val_mean_absolute_error: 0.3416\n",
            "Epoch 34/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2349 - mean_absolute_error: 0.3584 - val_loss: 0.2165 - val_mean_absolute_error: 0.3415\n",
            "Epoch 35/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2281 - mean_absolute_error: 0.3518 - val_loss: 0.2165 - val_mean_absolute_error: 0.3250\n",
            "Epoch 36/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2286 - mean_absolute_error: 0.3522 - val_loss: 0.2194 - val_mean_absolute_error: 0.3579\n",
            "Epoch 37/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2326 - mean_absolute_error: 0.3602 - val_loss: 0.2148 - val_mean_absolute_error: 0.3356\n",
            "Epoch 38/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2304 - mean_absolute_error: 0.3578 - val_loss: 0.2171 - val_mean_absolute_error: 0.3445\n",
            "Epoch 39/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2254 - mean_absolute_error: 0.3541 - val_loss: 0.2165 - val_mean_absolute_error: 0.3437\n",
            "Epoch 40/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2302 - mean_absolute_error: 0.3530 - val_loss: 0.2150 - val_mean_absolute_error: 0.3419\n",
            "Epoch 41/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2248 - mean_absolute_error: 0.3497 - val_loss: 0.2204 - val_mean_absolute_error: 0.3483\n",
            "Epoch 42/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2319 - mean_absolute_error: 0.3591 - val_loss: 0.2149 - val_mean_absolute_error: 0.3335\n",
            "Epoch 43/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2222 - mean_absolute_error: 0.3472 - val_loss: 0.2161 - val_mean_absolute_error: 0.3345\n",
            "Epoch 44/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2307 - mean_absolute_error: 0.3553 - val_loss: 0.2196 - val_mean_absolute_error: 0.3563\n",
            "Epoch 45/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2226 - mean_absolute_error: 0.3516 - val_loss: 0.2164 - val_mean_absolute_error: 0.3370\n",
            "Epoch 46/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2264 - mean_absolute_error: 0.3505 - val_loss: 0.2144 - val_mean_absolute_error: 0.3280\n",
            "Epoch 47/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2299 - mean_absolute_error: 0.3538 - val_loss: 0.2151 - val_mean_absolute_error: 0.3520\n",
            "Epoch 48/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2187 - mean_absolute_error: 0.3481 - val_loss: 0.2236 - val_mean_absolute_error: 0.3565\n",
            "Epoch 49/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2188 - mean_absolute_error: 0.3450 - val_loss: 0.2170 - val_mean_absolute_error: 0.3358\n",
            "Epoch 50/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2243 - mean_absolute_error: 0.3455 - val_loss: 0.2168 - val_mean_absolute_error: 0.3279\n",
            "Epoch 51/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2175 - mean_absolute_error: 0.3416 - val_loss: 0.2161 - val_mean_absolute_error: 0.3356\n",
            "Epoch 52/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2266 - mean_absolute_error: 0.3510 - val_loss: 0.2151 - val_mean_absolute_error: 0.3390\n",
            "Epoch 53/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2243 - mean_absolute_error: 0.3486 - val_loss: 0.2143 - val_mean_absolute_error: 0.3434\n",
            "Epoch 54/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2112 - mean_absolute_error: 0.3392 - val_loss: 0.2167 - val_mean_absolute_error: 0.3371\n",
            "Epoch 55/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2207 - mean_absolute_error: 0.3447 - val_loss: 0.2162 - val_mean_absolute_error: 0.3487\n",
            "Epoch 56/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2201 - mean_absolute_error: 0.3457 - val_loss: 0.2139 - val_mean_absolute_error: 0.3484\n",
            "Epoch 57/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2230 - mean_absolute_error: 0.3533 - val_loss: 0.2144 - val_mean_absolute_error: 0.3287\n",
            "Epoch 58/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2206 - mean_absolute_error: 0.3434 - val_loss: 0.2141 - val_mean_absolute_error: 0.3372\n",
            "Epoch 59/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2224 - mean_absolute_error: 0.3468 - val_loss: 0.2164 - val_mean_absolute_error: 0.3506\n",
            "Epoch 60/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2219 - mean_absolute_error: 0.3467 - val_loss: 0.2165 - val_mean_absolute_error: 0.3408\n",
            "Epoch 61/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2181 - mean_absolute_error: 0.3446 - val_loss: 0.2170 - val_mean_absolute_error: 0.3424\n",
            "Epoch 62/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2258 - mean_absolute_error: 0.3527 - val_loss: 0.2161 - val_mean_absolute_error: 0.3398\n",
            "Epoch 63/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2198 - mean_absolute_error: 0.3430 - val_loss: 0.2154 - val_mean_absolute_error: 0.3416\n",
            "Epoch 64/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2144 - mean_absolute_error: 0.3439 - val_loss: 0.2197 - val_mean_absolute_error: 0.3444\n",
            "Epoch 65/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2163 - mean_absolute_error: 0.3436 - val_loss: 0.2159 - val_mean_absolute_error: 0.3410\n",
            "Epoch 66/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2205 - mean_absolute_error: 0.3415 - val_loss: 0.2170 - val_mean_absolute_error: 0.3361\n",
            "Epoch 67/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2161 - mean_absolute_error: 0.3412 - val_loss: 0.2177 - val_mean_absolute_error: 0.3447\n",
            "Epoch 68/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2242 - mean_absolute_error: 0.3490 - val_loss: 0.2180 - val_mean_absolute_error: 0.3559\n",
            "Epoch 69/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2170 - mean_absolute_error: 0.3453 - val_loss: 0.2160 - val_mean_absolute_error: 0.3320\n",
            "Epoch 70/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2165 - mean_absolute_error: 0.3383 - val_loss: 0.2165 - val_mean_absolute_error: 0.3502\n",
            "Epoch 71/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2265 - mean_absolute_error: 0.3518 - val_loss: 0.2179 - val_mean_absolute_error: 0.3297\n",
            "Epoch 71: early stopping\n",
            "Restoring model weights from the end of the best epoch: 56.\n",
            "Optimized Model Validation MAE: 0.3484\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Predictions saved to 'submission_nn_optimized_optuna.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmisnh0vI4Ad",
        "outputId": "54056a0d-c96c-4487-cb2f-1df975169a17"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.3669566 ],\n",
              "       [0.14891481],\n",
              "       [0.24420136],\n",
              "       ...,\n",
              "       [0.436643  ],\n",
              "       [1.293821  ],\n",
              "       [1.0042803 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "results_df.to_csv('submission_nn_optimized_optuna.csv', index=False)\n",
        "print(\"Predictions saved to 'submission_nn_optimized_optuna.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOQ8_z588gac",
        "outputId": "7fcefe19-f7d1-4b9f-d3c4-285cc164d30a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Predictions saved to 'submission_nn_optimized_optuna.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=503)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),  # Increased dropout for robustness\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Linear for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=150,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=64,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_10.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\")\n"
      ],
      "metadata": {
        "id": "ec5tQZna8mEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a84a597-70e0-4920-9176-bb893e3bfa7e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - loss: 1.4572 - mean_absolute_error: 0.8740 - val_loss: 0.4490 - val_mean_absolute_error: 0.4416 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.5080 - mean_absolute_error: 0.5034 - val_loss: 0.3790 - val_mean_absolute_error: 0.3719 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4156 - mean_absolute_error: 0.4422 - val_loss: 0.3386 - val_mean_absolute_error: 0.3577 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3610 - mean_absolute_error: 0.3981 - val_loss: 0.3184 - val_mean_absolute_error: 0.3366 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3449 - mean_absolute_error: 0.3884 - val_loss: 0.3131 - val_mean_absolute_error: 0.3306 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3192 - mean_absolute_error: 0.3643 - val_loss: 0.3160 - val_mean_absolute_error: 0.3306 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3241 - mean_absolute_error: 0.3724 - val_loss: 0.3100 - val_mean_absolute_error: 0.3342 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3248 - mean_absolute_error: 0.3725 - val_loss: 0.3046 - val_mean_absolute_error: 0.3217 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3096 - mean_absolute_error: 0.3591 - val_loss: 0.3010 - val_mean_absolute_error: 0.3243 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3057 - mean_absolute_error: 0.3545 - val_loss: 0.2952 - val_mean_absolute_error: 0.3364 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3001 - mean_absolute_error: 0.3553 - val_loss: 0.2932 - val_mean_absolute_error: 0.3248 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2986 - mean_absolute_error: 0.3526 - val_loss: 0.2907 - val_mean_absolute_error: 0.3324 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3022 - mean_absolute_error: 0.3577 - val_loss: 0.2896 - val_mean_absolute_error: 0.3227 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2942 - mean_absolute_error: 0.3502 - val_loss: 0.2895 - val_mean_absolute_error: 0.3216 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2958 - mean_absolute_error: 0.3554 - val_loss: 0.2877 - val_mean_absolute_error: 0.3287 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2916 - mean_absolute_error: 0.3561 - val_loss: 0.2812 - val_mean_absolute_error: 0.3223 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2836 - mean_absolute_error: 0.3486 - val_loss: 0.2823 - val_mean_absolute_error: 0.3260 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2932 - mean_absolute_error: 0.3559 - val_loss: 0.2774 - val_mean_absolute_error: 0.3249 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2861 - mean_absolute_error: 0.3545 - val_loss: 0.2791 - val_mean_absolute_error: 0.3189 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2835 - mean_absolute_error: 0.3525 - val_loss: 0.2758 - val_mean_absolute_error: 0.3188 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2725 - mean_absolute_error: 0.3412 - val_loss: 0.2719 - val_mean_absolute_error: 0.3239 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2819 - mean_absolute_error: 0.3548 - val_loss: 0.2713 - val_mean_absolute_error: 0.3251 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2739 - mean_absolute_error: 0.3456 - val_loss: 0.2764 - val_mean_absolute_error: 0.3284 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2676 - mean_absolute_error: 0.3405 - val_loss: 0.2769 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2752 - mean_absolute_error: 0.3548 - val_loss: 0.2674 - val_mean_absolute_error: 0.3250 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2650 - mean_absolute_error: 0.3442 - val_loss: 0.2600 - val_mean_absolute_error: 0.3300 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2558 - mean_absolute_error: 0.3384 - val_loss: 0.2609 - val_mean_absolute_error: 0.3241 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2570 - mean_absolute_error: 0.3419 - val_loss: 0.2605 - val_mean_absolute_error: 0.3264 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2575 - mean_absolute_error: 0.3430 - val_loss: 0.2675 - val_mean_absolute_error: 0.3369 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2542 - mean_absolute_error: 0.3419 - val_loss: 0.2572 - val_mean_absolute_error: 0.3329 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2494 - mean_absolute_error: 0.3418 - val_loss: 0.2562 - val_mean_absolute_error: 0.3375 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2482 - mean_absolute_error: 0.3397 - val_loss: 0.2604 - val_mean_absolute_error: 0.3208 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2474 - mean_absolute_error: 0.3379 - val_loss: 0.2603 - val_mean_absolute_error: 0.3362 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2507 - mean_absolute_error: 0.3397 - val_loss: 0.2531 - val_mean_absolute_error: 0.3329 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2516 - mean_absolute_error: 0.3451 - val_loss: 0.2536 - val_mean_absolute_error: 0.3399 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2426 - mean_absolute_error: 0.3412 - val_loss: 0.2494 - val_mean_absolute_error: 0.3236 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2450 - mean_absolute_error: 0.3427 - val_loss: 0.2499 - val_mean_absolute_error: 0.3250 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2376 - mean_absolute_error: 0.3315 - val_loss: 0.2451 - val_mean_absolute_error: 0.3310 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2366 - mean_absolute_error: 0.3388 - val_loss: 0.2460 - val_mean_absolute_error: 0.3336 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2466 - mean_absolute_error: 0.3477 - val_loss: 0.2450 - val_mean_absolute_error: 0.3266 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2418 - mean_absolute_error: 0.3410 - val_loss: 0.2515 - val_mean_absolute_error: 0.3267 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2438 - mean_absolute_error: 0.3439 - val_loss: 0.2467 - val_mean_absolute_error: 0.3315 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2362 - mean_absolute_error: 0.3367 - val_loss: 0.2431 - val_mean_absolute_error: 0.3293 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2282 - mean_absolute_error: 0.3304 - val_loss: 0.2411 - val_mean_absolute_error: 0.3303 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2325 - mean_absolute_error: 0.3347 - val_loss: 0.2425 - val_mean_absolute_error: 0.3195 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2251 - mean_absolute_error: 0.3274 - val_loss: 0.2409 - val_mean_absolute_error: 0.3417 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2291 - mean_absolute_error: 0.3393 - val_loss: 0.2387 - val_mean_absolute_error: 0.3337 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2299 - mean_absolute_error: 0.3413 - val_loss: 0.2390 - val_mean_absolute_error: 0.3345 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2302 - mean_absolute_error: 0.3339 - val_loss: 0.2402 - val_mean_absolute_error: 0.3384 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2315 - mean_absolute_error: 0.3405 - val_loss: 0.2429 - val_mean_absolute_error: 0.3220 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2237 - mean_absolute_error: 0.3303 - val_loss: 0.2391 - val_mean_absolute_error: 0.3246 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2226 - mean_absolute_error: 0.3265 - val_loss: 0.2377 - val_mean_absolute_error: 0.3259 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2210 - mean_absolute_error: 0.3250 - val_loss: 0.2371 - val_mean_absolute_error: 0.3277 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2185 - mean_absolute_error: 0.3259 - val_loss: 0.2368 - val_mean_absolute_error: 0.3257 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2270 - mean_absolute_error: 0.3323 - val_loss: 0.2371 - val_mean_absolute_error: 0.3255 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2256 - mean_absolute_error: 0.3299 - val_loss: 0.2369 - val_mean_absolute_error: 0.3250 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2283 - mean_absolute_error: 0.3307 - val_loss: 0.2370 - val_mean_absolute_error: 0.3224 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2082 - mean_absolute_error: 0.3146 - val_loss: 0.2367 - val_mean_absolute_error: 0.3268 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2174 - mean_absolute_error: 0.3257 - val_loss: 0.2365 - val_mean_absolute_error: 0.3269 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2209 - mean_absolute_error: 0.3270 - val_loss: 0.2367 - val_mean_absolute_error: 0.3247 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2224 - mean_absolute_error: 0.3304 - val_loss: 0.2370 - val_mean_absolute_error: 0.3233 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2181 - mean_absolute_error: 0.3237 - val_loss: 0.2369 - val_mean_absolute_error: 0.3222 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2224 - mean_absolute_error: 0.3237 - val_loss: 0.2361 - val_mean_absolute_error: 0.3250 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2166 - mean_absolute_error: 0.3233 - val_loss: 0.2367 - val_mean_absolute_error: 0.3237 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2145 - mean_absolute_error: 0.3205 - val_loss: 0.2368 - val_mean_absolute_error: 0.3260 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2094 - mean_absolute_error: 0.3174 - val_loss: 0.2367 - val_mean_absolute_error: 0.3248 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2157 - mean_absolute_error: 0.3230 - val_loss: 0.2373 - val_mean_absolute_error: 0.3235 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 68/150\n",
            "\u001b[1m148/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2166 - mean_absolute_error: 0.3223\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2166 - mean_absolute_error: 0.3222 - val_loss: 0.2373 - val_mean_absolute_error: 0.3238 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 69/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2134 - mean_absolute_error: 0.3204 - val_loss: 0.2371 - val_mean_absolute_error: 0.3255 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 70/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2159 - mean_absolute_error: 0.3239 - val_loss: 0.2373 - val_mean_absolute_error: 0.3234 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 71/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2128 - mean_absolute_error: 0.3200 - val_loss: 0.2372 - val_mean_absolute_error: 0.3248 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 72/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2158 - mean_absolute_error: 0.3227 - val_loss: 0.2370 - val_mean_absolute_error: 0.3259 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 73/150\n",
            "\u001b[1m157/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2155 - mean_absolute_error: 0.3204\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2155 - mean_absolute_error: 0.3204 - val_loss: 0.2372 - val_mean_absolute_error: 0.3252 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 74/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2128 - mean_absolute_error: 0.3217 - val_loss: 0.2374 - val_mean_absolute_error: 0.3254 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 75/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2200 - mean_absolute_error: 0.3265 - val_loss: 0.2377 - val_mean_absolute_error: 0.3233 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 76/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2194 - mean_absolute_error: 0.3257 - val_loss: 0.2379 - val_mean_absolute_error: 0.3253 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 77/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2095 - mean_absolute_error: 0.3169 - val_loss: 0.2371 - val_mean_absolute_error: 0.3269 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 78/150\n",
            "\u001b[1m151/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2131 - mean_absolute_error: 0.3235\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2130 - mean_absolute_error: 0.3234 - val_loss: 0.2383 - val_mean_absolute_error: 0.3236 - learning_rate: 5.0000e-05\n",
            "Epoch 78: early stopping\n",
            "Restoring model weights from the end of the best epoch: 63.\n",
            "Validation MAE: 0.3250\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3250\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_10.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGabvAisZeBS",
        "outputId": "7484e2f8-8aa1-4df5-bf6d-e6812c3bd9c6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 1.2050 - mean_absolute_error: 0.8102 - val_loss: 0.2682 - val_mean_absolute_error: 0.4259 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.3773 - mean_absolute_error: 0.4748 - val_loss: 0.2575 - val_mean_absolute_error: 0.4016 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3150 - mean_absolute_error: 0.4304 - val_loss: 0.2416 - val_mean_absolute_error: 0.3770 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2676 - mean_absolute_error: 0.3948 - val_loss: 0.2322 - val_mean_absolute_error: 0.3649 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2562 - mean_absolute_error: 0.3817 - val_loss: 0.2203 - val_mean_absolute_error: 0.3555 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2414 - mean_absolute_error: 0.3718 - val_loss: 0.2320 - val_mean_absolute_error: 0.3400 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2341 - mean_absolute_error: 0.3583 - val_loss: 0.2235 - val_mean_absolute_error: 0.3702 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2335 - mean_absolute_error: 0.3625 - val_loss: 0.2259 - val_mean_absolute_error: 0.3359 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2286 - mean_absolute_error: 0.3498 - val_loss: 0.2129 - val_mean_absolute_error: 0.3327 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2303 - mean_absolute_error: 0.3565 - val_loss: 0.2216 - val_mean_absolute_error: 0.3268 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2278 - mean_absolute_error: 0.3547 - val_loss: 0.2142 - val_mean_absolute_error: 0.3385 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2187 - mean_absolute_error: 0.3455 - val_loss: 0.2113 - val_mean_absolute_error: 0.3621 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2284 - mean_absolute_error: 0.3658 - val_loss: 0.2135 - val_mean_absolute_error: 0.3189 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2143 - mean_absolute_error: 0.3425 - val_loss: 0.2150 - val_mean_absolute_error: 0.3351 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2194 - mean_absolute_error: 0.3419 - val_loss: 0.2102 - val_mean_absolute_error: 0.3341 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2248 - mean_absolute_error: 0.3536 - val_loss: 0.2154 - val_mean_absolute_error: 0.3161 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2131 - mean_absolute_error: 0.3419 - val_loss: 0.2155 - val_mean_absolute_error: 0.3206 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2146 - mean_absolute_error: 0.3422 - val_loss: 0.2131 - val_mean_absolute_error: 0.3291 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2207 - mean_absolute_error: 0.3481 - val_loss: 0.2101 - val_mean_absolute_error: 0.3242 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2208 - mean_absolute_error: 0.3479 - val_loss: 0.2189 - val_mean_absolute_error: 0.3377 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2182 - mean_absolute_error: 0.3435 - val_loss: 0.2093 - val_mean_absolute_error: 0.3214 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2175 - mean_absolute_error: 0.3423 - val_loss: 0.2115 - val_mean_absolute_error: 0.3266 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2096 - mean_absolute_error: 0.3380 - val_loss: 0.2123 - val_mean_absolute_error: 0.3459 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2148 - mean_absolute_error: 0.3404 - val_loss: 0.2158 - val_mean_absolute_error: 0.3139 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2207 - mean_absolute_error: 0.3479 - val_loss: 0.2137 - val_mean_absolute_error: 0.3239 - learning_rate: 0.0010\n",
            "Epoch 26/200\n",
            "\u001b[1m302/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2080 - mean_absolute_error: 0.3304\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2082 - mean_absolute_error: 0.3307 - val_loss: 0.2124 - val_mean_absolute_error: 0.3278 - learning_rate: 0.0010\n",
            "Epoch 27/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2089 - mean_absolute_error: 0.3344 - val_loss: 0.2094 - val_mean_absolute_error: 0.3197 - learning_rate: 2.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2018 - mean_absolute_error: 0.3252 - val_loss: 0.2095 - val_mean_absolute_error: 0.3279 - learning_rate: 2.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2056 - mean_absolute_error: 0.3342 - val_loss: 0.2095 - val_mean_absolute_error: 0.3224 - learning_rate: 2.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2073 - mean_absolute_error: 0.3323 - val_loss: 0.2117 - val_mean_absolute_error: 0.3281 - learning_rate: 2.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m303/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1985 - mean_absolute_error: 0.3263\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1986 - mean_absolute_error: 0.3264 - val_loss: 0.2096 - val_mean_absolute_error: 0.3319 - learning_rate: 2.0000e-04\n",
            "Epoch 32/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2058 - mean_absolute_error: 0.3368 - val_loss: 0.2097 - val_mean_absolute_error: 0.3246 - learning_rate: 4.0000e-05\n",
            "Epoch 33/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1959 - mean_absolute_error: 0.3274 - val_loss: 0.2097 - val_mean_absolute_error: 0.3244 - learning_rate: 4.0000e-05\n",
            "Epoch 34/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1980 - mean_absolute_error: 0.3293 - val_loss: 0.2101 - val_mean_absolute_error: 0.3229 - learning_rate: 4.0000e-05\n",
            "Epoch 35/200\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1991 - mean_absolute_error: 0.3244 - val_loss: 0.2102 - val_mean_absolute_error: 0.3222 - learning_rate: 4.0000e-05\n",
            "Epoch 36/200\n",
            "\u001b[1m297/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1973 - mean_absolute_error: 0.3230\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1974 - mean_absolute_error: 0.3231 - val_loss: 0.2101 - val_mean_absolute_error: 0.3216 - learning_rate: 4.0000e-05\n",
            "Epoch 36: early stopping\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "Validation MAE: 0.3214\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3214\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey-dkwTKZx9M",
        "outputId": "2f1ffb9b-b6f7-4ed5-9d65-e0318be6a1d7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.5.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=100)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),  # Increased dropout for robustness\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Linear for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=150,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=64,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_11.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rabu4a3EaHjI",
        "outputId": "9b0e5da9-78c8-4577-9a08-0d148f46aabc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - loss: 2.4014 - mean_absolute_error: 1.1321 - val_loss: 0.4043 - val_mean_absolute_error: 0.4478 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6002 - mean_absolute_error: 0.5547 - val_loss: 0.3571 - val_mean_absolute_error: 0.3907 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4463 - mean_absolute_error: 0.4621 - val_loss: 0.3529 - val_mean_absolute_error: 0.3770 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3924 - mean_absolute_error: 0.4199 - val_loss: 0.3197 - val_mean_absolute_error: 0.3521 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3523 - mean_absolute_error: 0.3907 - val_loss: 0.3151 - val_mean_absolute_error: 0.3469 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3390 - mean_absolute_error: 0.3796 - val_loss: 0.3182 - val_mean_absolute_error: 0.3481 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3335 - mean_absolute_error: 0.3736 - val_loss: 0.3221 - val_mean_absolute_error: 0.3486 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3212 - mean_absolute_error: 0.3656 - val_loss: 0.3156 - val_mean_absolute_error: 0.3288 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3138 - mean_absolute_error: 0.3546 - val_loss: 0.3028 - val_mean_absolute_error: 0.3440 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3120 - mean_absolute_error: 0.3605 - val_loss: 0.3035 - val_mean_absolute_error: 0.3340 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3032 - mean_absolute_error: 0.3470 - val_loss: 0.3075 - val_mean_absolute_error: 0.3426 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3141 - mean_absolute_error: 0.3610 - val_loss: 0.3024 - val_mean_absolute_error: 0.3309 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3084 - mean_absolute_error: 0.3568 - val_loss: 0.3087 - val_mean_absolute_error: 0.3338 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2975 - mean_absolute_error: 0.3549 - val_loss: 0.2949 - val_mean_absolute_error: 0.3399 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2946 - mean_absolute_error: 0.3490 - val_loss: 0.2923 - val_mean_absolute_error: 0.3401 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3016 - mean_absolute_error: 0.3581 - val_loss: 0.2891 - val_mean_absolute_error: 0.3398 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2945 - mean_absolute_error: 0.3521 - val_loss: 0.3078 - val_mean_absolute_error: 0.3486 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2885 - mean_absolute_error: 0.3518 - val_loss: 0.2883 - val_mean_absolute_error: 0.3307 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2816 - mean_absolute_error: 0.3439 - val_loss: 0.2828 - val_mean_absolute_error: 0.3355 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2861 - mean_absolute_error: 0.3491 - val_loss: 0.2885 - val_mean_absolute_error: 0.3388 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2771 - mean_absolute_error: 0.3426 - val_loss: 0.2909 - val_mean_absolute_error: 0.3416 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2761 - mean_absolute_error: 0.3469 - val_loss: 0.2778 - val_mean_absolute_error: 0.3336 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2782 - mean_absolute_error: 0.3507 - val_loss: 0.2771 - val_mean_absolute_error: 0.3285 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2733 - mean_absolute_error: 0.3426 - val_loss: 0.2749 - val_mean_absolute_error: 0.3375 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2703 - mean_absolute_error: 0.3453 - val_loss: 0.2714 - val_mean_absolute_error: 0.3365 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2667 - mean_absolute_error: 0.3418 - val_loss: 0.2706 - val_mean_absolute_error: 0.3506 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2646 - mean_absolute_error: 0.3478 - val_loss: 0.2694 - val_mean_absolute_error: 0.3360 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2653 - mean_absolute_error: 0.3453 - val_loss: 0.2731 - val_mean_absolute_error: 0.3422 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2576 - mean_absolute_error: 0.3380 - val_loss: 0.2668 - val_mean_absolute_error: 0.3496 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2614 - mean_absolute_error: 0.3486 - val_loss: 0.2684 - val_mean_absolute_error: 0.3433 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2484 - mean_absolute_error: 0.3375 - val_loss: 0.2603 - val_mean_absolute_error: 0.3415 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2559 - mean_absolute_error: 0.3432 - val_loss: 0.2635 - val_mean_absolute_error: 0.3347 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2437 - mean_absolute_error: 0.3306 - val_loss: 0.2573 - val_mean_absolute_error: 0.3476 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2489 - mean_absolute_error: 0.3406 - val_loss: 0.2557 - val_mean_absolute_error: 0.3327 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2498 - mean_absolute_error: 0.3446 - val_loss: 0.2566 - val_mean_absolute_error: 0.3406 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2475 - mean_absolute_error: 0.3406 - val_loss: 0.2533 - val_mean_absolute_error: 0.3325 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2373 - mean_absolute_error: 0.3321 - val_loss: 0.2520 - val_mean_absolute_error: 0.3427 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2481 - mean_absolute_error: 0.3415 - val_loss: 0.2499 - val_mean_absolute_error: 0.3375 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2397 - mean_absolute_error: 0.3339 - val_loss: 0.2510 - val_mean_absolute_error: 0.3386 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2417 - mean_absolute_error: 0.3384 - val_loss: 0.2525 - val_mean_absolute_error: 0.3420 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2366 - mean_absolute_error: 0.3342 - val_loss: 0.2575 - val_mean_absolute_error: 0.3418 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2398 - mean_absolute_error: 0.3389 - val_loss: 0.2454 - val_mean_absolute_error: 0.3384 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2396 - mean_absolute_error: 0.3385 - val_loss: 0.2468 - val_mean_absolute_error: 0.3446 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2347 - mean_absolute_error: 0.3358 - val_loss: 0.2454 - val_mean_absolute_error: 0.3433 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2308 - mean_absolute_error: 0.3337 - val_loss: 0.2443 - val_mean_absolute_error: 0.3457 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2356 - mean_absolute_error: 0.3351 - val_loss: 0.2447 - val_mean_absolute_error: 0.3361 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2218 - mean_absolute_error: 0.3246 - val_loss: 0.2441 - val_mean_absolute_error: 0.3295 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2279 - mean_absolute_error: 0.3261 - val_loss: 0.2439 - val_mean_absolute_error: 0.3422 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2396 - mean_absolute_error: 0.3419 - val_loss: 0.2426 - val_mean_absolute_error: 0.3306 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2227 - mean_absolute_error: 0.3272 - val_loss: 0.2424 - val_mean_absolute_error: 0.3298 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2231 - mean_absolute_error: 0.3268 - val_loss: 0.2413 - val_mean_absolute_error: 0.3287 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2256 - mean_absolute_error: 0.3259 - val_loss: 0.2398 - val_mean_absolute_error: 0.3328 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2202 - mean_absolute_error: 0.3239 - val_loss: 0.2402 - val_mean_absolute_error: 0.3311 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2187 - mean_absolute_error: 0.3228 - val_loss: 0.2405 - val_mean_absolute_error: 0.3311 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2232 - mean_absolute_error: 0.3266 - val_loss: 0.2399 - val_mean_absolute_error: 0.3311 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2157 - mean_absolute_error: 0.3193 - val_loss: 0.2402 - val_mean_absolute_error: 0.3318 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/150\n",
            "\u001b[1m130/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2243 - mean_absolute_error: 0.3291\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2240 - mean_absolute_error: 0.3290 - val_loss: 0.2402 - val_mean_absolute_error: 0.3321 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2168 - mean_absolute_error: 0.3221 - val_loss: 0.2399 - val_mean_absolute_error: 0.3322 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2185 - mean_absolute_error: 0.3235 - val_loss: 0.2396 - val_mean_absolute_error: 0.3315 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2091 - mean_absolute_error: 0.3141 - val_loss: 0.2396 - val_mean_absolute_error: 0.3314 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2205 - mean_absolute_error: 0.3250 - val_loss: 0.2398 - val_mean_absolute_error: 0.3296 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2221 - mean_absolute_error: 0.3249 - val_loss: 0.2394 - val_mean_absolute_error: 0.3303 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2208 - mean_absolute_error: 0.3242 - val_loss: 0.2390 - val_mean_absolute_error: 0.3329 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2168 - mean_absolute_error: 0.3238 - val_loss: 0.2397 - val_mean_absolute_error: 0.3312 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2229 - mean_absolute_error: 0.3280 - val_loss: 0.2397 - val_mean_absolute_error: 0.3300 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2195 - mean_absolute_error: 0.3257 - val_loss: 0.2400 - val_mean_absolute_error: 0.3294 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2189 - mean_absolute_error: 0.3224 - val_loss: 0.2400 - val_mean_absolute_error: 0.3306 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 68/150\n",
            "\u001b[1m143/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2168 - mean_absolute_error: 0.3197\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2168 - mean_absolute_error: 0.3198 - val_loss: 0.2399 - val_mean_absolute_error: 0.3303 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 69/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2157 - mean_absolute_error: 0.3196 - val_loss: 0.2392 - val_mean_absolute_error: 0.3299 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 70/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2226 - mean_absolute_error: 0.3271 - val_loss: 0.2392 - val_mean_absolute_error: 0.3313 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 71/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2146 - mean_absolute_error: 0.3209 - val_loss: 0.2396 - val_mean_absolute_error: 0.3296 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 72/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2126 - mean_absolute_error: 0.3179 - val_loss: 0.2389 - val_mean_absolute_error: 0.3312 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 73/150\n",
            "\u001b[1m130/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2138 - mean_absolute_error: 0.3195\n",
            "Epoch 73: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2141 - mean_absolute_error: 0.3198 - val_loss: 0.2390 - val_mean_absolute_error: 0.3307 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 74/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2191 - mean_absolute_error: 0.3235 - val_loss: 0.2388 - val_mean_absolute_error: 0.3299 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 75/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2095 - mean_absolute_error: 0.3156 - val_loss: 0.2385 - val_mean_absolute_error: 0.3314 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 76/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2180 - mean_absolute_error: 0.3248 - val_loss: 0.2387 - val_mean_absolute_error: 0.3292 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 77/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2104 - mean_absolute_error: 0.3187 - val_loss: 0.2386 - val_mean_absolute_error: 0.3308 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 78/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2134 - mean_absolute_error: 0.3172 - val_loss: 0.2392 - val_mean_absolute_error: 0.3312 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 79/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2149 - mean_absolute_error: 0.3226 - val_loss: 0.2396 - val_mean_absolute_error: 0.3293 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 80/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2137 - mean_absolute_error: 0.3209 - val_loss: 0.2384 - val_mean_absolute_error: 0.3326 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 81/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2163 - mean_absolute_error: 0.3219 - val_loss: 0.2388 - val_mean_absolute_error: 0.3288 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 82/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2028 - mean_absolute_error: 0.3115 - val_loss: 0.2386 - val_mean_absolute_error: 0.3305 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 83/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2156 - mean_absolute_error: 0.3213 - val_loss: 0.2383 - val_mean_absolute_error: 0.3305 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 84/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2069 - mean_absolute_error: 0.3146 - val_loss: 0.2384 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 85/150\n",
            "\u001b[1m140/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2119 - mean_absolute_error: 0.3206\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2120 - mean_absolute_error: 0.3206 - val_loss: 0.2394 - val_mean_absolute_error: 0.3298 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 86/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2134 - mean_absolute_error: 0.3177 - val_loss: 0.2378 - val_mean_absolute_error: 0.3308 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 87/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2142 - mean_absolute_error: 0.3216 - val_loss: 0.2381 - val_mean_absolute_error: 0.3291 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 88/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2108 - mean_absolute_error: 0.3178 - val_loss: 0.2382 - val_mean_absolute_error: 0.3283 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 89/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2107 - mean_absolute_error: 0.3167 - val_loss: 0.2388 - val_mean_absolute_error: 0.3293 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 90/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2093 - mean_absolute_error: 0.3141 - val_loss: 0.2378 - val_mean_absolute_error: 0.3300 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 91/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2120 - mean_absolute_error: 0.3216\n",
            "Epoch 91: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2120 - mean_absolute_error: 0.3216 - val_loss: 0.2385 - val_mean_absolute_error: 0.3283 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 92/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2071 - mean_absolute_error: 0.3151 - val_loss: 0.2383 - val_mean_absolute_error: 0.3282 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 93/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2130 - mean_absolute_error: 0.3184 - val_loss: 0.2383 - val_mean_absolute_error: 0.3298 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 94/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2141 - mean_absolute_error: 0.3219 - val_loss: 0.2376 - val_mean_absolute_error: 0.3298 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 95/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2034 - mean_absolute_error: 0.3141 - val_loss: 0.2374 - val_mean_absolute_error: 0.3298 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 96/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2097 - mean_absolute_error: 0.3185 - val_loss: 0.2379 - val_mean_absolute_error: 0.3297 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 97/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2117 - mean_absolute_error: 0.3178 - val_loss: 0.2378 - val_mean_absolute_error: 0.3296 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 98/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2079 - mean_absolute_error: 0.3133 - val_loss: 0.2377 - val_mean_absolute_error: 0.3304 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 99/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2076 - mean_absolute_error: 0.3163 - val_loss: 0.2375 - val_mean_absolute_error: 0.3290 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 100/150\n",
            "\u001b[1m130/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2089 - mean_absolute_error: 0.3169\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2094 - mean_absolute_error: 0.3172 - val_loss: 0.2377 - val_mean_absolute_error: 0.3285 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 101/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2097 - mean_absolute_error: 0.3150 - val_loss: 0.2368 - val_mean_absolute_error: 0.3307 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 102/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2075 - mean_absolute_error: 0.3165 - val_loss: 0.2372 - val_mean_absolute_error: 0.3284 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 103/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2080 - mean_absolute_error: 0.3154 - val_loss: 0.2374 - val_mean_absolute_error: 0.3288 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 104/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2083 - mean_absolute_error: 0.3154 - val_loss: 0.2377 - val_mean_absolute_error: 0.3262 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 105/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2109 - mean_absolute_error: 0.3183 - val_loss: 0.2378 - val_mean_absolute_error: 0.3282 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 106/150\n",
            "\u001b[1m129/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2062 - mean_absolute_error: 0.3125\n",
            "Epoch 106: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2064 - mean_absolute_error: 0.3127 - val_loss: 0.2379 - val_mean_absolute_error: 0.3272 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 107/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2100 - mean_absolute_error: 0.3192 - val_loss: 0.2384 - val_mean_absolute_error: 0.3280 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 108/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2060 - mean_absolute_error: 0.3117 - val_loss: 0.2375 - val_mean_absolute_error: 0.3284 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 109/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2082 - mean_absolute_error: 0.3149 - val_loss: 0.2376 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 110/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2071 - mean_absolute_error: 0.3160 - val_loss: 0.2375 - val_mean_absolute_error: 0.3279 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 111/150\n",
            "\u001b[1m126/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2065 - mean_absolute_error: 0.3144\n",
            "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2071 - mean_absolute_error: 0.3149 - val_loss: 0.2374 - val_mean_absolute_error: 0.3275 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 112/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2057 - mean_absolute_error: 0.3127 - val_loss: 0.2380 - val_mean_absolute_error: 0.3296 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 113/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2107 - mean_absolute_error: 0.3196 - val_loss: 0.2379 - val_mean_absolute_error: 0.3277 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 114/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2104 - mean_absolute_error: 0.3187 - val_loss: 0.2383 - val_mean_absolute_error: 0.3260 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 115/150\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2050 - mean_absolute_error: 0.3109 - val_loss: 0.2376 - val_mean_absolute_error: 0.3291 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 116/150\n",
            "\u001b[1m138/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2063 - mean_absolute_error: 0.3164\n",
            "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.2064 - mean_absolute_error: 0.3164 - val_loss: 0.2373 - val_mean_absolute_error: 0.3287 - learning_rate: 5.0000e-05\n",
            "Epoch 116: early stopping\n",
            "Restoring model weights from the end of the best epoch: 101.\n",
            "Validation MAE: 0.3307\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3307\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_3.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),  # Increased dropout for regularization\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model with class balancing\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=500,  # Allow longer training with early stopping\n",
        "    batch_size=32,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_binned_optimized_12.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzK6Gql3aR5Q",
        "outputId": "7619a869-318d-40bc-8663-97ca9b05e67f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 0.6946 - mean_absolute_error: 0.6339 - val_loss: 0.2624 - val_mean_absolute_error: 0.3681 - learning_rate: 0.0010\n",
            "Epoch 2/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3122 - mean_absolute_error: 0.4339 - val_loss: 0.2590 - val_mean_absolute_error: 0.3641 - learning_rate: 0.0010\n",
            "Epoch 3/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2774 - mean_absolute_error: 0.4061 - val_loss: 0.2319 - val_mean_absolute_error: 0.3526 - learning_rate: 0.0010\n",
            "Epoch 4/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2495 - mean_absolute_error: 0.3794 - val_loss: 0.2312 - val_mean_absolute_error: 0.3445 - learning_rate: 0.0010\n",
            "Epoch 5/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2327 - mean_absolute_error: 0.3637 - val_loss: 0.2236 - val_mean_absolute_error: 0.3406 - learning_rate: 0.0010\n",
            "Epoch 6/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2269 - mean_absolute_error: 0.3591 - val_loss: 0.2249 - val_mean_absolute_error: 0.3427 - learning_rate: 0.0010\n",
            "Epoch 7/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2220 - mean_absolute_error: 0.3525 - val_loss: 0.2258 - val_mean_absolute_error: 0.3480 - learning_rate: 0.0010\n",
            "Epoch 8/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2277 - mean_absolute_error: 0.3611 - val_loss: 0.2212 - val_mean_absolute_error: 0.3267 - learning_rate: 0.0010\n",
            "Epoch 9/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2227 - mean_absolute_error: 0.3555 - val_loss: 0.2344 - val_mean_absolute_error: 0.3348 - learning_rate: 0.0010\n",
            "Epoch 10/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2277 - mean_absolute_error: 0.3534 - val_loss: 0.2259 - val_mean_absolute_error: 0.3335 - learning_rate: 0.0010\n",
            "Epoch 11/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2238 - mean_absolute_error: 0.3552 - val_loss: 0.2216 - val_mean_absolute_error: 0.3313 - learning_rate: 0.0010\n",
            "Epoch 12/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2127 - mean_absolute_error: 0.3405 - val_loss: 0.2211 - val_mean_absolute_error: 0.3384 - learning_rate: 0.0010\n",
            "Epoch 13/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2163 - mean_absolute_error: 0.3463 - val_loss: 0.2212 - val_mean_absolute_error: 0.3359 - learning_rate: 0.0010\n",
            "Epoch 14/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2170 - mean_absolute_error: 0.3452 - val_loss: 0.2225 - val_mean_absolute_error: 0.3533 - learning_rate: 0.0010\n",
            "Epoch 15/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2204 - mean_absolute_error: 0.3514 - val_loss: 0.2199 - val_mean_absolute_error: 0.3282 - learning_rate: 0.0010\n",
            "Epoch 16/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2126 - mean_absolute_error: 0.3446 - val_loss: 0.2219 - val_mean_absolute_error: 0.3292 - learning_rate: 0.0010\n",
            "Epoch 17/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2140 - mean_absolute_error: 0.3464 - val_loss: 0.2202 - val_mean_absolute_error: 0.3340 - learning_rate: 0.0010\n",
            "Epoch 18/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2064 - mean_absolute_error: 0.3382 - val_loss: 0.2206 - val_mean_absolute_error: 0.3324 - learning_rate: 0.0010\n",
            "Epoch 19/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2139 - mean_absolute_error: 0.3444 - val_loss: 0.2206 - val_mean_absolute_error: 0.3410 - learning_rate: 0.0010\n",
            "Epoch 20/500\n",
            "\u001b[1m304/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2167 - mean_absolute_error: 0.3470\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2166 - mean_absolute_error: 0.3468 - val_loss: 0.2260 - val_mean_absolute_error: 0.3332 - learning_rate: 0.0010\n",
            "Epoch 21/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2023 - mean_absolute_error: 0.3312 - val_loss: 0.2175 - val_mean_absolute_error: 0.3295 - learning_rate: 1.0000e-04\n",
            "Epoch 22/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2053 - mean_absolute_error: 0.3333 - val_loss: 0.2174 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "Epoch 23/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2067 - mean_absolute_error: 0.3360 - val_loss: 0.2182 - val_mean_absolute_error: 0.3313 - learning_rate: 1.0000e-04\n",
            "Epoch 24/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2088 - mean_absolute_error: 0.3401 - val_loss: 0.2168 - val_mean_absolute_error: 0.3264 - learning_rate: 1.0000e-04\n",
            "Epoch 25/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2053 - mean_absolute_error: 0.3345 - val_loss: 0.2175 - val_mean_absolute_error: 0.3263 - learning_rate: 1.0000e-04\n",
            "Epoch 26/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2080 - mean_absolute_error: 0.3374 - val_loss: 0.2174 - val_mean_absolute_error: 0.3240 - learning_rate: 1.0000e-04\n",
            "Epoch 27/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2005 - mean_absolute_error: 0.3287 - val_loss: 0.2176 - val_mean_absolute_error: 0.3268 - learning_rate: 1.0000e-04\n",
            "Epoch 28/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2002 - mean_absolute_error: 0.3302 - val_loss: 0.2166 - val_mean_absolute_error: 0.3259 - learning_rate: 1.0000e-04\n",
            "Epoch 29/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2035 - mean_absolute_error: 0.3323 - val_loss: 0.2172 - val_mean_absolute_error: 0.3242 - learning_rate: 1.0000e-04\n",
            "Epoch 30/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2039 - mean_absolute_error: 0.3308 - val_loss: 0.2179 - val_mean_absolute_error: 0.3248 - learning_rate: 1.0000e-04\n",
            "Epoch 31/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2065 - mean_absolute_error: 0.3345 - val_loss: 0.2169 - val_mean_absolute_error: 0.3253 - learning_rate: 1.0000e-04\n",
            "Epoch 32/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2050 - mean_absolute_error: 0.3337 - val_loss: 0.2176 - val_mean_absolute_error: 0.3257 - learning_rate: 1.0000e-04\n",
            "Epoch 33/500\n",
            "\u001b[1m300/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2084 - mean_absolute_error: 0.3334\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2082 - mean_absolute_error: 0.3334 - val_loss: 0.2174 - val_mean_absolute_error: 0.3272 - learning_rate: 1.0000e-04\n",
            "Epoch 34/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1966 - mean_absolute_error: 0.3284 - val_loss: 0.2175 - val_mean_absolute_error: 0.3259 - learning_rate: 1.0000e-05\n",
            "Epoch 35/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1983 - mean_absolute_error: 0.3283 - val_loss: 0.2173 - val_mean_absolute_error: 0.3251 - learning_rate: 1.0000e-05\n",
            "Epoch 36/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1985 - mean_absolute_error: 0.3269 - val_loss: 0.2174 - val_mean_absolute_error: 0.3253 - learning_rate: 1.0000e-05\n",
            "Epoch 37/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2068 - mean_absolute_error: 0.3338 - val_loss: 0.2174 - val_mean_absolute_error: 0.3244 - learning_rate: 1.0000e-05\n",
            "Epoch 38/500\n",
            "\u001b[1m305/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2013 - mean_absolute_error: 0.3298\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2013 - mean_absolute_error: 0.3298 - val_loss: 0.2173 - val_mean_absolute_error: 0.3244 - learning_rate: 1.0000e-05\n",
            "Epoch 39/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1940 - mean_absolute_error: 0.3255 - val_loss: 0.2173 - val_mean_absolute_error: 0.3246 - learning_rate: 1.0000e-06\n",
            "Epoch 40/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2085 - mean_absolute_error: 0.3348 - val_loss: 0.2174 - val_mean_absolute_error: 0.3237 - learning_rate: 1.0000e-06\n",
            "Epoch 41/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2101 - mean_absolute_error: 0.3363 - val_loss: 0.2173 - val_mean_absolute_error: 0.3241 - learning_rate: 1.0000e-06\n",
            "Epoch 42/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2036 - mean_absolute_error: 0.3307 - val_loss: 0.2173 - val_mean_absolute_error: 0.3243 - learning_rate: 1.0000e-06\n",
            "Epoch 43/500\n",
            "\u001b[1m299/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1996 - mean_absolute_error: 0.3286\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1996 - mean_absolute_error: 0.3286 - val_loss: 0.2173 - val_mean_absolute_error: 0.3245 - learning_rate: 1.0000e-06\n",
            "Epoch 44/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2014 - mean_absolute_error: 0.3290 - val_loss: 0.2173 - val_mean_absolute_error: 0.3241 - learning_rate: 1.0000e-06\n",
            "Epoch 45/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1957 - mean_absolute_error: 0.3273 - val_loss: 0.2173 - val_mean_absolute_error: 0.3243 - learning_rate: 1.0000e-06\n",
            "Epoch 46/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2091 - mean_absolute_error: 0.3365 - val_loss: 0.2172 - val_mean_absolute_error: 0.3250 - learning_rate: 1.0000e-06\n",
            "Epoch 47/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2010 - mean_absolute_error: 0.3297 - val_loss: 0.2174 - val_mean_absolute_error: 0.3243 - learning_rate: 1.0000e-06\n",
            "Epoch 48/500\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2033 - mean_absolute_error: 0.3305 - val_loss: 0.2173 - val_mean_absolute_error: 0.3245 - learning_rate: 1.0000e-06\n",
            "Epoch 48: early stopping\n",
            "Restoring model weights from the end of the best epoch: 28.\n",
            "Validation MAE: 0.3259\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3259\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_binned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=69)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=300,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=32,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_20.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_20.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYSwbKlIamI-",
        "outputId": "d9486ccf-bc35-49fe-c42d-8daf596554f4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - loss: 1.4983 - mean_absolute_error: 0.8646 - val_loss: 0.4299 - val_mean_absolute_error: 0.3958 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5231 - mean_absolute_error: 0.4781 - val_loss: 0.3872 - val_mean_absolute_error: 0.3566 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4280 - mean_absolute_error: 0.4107 - val_loss: 0.3880 - val_mean_absolute_error: 0.3667 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3965 - mean_absolute_error: 0.3862 - val_loss: 0.3667 - val_mean_absolute_error: 0.3305 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3762 - mean_absolute_error: 0.3703 - val_loss: 0.3742 - val_mean_absolute_error: 0.3449 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3758 - mean_absolute_error: 0.3724 - val_loss: 0.3505 - val_mean_absolute_error: 0.3209 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3672 - mean_absolute_error: 0.3701 - val_loss: 0.3444 - val_mean_absolute_error: 0.3376 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3567 - mean_absolute_error: 0.3636 - val_loss: 0.3372 - val_mean_absolute_error: 0.3295 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3563 - mean_absolute_error: 0.3688 - val_loss: 0.3310 - val_mean_absolute_error: 0.3575 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3436 - mean_absolute_error: 0.3677 - val_loss: 0.3227 - val_mean_absolute_error: 0.3420 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3295 - mean_absolute_error: 0.3546 - val_loss: 0.3176 - val_mean_absolute_error: 0.3514 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3216 - mean_absolute_error: 0.3609 - val_loss: 0.3106 - val_mean_absolute_error: 0.3375 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3232 - mean_absolute_error: 0.3626 - val_loss: 0.3109 - val_mean_absolute_error: 0.3481 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3116 - mean_absolute_error: 0.3586 - val_loss: 0.2975 - val_mean_absolute_error: 0.3265 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3040 - mean_absolute_error: 0.3580 - val_loss: 0.2911 - val_mean_absolute_error: 0.3308 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2950 - mean_absolute_error: 0.3535 - val_loss: 0.2850 - val_mean_absolute_error: 0.3418 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2847 - mean_absolute_error: 0.3470 - val_loss: 0.2848 - val_mean_absolute_error: 0.3344 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2853 - mean_absolute_error: 0.3519 - val_loss: 0.2868 - val_mean_absolute_error: 0.3402 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2830 - mean_absolute_error: 0.3528 - val_loss: 0.2811 - val_mean_absolute_error: 0.3492 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2798 - mean_absolute_error: 0.3551 - val_loss: 0.2712 - val_mean_absolute_error: 0.3428 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2755 - mean_absolute_error: 0.3587 - val_loss: 0.2662 - val_mean_absolute_error: 0.3322 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2704 - mean_absolute_error: 0.3495 - val_loss: 0.2639 - val_mean_absolute_error: 0.3323 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2693 - mean_absolute_error: 0.3540 - val_loss: 0.2599 - val_mean_absolute_error: 0.3428 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2685 - mean_absolute_error: 0.3576 - val_loss: 0.2570 - val_mean_absolute_error: 0.3462 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2547 - mean_absolute_error: 0.3446 - val_loss: 0.2546 - val_mean_absolute_error: 0.3292 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2553 - mean_absolute_error: 0.3496 - val_loss: 0.2555 - val_mean_absolute_error: 0.3576 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2535 - mean_absolute_error: 0.3555 - val_loss: 0.2569 - val_mean_absolute_error: 0.3279 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2554 - mean_absolute_error: 0.3451 - val_loss: 0.2529 - val_mean_absolute_error: 0.3485 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2441 - mean_absolute_error: 0.3413 - val_loss: 0.2515 - val_mean_absolute_error: 0.3320 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2415 - mean_absolute_error: 0.3393 - val_loss: 0.2478 - val_mean_absolute_error: 0.3378 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2457 - mean_absolute_error: 0.3463 - val_loss: 0.2469 - val_mean_absolute_error: 0.3347 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2465 - mean_absolute_error: 0.3467 - val_loss: 0.2479 - val_mean_absolute_error: 0.3416 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2403 - mean_absolute_error: 0.3416 - val_loss: 0.2467 - val_mean_absolute_error: 0.3402 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2438 - mean_absolute_error: 0.3434 - val_loss: 0.2467 - val_mean_absolute_error: 0.3452 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2439 - mean_absolute_error: 0.3429 - val_loss: 0.2493 - val_mean_absolute_error: 0.3338 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2471 - mean_absolute_error: 0.3483 - val_loss: 0.2454 - val_mean_absolute_error: 0.3354 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2400 - mean_absolute_error: 0.3466 - val_loss: 0.2448 - val_mean_absolute_error: 0.3467 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2427 - mean_absolute_error: 0.3478 - val_loss: 0.2451 - val_mean_absolute_error: 0.3520 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2361 - mean_absolute_error: 0.3384 - val_loss: 0.2426 - val_mean_absolute_error: 0.3349 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2361 - mean_absolute_error: 0.3401 - val_loss: 0.2442 - val_mean_absolute_error: 0.3317 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2347 - mean_absolute_error: 0.3411 - val_loss: 0.2416 - val_mean_absolute_error: 0.3296 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2385 - mean_absolute_error: 0.3436 - val_loss: 0.2424 - val_mean_absolute_error: 0.3377 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2333 - mean_absolute_error: 0.3370 - val_loss: 0.2397 - val_mean_absolute_error: 0.3462 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2294 - mean_absolute_error: 0.3363 - val_loss: 0.2423 - val_mean_absolute_error: 0.3478 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2367 - mean_absolute_error: 0.3433 - val_loss: 0.2417 - val_mean_absolute_error: 0.3397 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2417 - mean_absolute_error: 0.3456 - val_loss: 0.2433 - val_mean_absolute_error: 0.3284 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2368 - mean_absolute_error: 0.3407 - val_loss: 0.2416 - val_mean_absolute_error: 0.3514 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2418 - mean_absolute_error: 0.3460 - val_loss: 0.2394 - val_mean_absolute_error: 0.3269 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2375 - mean_absolute_error: 0.3416 - val_loss: 0.2435 - val_mean_absolute_error: 0.3220 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2227 - mean_absolute_error: 0.3283 - val_loss: 0.2413 - val_mean_absolute_error: 0.3370 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2320 - mean_absolute_error: 0.3345 - val_loss: 0.2404 - val_mean_absolute_error: 0.3354 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2327 - mean_absolute_error: 0.3403 - val_loss: 0.2388 - val_mean_absolute_error: 0.3372 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2294 - mean_absolute_error: 0.3377 - val_loss: 0.2382 - val_mean_absolute_error: 0.3358 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2214 - mean_absolute_error: 0.3282 - val_loss: 0.2381 - val_mean_absolute_error: 0.3361 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2265 - mean_absolute_error: 0.3315 - val_loss: 0.2374 - val_mean_absolute_error: 0.3341 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2201 - mean_absolute_error: 0.3267 - val_loss: 0.2365 - val_mean_absolute_error: 0.3372 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2228 - mean_absolute_error: 0.3295 - val_loss: 0.2370 - val_mean_absolute_error: 0.3320 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2214 - mean_absolute_error: 0.3255 - val_loss: 0.2366 - val_mean_absolute_error: 0.3313 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2207 - mean_absolute_error: 0.3253 - val_loss: 0.2365 - val_mean_absolute_error: 0.3328 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2219 - mean_absolute_error: 0.3289 - val_loss: 0.2363 - val_mean_absolute_error: 0.3317 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2235 - mean_absolute_error: 0.3278 - val_loss: 0.2362 - val_mean_absolute_error: 0.3284 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2221 - mean_absolute_error: 0.3271 - val_loss: 0.2361 - val_mean_absolute_error: 0.3279 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2108 - mean_absolute_error: 0.3179 - val_loss: 0.2360 - val_mean_absolute_error: 0.3327 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2214 - mean_absolute_error: 0.3274 - val_loss: 0.2357 - val_mean_absolute_error: 0.3355 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2246 - mean_absolute_error: 0.3337 - val_loss: 0.2355 - val_mean_absolute_error: 0.3310 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2138 - mean_absolute_error: 0.3218 - val_loss: 0.2356 - val_mean_absolute_error: 0.3324 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2198 - mean_absolute_error: 0.3273 - val_loss: 0.2361 - val_mean_absolute_error: 0.3311 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 68/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2122 - mean_absolute_error: 0.3187 - val_loss: 0.2361 - val_mean_absolute_error: 0.3314 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 69/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2134 - mean_absolute_error: 0.3212 - val_loss: 0.2352 - val_mean_absolute_error: 0.3313 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 70/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2147 - mean_absolute_error: 0.3204 - val_loss: 0.2352 - val_mean_absolute_error: 0.3318 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 71/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2173 - mean_absolute_error: 0.3237 - val_loss: 0.2347 - val_mean_absolute_error: 0.3291 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 72/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2160 - mean_absolute_error: 0.3242 - val_loss: 0.2342 - val_mean_absolute_error: 0.3307 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 73/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2091 - mean_absolute_error: 0.3173 - val_loss: 0.2350 - val_mean_absolute_error: 0.3319 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 74/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2165 - mean_absolute_error: 0.3246 - val_loss: 0.2342 - val_mean_absolute_error: 0.3304 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 75/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2172 - mean_absolute_error: 0.3269 - val_loss: 0.2348 - val_mean_absolute_error: 0.3259 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 76/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2133 - mean_absolute_error: 0.3195 - val_loss: 0.2345 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 77/300\n",
            "\u001b[1m309/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2142 - mean_absolute_error: 0.3232\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2143 - mean_absolute_error: 0.3232 - val_loss: 0.2344 - val_mean_absolute_error: 0.3307 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 78/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2178 - mean_absolute_error: 0.3267 - val_loss: 0.2345 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 79/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2138 - mean_absolute_error: 0.3206 - val_loss: 0.2345 - val_mean_absolute_error: 0.3323 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 80/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2107 - mean_absolute_error: 0.3216 - val_loss: 0.2344 - val_mean_absolute_error: 0.3293 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 81/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2183 - mean_absolute_error: 0.3271 - val_loss: 0.2338 - val_mean_absolute_error: 0.3300 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 82/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2126 - mean_absolute_error: 0.3189 - val_loss: 0.2337 - val_mean_absolute_error: 0.3325 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 83/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2105 - mean_absolute_error: 0.3209 - val_loss: 0.2337 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 84/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2104 - mean_absolute_error: 0.3215 - val_loss: 0.2332 - val_mean_absolute_error: 0.3310 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 85/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2133 - mean_absolute_error: 0.3234 - val_loss: 0.2323 - val_mean_absolute_error: 0.3292 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 86/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2142 - mean_absolute_error: 0.3256 - val_loss: 0.2333 - val_mean_absolute_error: 0.3290 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 87/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2202 - mean_absolute_error: 0.3275 - val_loss: 0.2327 - val_mean_absolute_error: 0.3308 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 88/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2131 - mean_absolute_error: 0.3240 - val_loss: 0.2330 - val_mean_absolute_error: 0.3292 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 89/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2097 - mean_absolute_error: 0.3191 - val_loss: 0.2323 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 90/300\n",
            "\u001b[1m316/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2128 - mean_absolute_error: 0.3266\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2128 - mean_absolute_error: 0.3266 - val_loss: 0.2327 - val_mean_absolute_error: 0.3279 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 91/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2073 - mean_absolute_error: 0.3169 - val_loss: 0.2329 - val_mean_absolute_error: 0.3256 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 92/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2075 - mean_absolute_error: 0.3148 - val_loss: 0.2323 - val_mean_absolute_error: 0.3321 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 93/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2083 - mean_absolute_error: 0.3210 - val_loss: 0.2325 - val_mean_absolute_error: 0.3325 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 94/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2104 - mean_absolute_error: 0.3203 - val_loss: 0.2324 - val_mean_absolute_error: 0.3303 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 95/300\n",
            "\u001b[1m304/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2115 - mean_absolute_error: 0.3224\n",
            "Epoch 95: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2114 - mean_absolute_error: 0.3223 - val_loss: 0.2327 - val_mean_absolute_error: 0.3256 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 96/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2125 - mean_absolute_error: 0.3212 - val_loss: 0.2324 - val_mean_absolute_error: 0.3276 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 97/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2091 - mean_absolute_error: 0.3201 - val_loss: 0.2325 - val_mean_absolute_error: 0.3218 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 98/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2072 - mean_absolute_error: 0.3113 - val_loss: 0.2326 - val_mean_absolute_error: 0.3275 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 99/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2015 - mean_absolute_error: 0.3102 - val_loss: 0.2333 - val_mean_absolute_error: 0.3324 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 100/300\n",
            "\u001b[1m312/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2008 - mean_absolute_error: 0.3135\n",
            "Epoch 100: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2010 - mean_absolute_error: 0.3136 - val_loss: 0.2324 - val_mean_absolute_error: 0.3311 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 101/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2086 - mean_absolute_error: 0.3189 - val_loss: 0.2317 - val_mean_absolute_error: 0.3298 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 102/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2110 - mean_absolute_error: 0.3193 - val_loss: 0.2323 - val_mean_absolute_error: 0.3322 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 103/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2039 - mean_absolute_error: 0.3162 - val_loss: 0.2323 - val_mean_absolute_error: 0.3331 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 104/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2098 - mean_absolute_error: 0.3207 - val_loss: 0.2318 - val_mean_absolute_error: 0.3285 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 105/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2019 - mean_absolute_error: 0.3115 - val_loss: 0.2325 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 106/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2051 - mean_absolute_error: 0.3170 - val_loss: 0.2316 - val_mean_absolute_error: 0.3303 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 107/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2040 - mean_absolute_error: 0.3119 - val_loss: 0.2321 - val_mean_absolute_error: 0.3283 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 108/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2062 - mean_absolute_error: 0.3131 - val_loss: 0.2318 - val_mean_absolute_error: 0.3309 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 109/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2076 - mean_absolute_error: 0.3190 - val_loss: 0.2323 - val_mean_absolute_error: 0.3266 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 110/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2035 - mean_absolute_error: 0.3090 - val_loss: 0.2318 - val_mean_absolute_error: 0.3295 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 111/300\n",
            "\u001b[1m294/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2082 - mean_absolute_error: 0.3194\n",
            "Epoch 111: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2080 - mean_absolute_error: 0.3192 - val_loss: 0.2321 - val_mean_absolute_error: 0.3266 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 112/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.2093 - mean_absolute_error: 0.3179 - val_loss: 0.2326 - val_mean_absolute_error: 0.3301 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 113/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2056 - mean_absolute_error: 0.3161 - val_loss: 0.2323 - val_mean_absolute_error: 0.3278 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 114/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2051 - mean_absolute_error: 0.3165 - val_loss: 0.2329 - val_mean_absolute_error: 0.3269 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 115/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2056 - mean_absolute_error: 0.3123 - val_loss: 0.2323 - val_mean_absolute_error: 0.3297 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 116/300\n",
            "\u001b[1m302/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1978 - mean_absolute_error: 0.3090\n",
            "Epoch 116: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1981 - mean_absolute_error: 0.3092 - val_loss: 0.2328 - val_mean_absolute_error: 0.3289 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 117/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2079 - mean_absolute_error: 0.3202 - val_loss: 0.2326 - val_mean_absolute_error: 0.3274 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 118/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1992 - mean_absolute_error: 0.3091 - val_loss: 0.2326 - val_mean_absolute_error: 0.3262 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 119/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2094 - mean_absolute_error: 0.3187 - val_loss: 0.2314 - val_mean_absolute_error: 0.3272 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 120/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2038 - mean_absolute_error: 0.3133 - val_loss: 0.2313 - val_mean_absolute_error: 0.3274 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 121/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2011 - mean_absolute_error: 0.3114 - val_loss: 0.2314 - val_mean_absolute_error: 0.3291 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 122/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2001 - mean_absolute_error: 0.3134 - val_loss: 0.2312 - val_mean_absolute_error: 0.3307 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 123/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2061 - mean_absolute_error: 0.3202 - val_loss: 0.2312 - val_mean_absolute_error: 0.3268 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 124/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1958 - mean_absolute_error: 0.3078 - val_loss: 0.2320 - val_mean_absolute_error: 0.3288 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 125/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2042 - mean_absolute_error: 0.3159 - val_loss: 0.2318 - val_mean_absolute_error: 0.3317 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 126/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1990 - mean_absolute_error: 0.3134 - val_loss: 0.2317 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 127/300\n",
            "\u001b[1m302/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2003 - mean_absolute_error: 0.3125\n",
            "Epoch 127: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2005 - mean_absolute_error: 0.3127 - val_loss: 0.2318 - val_mean_absolute_error: 0.3316 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 128/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1953 - mean_absolute_error: 0.3064 - val_loss: 0.2317 - val_mean_absolute_error: 0.3306 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 129/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1999 - mean_absolute_error: 0.3122 - val_loss: 0.2323 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 130/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2052 - mean_absolute_error: 0.3166 - val_loss: 0.2310 - val_mean_absolute_error: 0.3290 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 131/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2031 - mean_absolute_error: 0.3106 - val_loss: 0.2313 - val_mean_absolute_error: 0.3296 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 132/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2048 - mean_absolute_error: 0.3159 - val_loss: 0.2316 - val_mean_absolute_error: 0.3265 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 133/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1966 - mean_absolute_error: 0.3074 - val_loss: 0.2322 - val_mean_absolute_error: 0.3292 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 134/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1997 - mean_absolute_error: 0.3106 - val_loss: 0.2320 - val_mean_absolute_error: 0.3269 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 135/300\n",
            "\u001b[1m298/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2068 - mean_absolute_error: 0.3172\n",
            "Epoch 135: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2066 - mean_absolute_error: 0.3169 - val_loss: 0.2313 - val_mean_absolute_error: 0.3250 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 136/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2004 - mean_absolute_error: 0.3118 - val_loss: 0.2311 - val_mean_absolute_error: 0.3275 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 137/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1996 - mean_absolute_error: 0.3118 - val_loss: 0.2309 - val_mean_absolute_error: 0.3303 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 138/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2029 - mean_absolute_error: 0.3142 - val_loss: 0.2311 - val_mean_absolute_error: 0.3246 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 139/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2037 - mean_absolute_error: 0.3107 - val_loss: 0.2310 - val_mean_absolute_error: 0.3269 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 140/300\n",
            "\u001b[1m309/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1892 - mean_absolute_error: 0.3014\n",
            "Epoch 140: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1895 - mean_absolute_error: 0.3017 - val_loss: 0.2309 - val_mean_absolute_error: 0.3304 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 141/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1993 - mean_absolute_error: 0.3138 - val_loss: 0.2308 - val_mean_absolute_error: 0.3274 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 142/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1965 - mean_absolute_error: 0.3052 - val_loss: 0.2321 - val_mean_absolute_error: 0.3289 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 143/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1997 - mean_absolute_error: 0.3113 - val_loss: 0.2326 - val_mean_absolute_error: 0.3302 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 144/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2035 - mean_absolute_error: 0.3156 - val_loss: 0.2317 - val_mean_absolute_error: 0.3295 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 145/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2033 - mean_absolute_error: 0.3128 - val_loss: 0.2305 - val_mean_absolute_error: 0.3271 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 146/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2013 - mean_absolute_error: 0.3125 - val_loss: 0.2305 - val_mean_absolute_error: 0.3316 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 147/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1979 - mean_absolute_error: 0.3099 - val_loss: 0.2314 - val_mean_absolute_error: 0.3286 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 148/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1969 - mean_absolute_error: 0.3082 - val_loss: 0.2311 - val_mean_absolute_error: 0.3271 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 149/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2001 - mean_absolute_error: 0.3089 - val_loss: 0.2323 - val_mean_absolute_error: 0.3303 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 150/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2020 - mean_absolute_error: 0.3118\n",
            "Epoch 150: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2019 - mean_absolute_error: 0.3118 - val_loss: 0.2325 - val_mean_absolute_error: 0.3298 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 151/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2023 - mean_absolute_error: 0.3132 - val_loss: 0.2316 - val_mean_absolute_error: 0.3275 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 152/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1944 - mean_absolute_error: 0.3077 - val_loss: 0.2319 - val_mean_absolute_error: 0.3277 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 153/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1989 - mean_absolute_error: 0.3099 - val_loss: 0.2319 - val_mean_absolute_error: 0.3287 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 154/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2001 - mean_absolute_error: 0.3117 - val_loss: 0.2329 - val_mean_absolute_error: 0.3265 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 155/300\n",
            "\u001b[1m314/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2022 - mean_absolute_error: 0.3115\n",
            "Epoch 155: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2021 - mean_absolute_error: 0.3115 - val_loss: 0.2314 - val_mean_absolute_error: 0.3243 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 156/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2000 - mean_absolute_error: 0.3081 - val_loss: 0.2316 - val_mean_absolute_error: 0.3265 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 157/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2029 - mean_absolute_error: 0.3146 - val_loss: 0.2313 - val_mean_absolute_error: 0.3266 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 158/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1984 - mean_absolute_error: 0.3096 - val_loss: 0.2320 - val_mean_absolute_error: 0.3266 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 159/300\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1948 - mean_absolute_error: 0.3086 - val_loss: 0.2319 - val_mean_absolute_error: 0.3295 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 160/300\n",
            "\u001b[1m309/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2019 - mean_absolute_error: 0.3152\n",
            "Epoch 160: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m317/317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2018 - mean_absolute_error: 0.3150 - val_loss: 0.2325 - val_mean_absolute_error: 0.3277 - learning_rate: 5.0000e-05\n",
            "Epoch 160: early stopping\n",
            "Restoring model weights from the end of the best epoch: 145.\n",
            "Validation MAE: 0.3271\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3271\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_20.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define features and target\n",
        "X = train_df_encoded.drop(columns=['category'])  # Features\n",
        "y = train_df_encoded['category']  # Target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=303)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Advanced Neural Network Function with Regularization and Weight Initialization\n",
        "def build_advanced_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(512, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(256, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),  # Increased dropout for robustness\n",
        "        Dense(128, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(32, activation='relu', kernel_initializer=HeNormal(), kernel_regularizer=l2(1e-4)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(1, activation='linear', kernel_initializer='glorot_uniform')  # Linear for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch > 50:\n",
        "        return initial_lr * 0.1\n",
        "    elif epoch > 100:\n",
        "        return initial_lr * 0.01\n",
        "    return initial_lr\n",
        "\n",
        "# Compile the model\n",
        "model = build_advanced_model(input_dim=X_train.shape[1])\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient clipping added\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "# Callbacks for optimization\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=150,  # Slightly reduced epochs with LR scheduling\n",
        "    batch_size=64,  # Larger batch size for better computation\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_mae_score = mean_absolute_error(y_val, y_val_pred)\n",
        "print(f\"Mean Absolute Error on Validation Data: {val_mae_score:.4f}\")\n",
        "\n",
        "# Apply binning to the predictions\n",
        "y_val_binned = pd.cut(y_val_pred.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Load the test data\n",
        "temp_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Scale the test data\n",
        "test_df_encoded_scaled = scaler.transform(test_df_encoded)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = model.predict(test_df_encoded_scaled)\n",
        "\n",
        "# Apply binning to the test predictions\n",
        "test_binned = pd.cut(test_predictions.flatten(), bins=[-float('inf'), 0.6, 1.3, float('inf')], labels=[0, 1, 2])\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "results_df = pd.DataFrame({\n",
        "    'trip_ID': temp_test_df['trip_ID'],\n",
        "    'category': test_binned\n",
        "})\n",
        "\n",
        "results_df.to_csv('submission_nn_optimized_45.csv', index=False)\n",
        "print(\"Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_45.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BGSb8RTapBP",
        "outputId": "e6897838-0059-41af-c381-0f0140438e40"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 53ms/step - loss: 3.3189 - mean_absolute_error: 1.3616 - val_loss: 0.5126 - val_mean_absolute_error: 0.4023 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 2/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 1.0922 - mean_absolute_error: 0.7309 - val_loss: 0.4585 - val_mean_absolute_error: 0.3528 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 3/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6618 - mean_absolute_error: 0.5240 - val_loss: 0.4278 - val_mean_absolute_error: 0.3273 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 4/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5238 - mean_absolute_error: 0.4381 - val_loss: 0.4134 - val_mean_absolute_error: 0.3278 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 5/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4811 - mean_absolute_error: 0.4113 - val_loss: 0.4102 - val_mean_absolute_error: 0.3463 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 6/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4571 - mean_absolute_error: 0.3931 - val_loss: 0.4004 - val_mean_absolute_error: 0.3417 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 7/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4332 - mean_absolute_error: 0.3796 - val_loss: 0.3975 - val_mean_absolute_error: 0.3432 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 8/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4279 - mean_absolute_error: 0.3759 - val_loss: 0.3903 - val_mean_absolute_error: 0.3557 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 9/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4228 - mean_absolute_error: 0.3808 - val_loss: 0.3830 - val_mean_absolute_error: 0.3372 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 10/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4183 - mean_absolute_error: 0.3775 - val_loss: 0.3784 - val_mean_absolute_error: 0.3460 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 11/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4080 - mean_absolute_error: 0.3764 - val_loss: 0.3713 - val_mean_absolute_error: 0.3295 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 12/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4014 - mean_absolute_error: 0.3731 - val_loss: 0.3668 - val_mean_absolute_error: 0.3503 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 13/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3883 - mean_absolute_error: 0.3685 - val_loss: 0.3659 - val_mean_absolute_error: 0.3560 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 14/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3893 - mean_absolute_error: 0.3718 - val_loss: 0.3544 - val_mean_absolute_error: 0.3413 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 15/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3787 - mean_absolute_error: 0.3679 - val_loss: 0.3472 - val_mean_absolute_error: 0.3454 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3723 - mean_absolute_error: 0.3660 - val_loss: 0.3437 - val_mean_absolute_error: 0.3445 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 17/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3641 - mean_absolute_error: 0.3660 - val_loss: 0.3378 - val_mean_absolute_error: 0.3564 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 18/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3606 - mean_absolute_error: 0.3705 - val_loss: 0.3336 - val_mean_absolute_error: 0.3544 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 19/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3553 - mean_absolute_error: 0.3686 - val_loss: 0.3254 - val_mean_absolute_error: 0.3456 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 20/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3410 - mean_absolute_error: 0.3600 - val_loss: 0.3219 - val_mean_absolute_error: 0.3449 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 21/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3319 - mean_absolute_error: 0.3561 - val_loss: 0.3134 - val_mean_absolute_error: 0.3508 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 22/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3395 - mean_absolute_error: 0.3694 - val_loss: 0.3054 - val_mean_absolute_error: 0.3281 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 23/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3303 - mean_absolute_error: 0.3623 - val_loss: 0.3023 - val_mean_absolute_error: 0.3480 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 24/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3178 - mean_absolute_error: 0.3588 - val_loss: 0.2965 - val_mean_absolute_error: 0.3261 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 25/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3083 - mean_absolute_error: 0.3544 - val_loss: 0.2945 - val_mean_absolute_error: 0.3448 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 26/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3189 - mean_absolute_error: 0.3663 - val_loss: 0.2858 - val_mean_absolute_error: 0.3354 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 27/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3143 - mean_absolute_error: 0.3683 - val_loss: 0.2841 - val_mean_absolute_error: 0.3343 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 28/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3056 - mean_absolute_error: 0.3615 - val_loss: 0.2788 - val_mean_absolute_error: 0.3497 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 29/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2936 - mean_absolute_error: 0.3576 - val_loss: 0.2805 - val_mean_absolute_error: 0.3502 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 30/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2997 - mean_absolute_error: 0.3642 - val_loss: 0.2786 - val_mean_absolute_error: 0.3497 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 31/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2859 - mean_absolute_error: 0.3560 - val_loss: 0.2672 - val_mean_absolute_error: 0.3333 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 32/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2857 - mean_absolute_error: 0.3569 - val_loss: 0.2643 - val_mean_absolute_error: 0.3380 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 33/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2773 - mean_absolute_error: 0.3509 - val_loss: 0.2625 - val_mean_absolute_error: 0.3576 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 34/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2837 - mean_absolute_error: 0.3649 - val_loss: 0.2607 - val_mean_absolute_error: 0.3281 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 35/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2773 - mean_absolute_error: 0.3579 - val_loss: 0.2599 - val_mean_absolute_error: 0.3553 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 36/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2680 - mean_absolute_error: 0.3585 - val_loss: 0.2545 - val_mean_absolute_error: 0.3402 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 37/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2665 - mean_absolute_error: 0.3512 - val_loss: 0.2500 - val_mean_absolute_error: 0.3373 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 38/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2650 - mean_absolute_error: 0.3514 - val_loss: 0.2525 - val_mean_absolute_error: 0.3435 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 39/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2642 - mean_absolute_error: 0.3517 - val_loss: 0.2504 - val_mean_absolute_error: 0.3443 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 40/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2623 - mean_absolute_error: 0.3532 - val_loss: 0.2508 - val_mean_absolute_error: 0.3299 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 41/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2601 - mean_absolute_error: 0.3543 - val_loss: 0.2444 - val_mean_absolute_error: 0.3428 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 42/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2618 - mean_absolute_error: 0.3551 - val_loss: 0.2482 - val_mean_absolute_error: 0.3538 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 43/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2637 - mean_absolute_error: 0.3585 - val_loss: 0.2460 - val_mean_absolute_error: 0.3380 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 44/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2556 - mean_absolute_error: 0.3548 - val_loss: 0.2412 - val_mean_absolute_error: 0.3291 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 45/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2657 - mean_absolute_error: 0.3609 - val_loss: 0.2458 - val_mean_absolute_error: 0.3294 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 46/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2633 - mean_absolute_error: 0.3567 - val_loss: 0.2452 - val_mean_absolute_error: 0.3442 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 47/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2501 - mean_absolute_error: 0.3489 - val_loss: 0.2444 - val_mean_absolute_error: 0.3565 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 48/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2501 - mean_absolute_error: 0.3536 - val_loss: 0.2415 - val_mean_absolute_error: 0.3384 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 49/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2488 - mean_absolute_error: 0.3511 - val_loss: 0.2406 - val_mean_absolute_error: 0.3433 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 50/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2456 - mean_absolute_error: 0.3487 - val_loss: 0.2382 - val_mean_absolute_error: 0.3394 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 51/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2483 - mean_absolute_error: 0.3495 - val_loss: 0.2344 - val_mean_absolute_error: 0.3317 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 52/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2367 - mean_absolute_error: 0.3384 - val_loss: 0.2346 - val_mean_absolute_error: 0.3361 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 53/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2403 - mean_absolute_error: 0.3425 - val_loss: 0.2352 - val_mean_absolute_error: 0.3360 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 54/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2454 - mean_absolute_error: 0.3450 - val_loss: 0.2343 - val_mean_absolute_error: 0.3372 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 55/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2324 - mean_absolute_error: 0.3363 - val_loss: 0.2346 - val_mean_absolute_error: 0.3355 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 56/150\n",
            "\u001b[1m143/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2378 - mean_absolute_error: 0.3382\n",
            "Epoch 56: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2378 - mean_absolute_error: 0.3383 - val_loss: 0.2345 - val_mean_absolute_error: 0.3341 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 57/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2365 - mean_absolute_error: 0.3366 - val_loss: 0.2351 - val_mean_absolute_error: 0.3368 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 58/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2409 - mean_absolute_error: 0.3424 - val_loss: 0.2344 - val_mean_absolute_error: 0.3347 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 59/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2339 - mean_absolute_error: 0.3348 - val_loss: 0.2353 - val_mean_absolute_error: 0.3395 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 60/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2372 - mean_absolute_error: 0.3402 - val_loss: 0.2353 - val_mean_absolute_error: 0.3386 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 61/150\n",
            "\u001b[1m155/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2352 - mean_absolute_error: 0.3386\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2352 - mean_absolute_error: 0.3386 - val_loss: 0.2352 - val_mean_absolute_error: 0.3367 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 62/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2329 - mean_absolute_error: 0.3368 - val_loss: 0.2346 - val_mean_absolute_error: 0.3363 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 63/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2371 - mean_absolute_error: 0.3398 - val_loss: 0.2350 - val_mean_absolute_error: 0.3382 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 64/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2361 - mean_absolute_error: 0.3392 - val_loss: 0.2344 - val_mean_absolute_error: 0.3378 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 65/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2375 - mean_absolute_error: 0.3398 - val_loss: 0.2349 - val_mean_absolute_error: 0.3384 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 66/150\n",
            "\u001b[1m151/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2341 - mean_absolute_error: 0.3388\n",
            "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2340 - mean_absolute_error: 0.3387 - val_loss: 0.2353 - val_mean_absolute_error: 0.3397 - learning_rate: 5.0000e-05\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 67/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2343 - mean_absolute_error: 0.3409 - val_loss: 0.2353 - val_mean_absolute_error: 0.3375 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 68/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2342 - mean_absolute_error: 0.3346 - val_loss: 0.2345 - val_mean_absolute_error: 0.3379 - learning_rate: 1.0000e-04\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.0001.\n",
            "Epoch 69/150\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2330 - mean_absolute_error: 0.3388 - val_loss: 0.2343 - val_mean_absolute_error: 0.3374 - learning_rate: 1.0000e-04\n",
            "Epoch 69: early stopping\n",
            "Restoring model weights from the end of the best epoch: 54.\n",
            "Validation MAE: 0.3372\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Mean Absolute Error on Validation Data: 0.3372\n",
            "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Optimized Neural Network predictions with bins saved to 'submission_nn_optimized_45.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9_qu3oscB0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}